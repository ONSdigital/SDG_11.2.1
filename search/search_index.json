{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The Public Transport Accessibility Project Analysis for SDG Indicator 11.2.1 This project is to build a data pipeline to analyse data for the UN Sustainable Development Goal indicator 11.2.1, which is part of goal 11: \"Make cities and human settlements inclusive, safe, resilient and sustainable\" By assessing geographical and census data with data on public transport access points (stops and stations), an assessment can be made about the degree of acessability individuals in the population have to public transport.","title":"Home"},{"location":"#the-public-transport-accessibility-project","text":"","title":"The Public Transport Accessibility Project"},{"location":"#analysis-for-sdg-indicator-1121","text":"This project is to build a data pipeline to analyse data for the UN Sustainable Development Goal indicator 11.2.1, which is part of goal 11: \"Make cities and human settlements inclusive, safe, resilient and sustainable\" By assessing geographical and census data with data on public transport access points (stops and stations), an assessment can be made about the degree of acessability individuals in the population have to public transport.","title":"Analysis for SDG Indicator 11.2.1"},{"location":"SDG_NI/","text":"Technical documentation for the SDG_NI module. Any docstrings in this file are automatically copied to this page.","title":"SDG NI"},{"location":"SDG_bus_timetable/","text":"Technical documentation for the SDG_bus_timetable module. Any docstrings in this file are automatically copied to this page.","title":"SDG bus timetable"},{"location":"SDG_scotland/","text":"Technical documentation for the SDG_scotland module. Any docstrings in this file are automatically copied to this page.","title":"SDG scotland"},{"location":"SDG_train_timetable/","text":"Technical documentation for the SDG_train_timetable module. Any docstrings in this file are automatically copied to this page.","title":"SDG train timetable"},{"location":"about/","text":"About the ONS SDG 11.2.1 project This project has been created as side project by James Westwood, Antonio Felton, Paige Hunter and Nathan Shaw, with Musa Chirikeni as a geospatial adviser. All work on the project has been done in our self-development time on a voluntary basis. Description of Target 11.2 By 2030, provide access to safe, affordable, accessible and sustainable transport systems for all, improving road safety, notably by expanding public transport, with special attention to the needs of those in vulnerable situations, women, children, persons with disabilities and older persons Description of Indicator 11.2.1 \"Proportion of population that has convenient access to public transport, by sex, age and persons with disabilities\" Aims of the project The aims are: to build a data pipeline that can analyse data to assess the availability of public transport services for the population of the UK to make the code reusable so that it may be able to analyse other data and: assess the availability of other services across the UK be used to assess the availability of services in other nations Deliverables of the project Open source reuseable code which relies only on open source resources A new dataset with analysis of public transport availability for the UK population Three additional datasets with analysis of public transport availability disagregated by sex, age and disabilities","title":"About"},{"location":"about/#about-the-ons-sdg-1121-project","text":"This project has been created as side project by James Westwood, Antonio Felton, Paige Hunter and Nathan Shaw, with Musa Chirikeni as a geospatial adviser. All work on the project has been done in our self-development time on a voluntary basis.","title":"About the ONS SDG 11.2.1 project"},{"location":"about/#description-of-target-112","text":"By 2030, provide access to safe, affordable, accessible and sustainable transport systems for all, improving road safety, notably by expanding public transport, with special attention to the needs of those in vulnerable situations, women, children, persons with disabilities and older persons","title":"Description of Target 11.2"},{"location":"about/#description-of-indicator-1121","text":"\"Proportion of population that has convenient access to public transport, by sex, age and persons with disabilities\"","title":"Description of Indicator 11.2.1"},{"location":"about/#aims-of-the-project","text":"The aims are: to build a data pipeline that can analyse data to assess the availability of public transport services for the population of the UK to make the code reusable so that it may be able to analyse other data and: assess the availability of other services across the UK be used to assess the availability of services in other nations","title":"Aims of the project"},{"location":"about/#deliverables-of-the-project","text":"Open source reuseable code which relies only on open source resources A new dataset with analysis of public transport availability for the UK population Three additional datasets with analysis of public transport availability disagregated by sex, age and disabilities","title":"Deliverables of the project"},{"location":"building_docs/","text":"Using mkdocs to edit these pages Modifying a page If you want to edit an existing page, you need to edit the corresponding .md file which can be found in the docs/ folder of this project. Follow these instructions: Create a new branch (with the example name of my-page-edits ) using Git with the following command: git checkout -b my-page-edits Make the necessary changes to the markdown files that you want to update. Git add and commit as usual. After committing your changes, run the following command to rebuild your site: mkdocs build After running the build command, mkdocs will create a new set of HTML files in the 'site/' directory, replacing the previous version of your site. Before pushing, it's a good idea to see the changes on your local machine. To preview your changes, you can run the following command to launch a local server: mkdocs serve Open your web browser and navigate to http://localhost:8000 to see your changes in action. If you are happy with your changes, you can push your updated branch to your Git repository. After that create a pull request to main and the Github Action (controlled by this yaml .github/workflows/pages.yml ) should trigger the re-building and deployment of the pages. Manually deploying pages with gh-deploy You can deploy any changes you have made locally using mkdocs gh-deploy . Look for more guidance in the mkdocs official documentation Repository \"Pages\" settings You should select the \"gh-pages\" branch as the source branch in the Pages section of Github's Pages settings. Here are the steps to follow: Go to your Github repository's settings page. Scroll down to the \"Pages\" section. Under \"Source\", select \"gh-pages\" from the dropdown menu. Click \"Save\". This tells Github to serve your pages from the \"gh-pages\" branch of your repository, which is where mkdocs gh-deploy will push your documentation. Note that it may take a few minutes for your changes to take effect on the Github Pages site.","title":"Using mkdocs to edit these pages"},{"location":"building_docs/#using-mkdocs-to-edit-these-pages","text":"","title":"Using mkdocs to edit these pages"},{"location":"building_docs/#modifying-a-page","text":"If you want to edit an existing page, you need to edit the corresponding .md file which can be found in the docs/ folder of this project. Follow these instructions: Create a new branch (with the example name of my-page-edits ) using Git with the following command: git checkout -b my-page-edits Make the necessary changes to the markdown files that you want to update. Git add and commit as usual. After committing your changes, run the following command to rebuild your site: mkdocs build After running the build command, mkdocs will create a new set of HTML files in the 'site/' directory, replacing the previous version of your site. Before pushing, it's a good idea to see the changes on your local machine. To preview your changes, you can run the following command to launch a local server: mkdocs serve Open your web browser and navigate to http://localhost:8000 to see your changes in action. If you are happy with your changes, you can push your updated branch to your Git repository. After that create a pull request to main and the Github Action (controlled by this yaml .github/workflows/pages.yml ) should trigger the re-building and deployment of the pages.","title":"Modifying a page"},{"location":"building_docs/#manually-deploying-pages-with-gh-deploy","text":"You can deploy any changes you have made locally using mkdocs gh-deploy . Look for more guidance in the mkdocs official documentation","title":"Manually deploying pages with gh-deploy"},{"location":"building_docs/#repository-pages-settings","text":"You should select the \"gh-pages\" branch as the source branch in the Pages section of Github's Pages settings. Here are the steps to follow: Go to your Github repository's settings page. Scroll down to the \"Pages\" section. Under \"Source\", select \"gh-pages\" from the dropdown menu. Click \"Save\". This tells Github to serve your pages from the \"gh-pages\" branch of your repository, which is where mkdocs gh-deploy will push your documentation. Note that it may take a few minutes for your changes to take effect on the Github Pages site.","title":"Repository \"Pages\" settings"},{"location":"comparison_to_EU_method/","text":"Calculation of Statistics for SDG 11.2.1: The UK Method Compared to the EU Method Differences and similarities between our method and that of the EU The following is a comparison of the method employed to calculate public transport accessibility by a team researching for the EU Commission and a UK team, working on behalf of the Sustainable Development Goals team in the Office for National Statistics (ONS). The two teams will be referred to as the \u201cONS team\u201d and the \u201cEU team\u201d going forward. Technology used Open source development The SDG team at ONS signed up to the Inclusive Data Charter 1 . In line with its principles, those of the SDGs and OSAT we have worked as transparently as possible, making all code open source and conversations around method and development decisions published on our Github repository, and used free and open source tools and technology throughout the development process. The EU team have made their code available (see annexes here ) as well as a write up about their chosen method and results, though an open development process was not found, though it may exist. Software The EU team and the ONS 11.2.1 project team have approached the challenges of calculating transport accessibility similarly, both in the methodology and the technology involved. Both teams have made use of Python as a means to ingest and process data. However the main processing of the data is done using different technologies, namely that the EU team has used the ArcPy library to interact with ArcGIS, a program which is not open source or free. The ONS team, by comparison, carries out the data processing in Pandas and the Geospatial analysis in GeoPandas , all of which are open source and free. Computation method The ONS computation calculates the service accessibility (see definitions ) for the whole country - except that currently the script cycles through local authorities and aggregates the data into a single table later. This method of calculating areas and combining up to the whole nation later will possibly be removed when the ONS team optimise the script. The ONS pipeline calculates the service accessibility for the whole country by cycling through local authorities and aggregating at the end of the process (for version 1.0 of our pipeline). Version 1.0 will only use data for 2011 and 1.1 will make the calculation over multiple years. The type of computation may be changed in future iterations of the project to a more vectorised computation (probably in version 1.3) - if this feature is implemented. The EU team calculates the transport service accessibility for each \u201carea of interest\u201d (e.g. city, region,urban centre, etc.) focusing on the urban area (in line with 11.2.1 methodology) but the ONS team has calculated for rural areas too - output data that is then compared with urban areas to get an idea of comparative transport access. The methodology for the calculation used The method used for the calculation is also very similar, but there are some important differences. Transport access points The data on the location of the transport nodes (stops and stations) is required for this calculation. The ONS team sourced the data from the National Public Transport Access Nodes (NaPTAN) data set. It is not clear where the EU team sourced their data from. The EU team cluster stops together if they are within 50 metres of each other. The cluster is then represented by a single point, mid distance between the stops that have been clustered. On the other hand, the ONS team treats every stop in the country as individual stops, each of which is treated as an individual point. To establish the effect on the results this would have, our team would have to update our code to cluster stops in the same way, a step the ONS team will consider for future iterations of the project. Public transport service areas When it comes to calculating the service area, both teams use the same distance as a buffer. The difference is how the distance from the central node (station/stop/cluster) is calculated. In the EU team\u2019s method, a service area (catchment areas) of 500 metres or 1000m (according to capacity) is created. This is made using a pedestrian path and road network. The ONS team use a simple Euclidean buffer to create a circle (techinically a polygon) around each stop. The polygons are then combined to create the entire service area. Our team has considered the network/path method, and it has not been ruled out, however the ONS team note that computationally this will be more intensive and accurate results rely on a very accurate pedestrian path network. If the network is incomplete, or maps paths on to pedestrian unfriendly roads (such as fast roads with no pavements) the results will be unreliable. Examining the EU method, by using the road network, based on the shape_length parameter (which is probably the path line) and the walking speed, an isochrone of the service area is created. This is almost certainly how the above-mentioned service area polygons are created. The current ONS method makes no use of isochrones or irregular polygons in the calculation of service areas. Calculation of served vs. unserved population Seemingly the method of counting the population inside and outside of the service areas is the same on both teams, though the method of establishing the population in each area is different. Both the ONS team and the EU team count the population inside and outside of the service areas. The ONS team use sum method in Pandas dataframe, after running a points in polygons query (a kind of spatial join) to only include population centres that are within the population centre. Differentiating high and low capacity stops The ONS team have differentiated high and low capacity stops using the StopType field in the NaPTAN dataset, the feasibility and methodology of which was discussed here and here ; and the implementation of these features are discussed here and here . The EU team imports two datasets of high and low capacity stops. However it is unclear how those datasets are generated. The ONS team group underground service (in London) at the same level as Trams (e.g in Manchester), however this may change based on further discussion before the release of version 1.0. Similarities and differences in our data Spatial granularity of data The ONS team have used population data at output area level, which is the most granular level available. The Government Statistical Service states that the 2011 Census, England was divided into 171,372 Output Areas (OAs) which on average have a resident population of 309 people. According to their publication the EU team population estimate figures at the building-block level and combine that data at the best available spatial resolution with data on land cover, land use, and data on the location, function and height of buildings to obtain estimates of a useful quality. Urban and rural definitions The ONS definition of \"Urban\" and \"Rural\" is defined by their population density in each output area. According to Government Statistical Service, urban areas are the connected built up areas identified by Ordnance Survey mapping that have resident populations \\ above 10,000 people (2011 Census). On the other hand rural areas are those areas that are not urban, i.e. consisting of settlements below 10,000 people or are open countryside. The EU team have used the EU definition of urban, which states that urban centres have a population density of more than 1 500 inhabitants/km\u00b2.\u201d (Poelman et al., pg. 30). Additionally they use the EU-OECD \u201cFunctional Urban Area\u201d definition for urban centres, for which is the conglomerate they calculate. Disaggregations Age and sex The ONS team disaggregates on age and sex. Having used the UK census population data which is detailed all ages and sex of population at output level up to 90+. The population data was then \u201cbinned\u201d into 5 year groups based on a discussion with the SDG data team noted here (Nov, 2020). The binning can be changed as it is defined in the config file; though this will be improved and fully tested in a future iteration . The EU team intends to use the European population grid for 2021 for disaggregated breakdowns, but in the most recent publication , they did not carry out disaggregated analysis by age, nor sex. Disability status NOMIS is a research database and analytical tool that provides data on labour markets, demographics, and the economy. NOMIS stands for \"New Opportunities for Migrants' Integration and Success\" and is funded by the Economic and Social Research Council (ESRC) in the United Kingdom. Via NOMIS , data on disability status output from the UK census data is available for the year of the census. For years later than the census, the population estimate for each area was multiplied by the proportion of each disability category, calculated from the census year (see the methodology writeup on disability ). Again, the EU team did not carry out disaggregated analysis on disability across the EU due to a lack of data. Notes Inclusive data charter action plan for the global Sustainable Development Goals \u21a9","title":"EU and UK Methodology Comparison"},{"location":"comparison_to_EU_method/#calculation-of-statistics-for-sdg-1121-the-uk-method-compared-to-the-eu-method","text":"","title":"Calculation of Statistics for SDG 11.2.1: The UK Method Compared to the EU Method"},{"location":"comparison_to_EU_method/#differences-and-similarities-between-our-method-and-that-of-the-eu","text":"The following is a comparison of the method employed to calculate public transport accessibility by a team researching for the EU Commission and a UK team, working on behalf of the Sustainable Development Goals team in the Office for National Statistics (ONS). The two teams will be referred to as the \u201cONS team\u201d and the \u201cEU team\u201d going forward.","title":"Differences and similarities between our method and that of the EU"},{"location":"comparison_to_EU_method/#technology-used","text":"","title":"Technology used"},{"location":"comparison_to_EU_method/#open-source-development","text":"The SDG team at ONS signed up to the Inclusive Data Charter 1 . In line with its principles, those of the SDGs and OSAT we have worked as transparently as possible, making all code open source and conversations around method and development decisions published on our Github repository, and used free and open source tools and technology throughout the development process. The EU team have made their code available (see annexes here ) as well as a write up about their chosen method and results, though an open development process was not found, though it may exist.","title":"Open source development"},{"location":"comparison_to_EU_method/#software","text":"The EU team and the ONS 11.2.1 project team have approached the challenges of calculating transport accessibility similarly, both in the methodology and the technology involved. Both teams have made use of Python as a means to ingest and process data. However the main processing of the data is done using different technologies, namely that the EU team has used the ArcPy library to interact with ArcGIS, a program which is not open source or free. The ONS team, by comparison, carries out the data processing in Pandas and the Geospatial analysis in GeoPandas , all of which are open source and free.","title":"Software"},{"location":"comparison_to_EU_method/#computation-method","text":"The ONS computation calculates the service accessibility (see definitions ) for the whole country - except that currently the script cycles through local authorities and aggregates the data into a single table later. This method of calculating areas and combining up to the whole nation later will possibly be removed when the ONS team optimise the script. The ONS pipeline calculates the service accessibility for the whole country by cycling through local authorities and aggregating at the end of the process (for version 1.0 of our pipeline). Version 1.0 will only use data for 2011 and 1.1 will make the calculation over multiple years. The type of computation may be changed in future iterations of the project to a more vectorised computation (probably in version 1.3) - if this feature is implemented. The EU team calculates the transport service accessibility for each \u201carea of interest\u201d (e.g. city, region,urban centre, etc.) focusing on the urban area (in line with 11.2.1 methodology) but the ONS team has calculated for rural areas too - output data that is then compared with urban areas to get an idea of comparative transport access.","title":"Computation method"},{"location":"comparison_to_EU_method/#the-methodology-for-the-calculation-used","text":"The method used for the calculation is also very similar, but there are some important differences.","title":"The methodology for the calculation used"},{"location":"comparison_to_EU_method/#transport-access-points","text":"The data on the location of the transport nodes (stops and stations) is required for this calculation. The ONS team sourced the data from the National Public Transport Access Nodes (NaPTAN) data set. It is not clear where the EU team sourced their data from. The EU team cluster stops together if they are within 50 metres of each other. The cluster is then represented by a single point, mid distance between the stops that have been clustered. On the other hand, the ONS team treats every stop in the country as individual stops, each of which is treated as an individual point. To establish the effect on the results this would have, our team would have to update our code to cluster stops in the same way, a step the ONS team will consider for future iterations of the project.","title":"Transport access points"},{"location":"comparison_to_EU_method/#public-transport-service-areas","text":"When it comes to calculating the service area, both teams use the same distance as a buffer. The difference is how the distance from the central node (station/stop/cluster) is calculated. In the EU team\u2019s method, a service area (catchment areas) of 500 metres or 1000m (according to capacity) is created. This is made using a pedestrian path and road network. The ONS team use a simple Euclidean buffer to create a circle (techinically a polygon) around each stop. The polygons are then combined to create the entire service area. Our team has considered the network/path method, and it has not been ruled out, however the ONS team note that computationally this will be more intensive and accurate results rely on a very accurate pedestrian path network. If the network is incomplete, or maps paths on to pedestrian unfriendly roads (such as fast roads with no pavements) the results will be unreliable. Examining the EU method, by using the road network, based on the shape_length parameter (which is probably the path line) and the walking speed, an isochrone of the service area is created. This is almost certainly how the above-mentioned service area polygons are created. The current ONS method makes no use of isochrones or irregular polygons in the calculation of service areas.","title":"Public transport service areas"},{"location":"comparison_to_EU_method/#calculation-of-served-vs-unserved-population","text":"Seemingly the method of counting the population inside and outside of the service areas is the same on both teams, though the method of establishing the population in each area is different. Both the ONS team and the EU team count the population inside and outside of the service areas. The ONS team use sum method in Pandas dataframe, after running a points in polygons query (a kind of spatial join) to only include population centres that are within the population centre.","title":"Calculation of served vs. unserved population"},{"location":"comparison_to_EU_method/#differentiating-high-and-low-capacity-stops","text":"The ONS team have differentiated high and low capacity stops using the StopType field in the NaPTAN dataset, the feasibility and methodology of which was discussed here and here ; and the implementation of these features are discussed here and here . The EU team imports two datasets of high and low capacity stops. However it is unclear how those datasets are generated. The ONS team group underground service (in London) at the same level as Trams (e.g in Manchester), however this may change based on further discussion before the release of version 1.0.","title":"Differentiating high and low capacity stops"},{"location":"comparison_to_EU_method/#similarities-and-differences-in-our-data","text":"","title":"Similarities and differences in our data"},{"location":"comparison_to_EU_method/#spatial-granularity-of-data","text":"The ONS team have used population data at output area level, which is the most granular level available. The Government Statistical Service states that the 2011 Census, England was divided into 171,372 Output Areas (OAs) which on average have a resident population of 309 people. According to their publication the EU team population estimate figures at the building-block level and combine that data at the best available spatial resolution with data on land cover, land use, and data on the location, function and height of buildings to obtain estimates of a useful quality.","title":"Spatial granularity of data"},{"location":"comparison_to_EU_method/#urban-and-rural-definitions","text":"The ONS definition of \"Urban\" and \"Rural\" is defined by their population density in each output area. According to Government Statistical Service, urban areas are the connected built up areas identified by Ordnance Survey mapping that have resident populations \\ above 10,000 people (2011 Census). On the other hand rural areas are those areas that are not urban, i.e. consisting of settlements below 10,000 people or are open countryside. The EU team have used the EU definition of urban, which states that urban centres have a population density of more than 1 500 inhabitants/km\u00b2.\u201d (Poelman et al., pg. 30). Additionally they use the EU-OECD \u201cFunctional Urban Area\u201d definition for urban centres, for which is the conglomerate they calculate.","title":"Urban and rural definitions"},{"location":"comparison_to_EU_method/#disaggregations","text":"Age and sex The ONS team disaggregates on age and sex. Having used the UK census population data which is detailed all ages and sex of population at output level up to 90+. The population data was then \u201cbinned\u201d into 5 year groups based on a discussion with the SDG data team noted here (Nov, 2020). The binning can be changed as it is defined in the config file; though this will be improved and fully tested in a future iteration . The EU team intends to use the European population grid for 2021 for disaggregated breakdowns, but in the most recent publication , they did not carry out disaggregated analysis by age, nor sex. Disability status NOMIS is a research database and analytical tool that provides data on labour markets, demographics, and the economy. NOMIS stands for \"New Opportunities for Migrants' Integration and Success\" and is funded by the Economic and Social Research Council (ESRC) in the United Kingdom. Via NOMIS , data on disability status output from the UK census data is available for the year of the census. For years later than the census, the population estimate for each area was multiplied by the proportion of each disability category, calculated from the census year (see the methodology writeup on disability ). Again, the EU team did not carry out disaggregated analysis on disability across the EU due to a lack of data.","title":"Disaggregations"},{"location":"comparison_to_EU_method/#notes","text":"Inclusive data charter action plan for the global Sustainable Development Goals \u21a9","title":"Notes"},{"location":"data_description/","text":"Data The Public Transport Availability project looks to assess the proportion of people who live near a public transport stop. Below is a description of the data sources used in order to perform this calculation. Most data sources are split into 3 sections: England & Wales, Scotland, and Northern Ireland (NI). NaPTAN The National Public Transport Access Nodes (NaPTAN) dataset contains a list of all public transport access points in Great Britain including bus, rail and tram. This is open-source data and is publicly available. As of 3rd May 2022, the dataset includes around 435,000 public transport access points. The following columns are used within our calculations. The full schema for NapTAN can be found here . Column Description NaptanCode Seven- or eight-digit identifier for access point. CommonName Name of bus stop. Easting Uk Geocoding reference. Northing Status Whether an access point is active, inactive or pending. StopType The type of access point e.g bus or rail The dataset is filtered based on two conditions. The Status column must not be inactive. This ensures that historic public transport access points are not included in the calculations. The StopType is either a bus or rail public transport access point. Derived Variables The StopType that are included are in the calculations are \"RSE\", \"RLY\", \"RPL\", \"TMU\", \"MET\", \"PLT\", \"BCE\", \"BST\",\"BCQ\", \"BCS\",\"BCT\". After filtering there are 383,662 public transport access points. A capacity_type variable is derived which classifies public transport as either high or low capacity. This is consistent with the UN definition . A geometry variable is derived which creates a polygon around each public transport access point. The polygon for a low capacity bus stop is a circle with radius of 500 metres with the access point being the centre point. The polygon for high capacity is the same with a circle with a radius of 1000 metres. These polygons will be used to determine if a weighted centroid lives within the polygon. Census Data The census takes place every 10 years and aims to obtain information on all households in the UK and statistics are published at various geographic levels. Output area (OA) is a lower level geography which contains on average approximately 300 people. For the purposes of our calculations each OA will be grouped together into one household. In Northern Ireland, small areas are used as opposed to OAs. The use of OAs were replaced after the 2001 NI census and small areas have an approximate population of 400 people. Census data is used to calculate percentages of certain demographics which can then be applied to the annual population estimates. For example the annual population estimates do not include information on disability status. A proportion of people who are disabed can be calculated from the 2011 Census per OA and then applied to the population estimates data. England & Wales The population weighted centroids for OA from the 2011 census are used. These are OA\u2019s containing where the midpoint of their population is. These are the points used to deduce whether an OA is contained within a service area. The urban/rural classification is used to classify if an OA is urban or rural. This is used to be able to calculate different estimates for each classification. The OA\u2019s are classed as \u2018urban\u2019 if they were allocated to a 2011 built-up area with a population of 10,000 people or more. All other remaining OAs are classed as \u2018rural\u2019. The QS303EW , a long-term health problem or disability dataset, derived from the 2011 census, contains disability information on an OA basis. This information is transformed to be consistent with the GSS harmonized disability data and allows us to produce estimates disaggregated by disability status. The QS104EW contains sex population estimates for each OA. Scotland The population weighted centroids for each OA from the 2011 census are used. These are OA\u2019s containing where the midpoint of their population is. These are the points used to deduce whether an OA is contained within a service area. The urban/rural classification is used to classify if an OA is urban or rural. The UR6_2013_2014 column is used to determine if an output area is rural or urban. An OA is classified as 'urban' if they were allocated to a 2013/14 built-up area with a population of 10,000 people or more. All other remaining OAs are classified as 'rural'. This is consistent with the England & Wales definition. The QS303S , a long-term health problem or disability dataset, derived from the 2011 census, contains disability information on an OA basis. This information is transformed to be consistent with the GSS harmonized disability data and allows us to produce estimates disaggregated by disability status. Northern Ireland The population weighted centroids used in Northern Ireland are based on the 2001 NI census' OAs as no further updates to the data have been given. Since the 2011 census, Northern Ireland use small areas which are slightly different to OAs. There is a 1-1 lookup table which converts each OA to a small area so that we can use the population weighted centroids from the 2001 census. The urban/rural classification classifies a NI small area to be urban or rural. We classified an urban area to be where a settlement has a population greater than 10,000. The NISRA classify an urban area as greater than 5,000 population which is inconsistent with our classification. The previous definition was chosen to be consistent between England & Wales. The QS303NI , a long-term health problem or disability dataset, derived from the 2011 census, contains disability information on a small area basis. This information is transformed to be consistent with the GSS harmonized disability data and allows us to produce estimates disaggregated by disability status. Annual Population Estimates The ONS produces population estimates every year for England & Wales. This contains information on population estimates for each OA. This also contains splits of age and sex for England & Wales only . These are annual so the year used is consistent with the calculation year. We also use NI population estimates from the Northern Ireland Statistics and Research Agency (NISRA) which give mid-year population estimates for small areas. These are not disaggregated by age and sex, so has to be calculated using proportions from 2011 census. This is explained in the calculation process. Scotland currently has no mid-year population estimates for OA, and we can only calculate population for each OA based on LA level. Local Authorities (LA) Boundary Data This section discusses the local authority boundaries data used for England & Wales, Scotland, and Northern Ireland. England & Wales The boundaries of each local authority are used to ensure calculations are aggregated to an LA basis. This lookup file between LA and OA is used as it is important when aggregating OA estimates to a LA level for England & Wales. These boundaries and lookup files are annual so the year used is consistent with the calculation year. Scotland The boundaries of each local authority for Scotland use 2021 boundaries only at the moment. We are able to access a LA 2011 boundary but there is no lookup table for this so are using LA2021 boundary data and lookup table . Future work will look to investigating the differences between LA 2011 boundaries and LA 2021 boundaries. Unfortunately, Scotland boundaries and lookup files are not annual so we cannot access more accurate data for other years. Northern Ireland The same local authorities boundary file that is used in England & Wales is also used for Northern Ireland. We used an OA to SA lookup file, as well as an SA to LA lookup file. This was due to only having population weighted centroids for each OA in 2011, meaning we converted each OA to an SA using a 1:1 lookup table. This meant we could then use an SA to LA lookup table to aggregate the SA estimates to LA level.","title":"Data Description"},{"location":"data_description/#data","text":"The Public Transport Availability project looks to assess the proportion of people who live near a public transport stop. Below is a description of the data sources used in order to perform this calculation. Most data sources are split into 3 sections: England & Wales, Scotland, and Northern Ireland (NI).","title":"Data"},{"location":"data_description/#naptan","text":"The National Public Transport Access Nodes (NaPTAN) dataset contains a list of all public transport access points in Great Britain including bus, rail and tram. This is open-source data and is publicly available. As of 3rd May 2022, the dataset includes around 435,000 public transport access points. The following columns are used within our calculations. The full schema for NapTAN can be found here . Column Description NaptanCode Seven- or eight-digit identifier for access point. CommonName Name of bus stop. Easting Uk Geocoding reference. Northing Status Whether an access point is active, inactive or pending. StopType The type of access point e.g bus or rail The dataset is filtered based on two conditions. The Status column must not be inactive. This ensures that historic public transport access points are not included in the calculations. The StopType is either a bus or rail public transport access point.","title":"NaPTAN"},{"location":"data_description/#derived-variables","text":"The StopType that are included are in the calculations are \"RSE\", \"RLY\", \"RPL\", \"TMU\", \"MET\", \"PLT\", \"BCE\", \"BST\",\"BCQ\", \"BCS\",\"BCT\". After filtering there are 383,662 public transport access points. A capacity_type variable is derived which classifies public transport as either high or low capacity. This is consistent with the UN definition . A geometry variable is derived which creates a polygon around each public transport access point. The polygon for a low capacity bus stop is a circle with radius of 500 metres with the access point being the centre point. The polygon for high capacity is the same with a circle with a radius of 1000 metres. These polygons will be used to determine if a weighted centroid lives within the polygon.","title":"Derived Variables"},{"location":"data_description/#census-data","text":"The census takes place every 10 years and aims to obtain information on all households in the UK and statistics are published at various geographic levels. Output area (OA) is a lower level geography which contains on average approximately 300 people. For the purposes of our calculations each OA will be grouped together into one household. In Northern Ireland, small areas are used as opposed to OAs. The use of OAs were replaced after the 2001 NI census and small areas have an approximate population of 400 people. Census data is used to calculate percentages of certain demographics which can then be applied to the annual population estimates. For example the annual population estimates do not include information on disability status. A proportion of people who are disabed can be calculated from the 2011 Census per OA and then applied to the population estimates data.","title":"Census Data"},{"location":"data_description/#england-wales","text":"The population weighted centroids for OA from the 2011 census are used. These are OA\u2019s containing where the midpoint of their population is. These are the points used to deduce whether an OA is contained within a service area. The urban/rural classification is used to classify if an OA is urban or rural. This is used to be able to calculate different estimates for each classification. The OA\u2019s are classed as \u2018urban\u2019 if they were allocated to a 2011 built-up area with a population of 10,000 people or more. All other remaining OAs are classed as \u2018rural\u2019. The QS303EW , a long-term health problem or disability dataset, derived from the 2011 census, contains disability information on an OA basis. This information is transformed to be consistent with the GSS harmonized disability data and allows us to produce estimates disaggregated by disability status. The QS104EW contains sex population estimates for each OA.","title":"England &amp; Wales"},{"location":"data_description/#scotland","text":"The population weighted centroids for each OA from the 2011 census are used. These are OA\u2019s containing where the midpoint of their population is. These are the points used to deduce whether an OA is contained within a service area. The urban/rural classification is used to classify if an OA is urban or rural. The UR6_2013_2014 column is used to determine if an output area is rural or urban. An OA is classified as 'urban' if they were allocated to a 2013/14 built-up area with a population of 10,000 people or more. All other remaining OAs are classified as 'rural'. This is consistent with the England & Wales definition. The QS303S , a long-term health problem or disability dataset, derived from the 2011 census, contains disability information on an OA basis. This information is transformed to be consistent with the GSS harmonized disability data and allows us to produce estimates disaggregated by disability status.","title":"Scotland"},{"location":"data_description/#northern-ireland","text":"The population weighted centroids used in Northern Ireland are based on the 2001 NI census' OAs as no further updates to the data have been given. Since the 2011 census, Northern Ireland use small areas which are slightly different to OAs. There is a 1-1 lookup table which converts each OA to a small area so that we can use the population weighted centroids from the 2001 census. The urban/rural classification classifies a NI small area to be urban or rural. We classified an urban area to be where a settlement has a population greater than 10,000. The NISRA classify an urban area as greater than 5,000 population which is inconsistent with our classification. The previous definition was chosen to be consistent between England & Wales. The QS303NI , a long-term health problem or disability dataset, derived from the 2011 census, contains disability information on a small area basis. This information is transformed to be consistent with the GSS harmonized disability data and allows us to produce estimates disaggregated by disability status.","title":"Northern Ireland"},{"location":"data_description/#annual-population-estimates","text":"The ONS produces population estimates every year for England & Wales. This contains information on population estimates for each OA. This also contains splits of age and sex for England & Wales only . These are annual so the year used is consistent with the calculation year. We also use NI population estimates from the Northern Ireland Statistics and Research Agency (NISRA) which give mid-year population estimates for small areas. These are not disaggregated by age and sex, so has to be calculated using proportions from 2011 census. This is explained in the calculation process. Scotland currently has no mid-year population estimates for OA, and we can only calculate population for each OA based on LA level.","title":"Annual Population Estimates"},{"location":"data_description/#local-authorities-la-boundary-data","text":"This section discusses the local authority boundaries data used for England & Wales, Scotland, and Northern Ireland.","title":"Local Authorities (LA) Boundary Data"},{"location":"data_description/#england-wales_1","text":"The boundaries of each local authority are used to ensure calculations are aggregated to an LA basis. This lookup file between LA and OA is used as it is important when aggregating OA estimates to a LA level for England & Wales. These boundaries and lookup files are annual so the year used is consistent with the calculation year.","title":"England &amp; Wales"},{"location":"data_description/#scotland_1","text":"The boundaries of each local authority for Scotland use 2021 boundaries only at the moment. We are able to access a LA 2011 boundary but there is no lookup table for this so are using LA2021 boundary data and lookup table . Future work will look to investigating the differences between LA 2011 boundaries and LA 2021 boundaries. Unfortunately, Scotland boundaries and lookup files are not annual so we cannot access more accurate data for other years.","title":"Scotland"},{"location":"data_description/#northern-ireland_1","text":"The same local authorities boundary file that is used in England & Wales is also used for Northern Ireland. We used an OA to SA lookup file, as well as an SA to LA lookup file. This was due to only having population weighted centroids for each OA in 2011, meaning we converted each OA to an SA using a 1:1 lookup table. This meant we could then use an SA to LA lookup table to aggregate the SA estimates to LA level.","title":"Northern Ireland"},{"location":"data_ingest/","text":"All functions and classes related to ingesting data into the pipeline GCPBucket This class sets up an instance of a GCP storage client with the right credentials. The methods either download a file from the bucket or generate a signed url for a file. Source code in src/data_ingest.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 class GCPBucket : \"\"\" This class sets up an instance of a GCP storage client with the right credentials. The methods either download a file from the bucket or generate a signed url for a file. \"\"\" def __init__ ( self ): self . credentials = service_account . Credentials . from_service_account_file ( glob . glob ( 'secrets/*.json' )[ 0 ]) self . client = storage . Client ( credentials = self . credentials ) self . bucket_name = '11-2-1-all-data' self . bucket = self . client . bucket ( self . bucket_name ) def download_file ( self , file_name , destination_file_name ): \"\"\"Downloads a blob from the bucket.\"\"\" blob = self . bucket . blob ( file_name ) # Create the folder (if it doesn't exist) folder_path = os . path . dirname ( destination_file_name ) make_non_existent_folder ( folder_path ) blob . download_to_filename ( destination_file_name ) print ( f \"Blob { self . client } downloaded to { destination_file_name } .\" ) def generate_signed_url ( self , object_name ): \"\"\" This will generate a signed URL that is valid for 5 minutes. The URL should be able to be used by our data ingest functions to download any file.\"\"\" expiration = datetime . utcnow () + timedelta ( hours = 1 ) url = self . bucket . blob ( object_name ) . generate_signed_url ( expiration , method = 'GET' , credentials = self . credentials , ) return url def get_file_list ( self ): \"\"\"This will return a list of all the files in the bucket, as \"blobs\".\"\"\" files = self . bucket . list_blobs () return files def get_file_names ( self ): \"\"\"Prints and returns a list of all the file names in the bucket. Returns: List: List of file names in the bucket. \"\"\" files = self . bucket . list_blobs () file_names = [ file . name for file in files ] print ( f \"Found { len ( file_names ) } files in the bucket.\" ) #for file in file_names: #print(file) return file_names download_file ( file_name , destination_file_name ) Downloads a blob from the bucket. Source code in src/data_ingest.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def download_file ( self , file_name , destination_file_name ): \"\"\"Downloads a blob from the bucket.\"\"\" blob = self . bucket . blob ( file_name ) # Create the folder (if it doesn't exist) folder_path = os . path . dirname ( destination_file_name ) make_non_existent_folder ( folder_path ) blob . download_to_filename ( destination_file_name ) print ( f \"Blob { self . client } downloaded to { destination_file_name } .\" ) generate_signed_url ( object_name ) This will generate a signed URL that is valid for 5 minutes. The URL should be able to be used by our data ingest functions to download any file. Source code in src/data_ingest.py 68 69 70 71 72 73 74 75 76 77 78 79 def generate_signed_url ( self , object_name ): \"\"\" This will generate a signed URL that is valid for 5 minutes. The URL should be able to be used by our data ingest functions to download any file.\"\"\" expiration = datetime . utcnow () + timedelta ( hours = 1 ) url = self . bucket . blob ( object_name ) . generate_signed_url ( expiration , method = 'GET' , credentials = self . credentials , ) return url get_file_list () This will return a list of all the files in the bucket, as \"blobs\". Source code in src/data_ingest.py 81 82 83 84 85 86 def get_file_list ( self ): \"\"\"This will return a list of all the files in the bucket, as \"blobs\".\"\"\" files = self . bucket . list_blobs () return files get_file_names () Prints and returns a list of all the file names in the bucket. Returns: List \u2013 List of file names in the bucket. Source code in src/data_ingest.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def get_file_names ( self ): \"\"\"Prints and returns a list of all the file names in the bucket. Returns: List: List of file names in the bucket. \"\"\" files = self . bucket . list_blobs () file_names = [ file . name for file in files ] print ( f \"Found { len ( file_names ) } files in the bucket.\" ) #for file in file_names: #print(file) return file_names best_before ( path , number_of_days ) Checks whether a path has been modified within a period of days. Parameters: path ( str ) \u2013 the path to check number_of_days ( int ) \u2013 number of days previous to check for modifications Returns: Bool \u2013 True if path has not been modified within the number of days specified Source code in src/data_ingest.py 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 def best_before ( path , number_of_days ): \"\"\" Checks whether a path has been modified within a period of days. Args: path (str): the path to check number_of_days (int): number of days previous to check for modifications Returns: Bool: True if path has not been modified within the number of days specified \"\"\" todays_date = datetime . today () last_modified_date = datetime . fromtimestamp ( os . path . getmtime ( path )) days_since_last_modification = ( todays_date - last_modified_date ) . days if days_since_last_modification > number_of_days : expired = True else : expired = False return expired capture_region ( file_nm ) Extracts the region name from the ONS population estimate excel files. Parameters: file_nm ( str ) \u2013 Full name of the regional population Excel file. Returns: str \u2013 The name of the region that the file covers Source code in src/data_ingest.py 410 411 412 413 414 415 416 417 418 419 420 421 422 def capture_region ( file_nm : str ): \"\"\"Extracts the region name from the ONS population estimate excel files. Args: file_nm (str): Full name of the regional population Excel file. Returns: str: The name of the region that the file covers \"\"\" patt = re . compile ( \"^(.*estimates[-]?)(?P<region>.*)(\\.xls)\" ) region = re . search ( patt , file_nm ) . group ( \"region\" ) region = region . replace ( \"-\" , \" \" ) . capitalize () return region csv_to_df ( file_nm , csv_path , dtypes , persistent_exists = True , zip_url = None ) Creates pandas DataFrame from csv; optionally using datatypes & selected columns. Sub func of both any_to_pd and _import_extract_delete_zip. Parameters: file_nm ( str ) \u2013 The name of the source csv without the extension. e.g. \"stops\", not \"stops.csv\". csv_path ( PathLike ) \u2013 The path/to/csv_file on local machine. dtypes ( Optional [ Dict ] ) \u2013 Datatypes of columns in the csv. Helps optimise import. persistent_exists ( bool , default: True ) \u2013 Boolean supplied by the _persistent_exists function. Defaults to True. zip_url ( str , default: None ) \u2013 URL for the zip resource if it is to be downloaded. Defaults to None. Returns: DataFrame \u2013 pd.DataFrame Source code in src/data_ingest.py 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 def csv_to_df ( file_nm : str , csv_path : PathLike , dtypes : Optional [ Dict ], persistent_exists = True , zip_url = None ) -> pd . DataFrame : \"\"\"Creates pandas DataFrame from csv; optionally using datatypes & selected columns. Sub func of both any_to_pd and _import_extract_delete_zip. Args: file_nm (str): The name of the source csv without the extension. e.g. \"stops\", not \"stops.csv\". csv_path (PathLike): The path/to/csv_file on local machine. dtypes (Optional[Dict]): Datatypes of columns in the csv. Helps optimise import. persistent_exists (bool, optional): Boolean supplied by the _persistent_exists function. Defaults to True. zip_url (str, optional): URL for the zip resource if it is to be downloaded. Defaults to None. Returns: pd.DataFrame \"\"\" print ( f \"Reading { file_nm } .csv from { csv_path } .\" ) if dtypes : cols = list ( dtypes . keys ()) tic = perf_counter () pd_df = pd . read_csv ( csv_path , usecols = cols , dtype = dtypes , encoding_errors = \"ignore\" ) toc = perf_counter () print ( f \"Time taken for csv reading is { toc - tic : .2f } seconds\" ) else : pd_df = pd . read_csv ( csv_path ) # Calling the pd_to_feather function to make a persistent feather file # for faster retrieval pd_to_feather ( pd_df , csv_path ) return pd_df delete_junk ( file_nm , zip_path ) Used by _import_extract_delete_zip function to delete the zip file after it was downloaded and the needed data extracted it. Parameters: file_nm ( str ) \u2013 the name of the file without extension. zip_path ( PathLike ) \u2013 path/to/local/zip/file.zip that is to deleted. Source code in src/data_ingest.py 296 297 298 299 300 301 302 303 304 305 306 307 def delete_junk ( file_nm : str , zip_path : PathLike ): \"\"\"Used by _import_extract_delete_zip function to delete the zip file after it was downloaded and the needed data extracted it. Args: file_nm (str): the name of the file without extension. zip_path (PathLike): path/to/local/zip/file.zip that is to deleted. \"\"\" # Delete the zipfile as it's uneeded now print ( f \"Deleting { file_nm } from { zip_path } \" ) os . remove ( zip_path ) dl_stops_make_df ( today , url ) Downloads the latest data from api, saves as csv & feather, returns df Parameters: today ( str ) \u2013 todays date url ( str ) \u2013 API URL Returns: \u2013 pd.DataFrame: df of latest stops data Source code in src/data_ingest.py 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 def dl_stops_make_df ( today , url ): \"\"\"Downloads the latest data from api, saves as csv & feather, returns df Args: today (str): todays date url (str): API URL Returns: pd.DataFrame: df of latest stops data \"\"\" csv_path = os . path . join ( os . getcwd (), \"data\" , \"stops\" , f \"stops_ { today } .csv\" ) # Save latest data as csv _get_stops_from_api ( url , csv_path ) # Save as feather feather_path = save_latest_stops_as_feather ( csv_path ) # Load feather as pd df stops_df = pd . read_feather ( feather_path ) return stops_df download_shp_data ( file_path_to_get ) This function downloads data from the cloud so we can process shapefiles. Parameters: file_path_to_get ( str ) \u2013 the filepath/folder we want to download data from Returns: \u2013 None, it will download the data locally. Source code in src/data_ingest.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def download_shp_data ( file_path_to_get ): \"\"\"This function downloads data from the cloud so we can process shapefiles. Args: file_path_to_get (str): the filepath/folder we want to download data from Returns: None, it will download the data locally. \"\"\" if CLOUD_LOCAL == \"cloud\" : lst = bucket . get_file_names () file_paths = [ x for x in lst if str ( file_path_to_get ) in x ] for file in file_paths : bucket . download_file ( file , os . path . join ( file )) else : pass extract_zip ( file_nm , csv_nm , zip_path , csv_path ) Used by _import_extract_delete_zip function to extract the needed csv dataset from the locally stored zip file. Parameters: file_nm ( str ) \u2013 the name of the file without extension. csv_nm ( str ) \u2013 the name of the csv file that is expected inside the zip file. zip_path ( PathLike ) \u2013 path/to/local/zip/file.zip. csv_path ( PathLike ) \u2013 The path where the csv should be written to, e.g. /data/. Source code in src/data_ingest.py 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 def extract_zip ( file_nm : str , csv_nm : str , zip_path : PathLike , csv_path : PathLike ): \"\"\"Used by _import_extract_delete_zip function to extract the needed csv dataset from the locally stored zip file. Args: file_nm (str): the name of the file without extension. csv_nm (str): the name of the csv file that is expected inside the zip file. zip_path (PathLike): path/to/local/zip/file.zip. csv_path (PathLike): The path where the csv should be written to, e.g. /data/. \"\"\" # Open the zip file and extract with ZipFile ( zip_path , 'r' ) as zip : print ( f \"Extracting { csv_nm } from { zip_path } \" ) try : _ = zip . extract ( csv_nm , csv_path ) except BaseException : csv_nm feath_to_df ( file_nm , feather_path ) Feather reading function used by the any_to_pd function. Parameters: file_nm ( str ) \u2013 the name of the file without extension. feather_path ( PathLike ) \u2013 the path/to/the/featherfile. Returns: DataFrame \u2013 pd.DataFrame: Pandas dataframe read from the persistent feather file. Source code in src/data_ingest.py 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 def feath_to_df ( file_nm : str , feather_path : PathLike ) -> pd . DataFrame : \"\"\"Feather reading function used by the any_to_pd function. Args: file_nm (str): the name of the file without extension. feather_path (PathLike): the path/to/the/featherfile. Returns: pd.DataFrame: Pandas dataframe read from the persistent feather file. \"\"\" print ( f \"Reading { file_nm } .feather from { feather_path } .\" ) # check if supplied path is a directory or a file if os . path . isdir ( feather_path ): # if directory then append the file name feather_path = os . path . join ( feather_path , f \" { file_nm } .feather\" ) # Time the read tic = perf_counter () pd_df = pd . read_feather ( feather_path ) toc = perf_counter () print ( f \"\"\"Time taken for { file_nm } .feather reading is { toc - tic : .2f } seconds\"\"\" ) return pd_df geo_df_from_geospatialfile ( path_to_file , crs = 'epsg:27700' ) Function to create a Geo-dataframe from a geospatial (geojson, shp) file. The process goes via Pandas.s Parameters: path_to_file ( str ) \u2013 path to the geojson, shp and other geospatial data files. Returns: \u2013 Geopandas Dataframe Source code in src/data_ingest.py 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 def geo_df_from_geospatialfile ( path_to_file , crs = 'epsg:27700' ): \"\"\"Function to create a Geo-dataframe from a geospatial (geojson, shp) file. The process goes via Pandas.s Args: path_to_file (str): path to the geojson, shp and other geospatial data files. Returns: Geopandas Dataframe \"\"\" geo_df = gpd . read_file ( path_to_file ) if geo_df . crs is None : geo_df . crs = 'epsg:27700' elif geo_df . crs != crs : geo_df = geo_df . to_crs ( 'epsg:27700' ) return geo_df get_abspath_or_list_files ( dir , list_or_abs , extension ) Takes a directory as str and returns the absolute path of output area csv file. Parameters: dir ( str ) \u2013 Path created with os.path.join. list_or_abs ( str ) \u2013 either \"list\" or \"abs\" to return either a list of paths/urls or a single path/url extension ( str ) \u2013 a file extension, e.g. \"csv\" to search for files of that type in the specified directory Returns: \u2013 Union [str, List]: Absolute path or list of the paths of file of the the given extension in the directory provided. Source code in src/data_ingest.py 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 def get_abspath_or_list_files ( dir , list_or_abs , extension ): \"\"\"Takes a directory as str and returns the absolute path of output area csv file. Args: dir (str): Path created with os.path.join. list_or_abs (str): either \"list\" or \"abs\" to return either a list of paths/urls or a single path/url extension (str): a file extension, e.g. \"csv\" to search for files of that type in the specified directory Returns: Union [str, List]: Absolute path or list of the paths of file of the the given extension in the directory provided. \"\"\" make_non_existent_folder ( dir ) files = os . listdir ( dir ) csv_files = [ file for file in files if file . endswith ( f \". { extension } \" )] if csv_files : if list_or_abs == \"list\" : list_of_paths = [ os . path . join ( dir , csv_file ) for csv_file in csv_files ] return list_of_paths elif list_or_abs == \"abs\" : csv_file = csv_files [ 0 ] absolute_path = os . path . join ( dir , csv_file ) return absolute_path else : blob_list = list ( bucket . bucket . list_blobs ( prefix = dir )) csv_files = [ file for file in blob_list if file . name . endswith ( f \". { extension } \" )] if list_or_abs == \"list\" : list_of_urls = [ path_or_url ( csv_file . name ) for csv_file in csv_files ] return list_of_urls elif list_or_abs == \"abs\" : csv_file = csv_files [ 0 ] . name absolute_path = path_or_url ( csv_file ) return absolute_path get_and_save_geo_dataset ( url , localpath , filename ) Fetches a geodataset in json format from a web resource and saves it to the local data/ directory and returns the json as a dict into memory. Parameters: filename ( str ) \u2013 the name of file as it should be saved locally. url ( str ) \u2013 URL of the web resource where json file is hosted. localpath ( str ) \u2013 path to folder where json is to be saved locally. Returns: \u2013 dict Source code in src/data_ingest.py 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 def get_and_save_geo_dataset ( url , localpath , filename ): \"\"\"Fetches a geodataset in json format from a web resource and saves it to the local data/ directory and returns the json as a dict into memory. Args: filename (str): the name of file as it should be saved locally. url (str): URL of the web resource where json file is hosted. localpath (str): path to folder where json is to be saved locally. Returns: dict \"\"\" file = requests . get ( url ) . json () full_path = os . path . join ( localpath , filename ) with open ( full_path , 'w' ) as dset : json . dump ( file , dset ) return file get_shp_abs_path ( dir ) Passed a directory into the function and returns the absolute path of where the shp file is within that directory. Parameters: dir ( PathLike ) \u2013 /path/to/directory/of/shape_files. Returns: str \u2013 the absolute path of the .shp file within a directory. Source code in src/data_ingest.py 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 def get_shp_abs_path ( dir ): \"\"\"Passed a directory into the function and returns the absolute path of where the shp file is within that directory. Args: dir (PathLike): /path/to/directory/of/shape_files. Returns: str: the absolute path of the .shp file within a directory. \"\"\" files = os . listdir ( dir ) shp_files = [ file for file in files if file . endswith ( \".shp\" )] # Add warning if there isn't a shp file in the directory if len ( shp_files ) == 0 : raise ValueError ( f \"No .shp file in directory { dir } \" ) shp_file = shp_files [ 0 ] absolute_path = os . path . join ( dir , shp_file ) return absolute_path get_stops_file ( url , dir ) Gets the latest stop dataset. If the latest stop df from the api is older then 28 days then function grabs a new version of file from API and saves this as a feather file. If the latest stop df from the api is less then 28 days old then just grabs the feather file. Parameters: url ( str ) \u2013 NAPTAN API url. dir ( str ) \u2013 directory where the stop data is stored. Returns: \u2013 pd.DataFrame Source code in src/data_ingest.py 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 def get_stops_file ( url , dir ): \"\"\"Gets the latest stop dataset. If the latest stop df from the api is older then 28 days then function grabs a new version of file from API and saves this as a feather file. If the latest stop df from the api is less then 28 days old then just grabs the feather file. Args: url (str): NAPTAN API url. dir (str): directory where the stop data is stored. Returns: pd.DataFrame \"\"\" # gets todays date and latest date of stops df today = int ( datetime . now () . strftime ( '%Y%m %d ' )) # gets feather stop path feather_path = os . path . join ( os . getcwd (), \"data\" , \"stops\" , \"Stops.feather\" ) # Check that the feather exists if not persistent_exists ( feather_path ): print ( f \"Downloading stops file from { url } \" ) stops_df = dl_stops_make_df ( today , url ) else : # does exist latest_date = _get_latest_stop_file_date ( dir ) if today - latest_date < 28 : stops_df = pd . read_feather ( feather_path ) else : stops_df = dl_stops_make_df ( today , url ) return stops_df get_whole_nation_pop_df ( pop_files , pop_year ) Gets the population data for all regions in the country and puts them into one dataframe. Parameters: pop_files ( list ) \u2013 Population data to be unioned. pop_year ( str ) \u2013 The year of population estimation data to process. Returns: \u2013 pd.DataFrame: Dataframe of population data for all regions in the country Source code in src/data_ingest.py 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 def get_whole_nation_pop_df ( pop_files , pop_year ): \"\"\"Gets the population data for all regions in the country and puts them into one dataframe. Args: pop_files (list): Population data to be unioned. pop_year (str): The year of population estimation data to process. Returns: pd.DataFrame: Dataframe of population data for all regions in the country \"\"\" # Remove gitkeep file from list of pop files pop_files = [ f for f in pop_files if f != '.gitkeep' ] # Dict of region:file_name. Capture the region name from the filename region_dict = { capture_region ( file ): file for file in pop_files } # make a df of each region then concat national_pop_feather_path = os . path . join ( DATA_DIR , f \"whole_nation_ { pop_year } .feather\" ) if not os . path . exists ( national_pop_feather_path ): print ( f \"No national_pop_feather found for { pop_year } \" ) print ( f \"Rebuilding population file. This will take a while!\" ) region_dfs_dict = {} for region in region_dict : print ( f \"Reading { region } Excel file\" ) xls_path = region_dict [ region ] # Read Excel file as object xlFile = pd . ExcelFile ( xls_path ) # Access sheets in Excel file total_cols = [ \"OA11CD\" , \"All Ages\" ] . append ( config [ \"age_lst\" ]) total_pop = pd . read_excel ( xlFile , f \"Mid- { pop_year } Persons\" , header = 4 , usecols = total_cols ) males_pop = pd . read_excel ( xlFile , f \"Mid- { pop_year } Males\" , header = 4 , usecols = [ \"OA11CD\" , \"All Ages\" ]) fem_pop = pd . read_excel ( xlFile , f \"Mid- { pop_year } Females\" , header = 4 , usecols = [ \"OA11CD\" , \"All Ages\" ]) # Rename the \"All Ages\" columns appropriately before concating total_pop . rename ( columns = { \"All Ages\" : \"pop_count\" }, inplace = True ) males_pop . rename ( columns = { \"All Ages\" : \"males_pop\" }, inplace = True ) fem_pop . rename ( columns = { \"All Ages\" : \"fem_pop\" }, inplace = True ) # Merge the data from different sheets dfs_to_merge = [ total_pop , males_pop , fem_pop ] df_final = reduce ( lambda left , right : pd . merge ( left , right , on = 'OA11CD' ), dfs_to_merge ) # Store merged df in dict under region name region_dfs_dict [ region ] = df_final # Concat all regions into national pop df whole_nation_pop_df = pd . concat ( region_dfs_dict . values ()) # Create the pop_year column to show which year the data is from whole_nation_pop_df [ \"pop_year\" ] = pop_year # Change all column names to str whole_nation_pop_df . columns = whole_nation_pop_df . columns . astype ( str ) # Write df out to feather for quicker reading print ( \"Writing whole_nation_pop_df.feather\" ) whole_nation_pop_df . reset_index () . to_feather ( national_pop_feather_path ) else : # if it exists, read from a feather for quicker data retreval print ( f \"Reading whole_nation_pop_df from { national_pop_feather_path } \" ) whole_nation_pop_df = pd . read_feather ( national_pop_feather_path ) # Temporary TODO: remove this line whole_nation_pop_df . rename ( columns = { \"total_pop\" : \"pop_count\" }, inplace = True ) return whole_nation_pop_df grab_zip ( file_nm , zip_link , zip_path ) Used by _import_extract_delete_zip function to download a zip file from the URI specified in the the zip_link parameter. Parameters: file_nm ( str ) \u2013 the name of the file without extension. zip_link ( str ) \u2013 URI to the zip to be downloaded. zip_path ( PathLike ) \u2013 path/to/the/write/directory, e.g. '/data/'. Source code in src/data_ingest.py 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 def grab_zip ( file_nm : str , zip_link , zip_path : PathLike ): \"\"\"Used by _import_extract_delete_zip function to download a zip file from the URI specified in the the zip_link parameter. Args: file_nm (str): the name of the file without extension. zip_link (str): URI to the zip to be downloaded. zip_path (PathLike): path/to/the/write/directory, e.g. '/data/'. \"\"\" # Grab the zipfile from URI print ( f \"Downloading { file_nm } from { zip_link } \" ) r = requests . get ( zip_link ) with open ( zip_path , 'wb' ) as output_file : print ( f \"Saving { file_nm } to { zip_path } \" ) output_file . write ( r . content ) import_extract_delete_zip ( file_nm , zip_path , persistent_exists = True , zip_url = None , * cols , ** dtypes ) Downloads and opens zip file, extracts contents, deletes zip. Subfunc of any_to_pd. Parameters: file_nm ( str ) \u2013 The name of the target csv within the zip. zip_path ( PathLike ) \u2013 the path/to/zip_file on local machine persistent_exists ( bool , default: True ) \u2013 Boolean supplied by the _persistent_exists function. Defaults to True. zip_url ( str , default: None ) \u2013 URL for the zip resource if it is to be downloaded. Defaults to None. Returns: DataFrame \u2013 pd.DataFrame: dataframe of the data from the CSV Source code in src/data_ingest.py 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 def import_extract_delete_zip ( file_nm : str , zip_path : PathLike , persistent_exists = True , zip_url = None , * cols , ** dtypes ) -> pd . DataFrame : \"\"\"Downloads and opens zip file, extracts contents, deletes zip. Subfunc of any_to_pd. Args: file_nm (str): The name of the target csv within the zip. zip_path (PathLike): the path/to/zip_file on local machine persistent_exists (bool, optional): Boolean supplied by the _persistent_exists function. Defaults to True. zip_url (str, optional): URL for the zip resource if it is to be downloaded. Defaults to None. Returns: pd.DataFrame: dataframe of the data from the CSV \"\"\" if not persistent_exists : # checking if a persistent zip exists to save downloading grab_zip ( file_nm , zip_url , zip_path ) csv_nm = file_nm + \".csv\" csv_path = _make_data_path ( \"data\" , csv_nm ) extract_zip ( file_nm , csv_nm , zip_path , csv_path ) delete_junk ( file_nm , zip_path ) pd_df = csv_to_df ( file_nm , csv_path , dtypes ) return pd_df make_non_existent_folder ( folder_path ) Creates a folder if it doesn't exist. Source code in src/data_ingest.py 142 143 144 145 146 147 148 149 150 151 152 153 def make_non_existent_folder ( folder_path : pl . Path ) -> None : \"\"\"Creates a folder if it doesn't exist.\"\"\" # Check if folder_path is str if isinstance ( folder_path , str ): folder_path = pl . Path ( folder_path ) if not folder_path . exists (): MainLogger . info ( f \"Folder { folder_path } does not exist. Creating it now.\" ) folder_path . mkdir ( parents = True ) # Add a .gitkeep file to the folder so it gets pushed to GitHub ( folder_path / \".gitkeep\" ) . touch () return None path_or_url ( file_path ) Function to read data depending on config if cloud or local. Parameters: file_path ( str ) \u2013 File path you want to read Returns: \u2013 file_path or url depending if cloud or local Source code in src/data_ingest.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 def path_or_url ( file_path ): \"\"\"Function to read data depending on config if cloud or local. Args: file_path (str): File path you want to read Returns: file_path or url depending if cloud or local \"\"\" if CLOUD_LOCAL == \"cloud\" : url = bucket . generate_signed_url ( file_path ) return url elif CLOUD_LOCAL == \"local\" : return file_path else : MainLogger . error ( \"Cloud or local configuration is incorrect\" ) raise ImportError pd_to_feather ( pd_df , current_file_path ) Used by the any_to_pd function to writes a Pandas dataframe to feather for quick reading and retrieval later. This function returns nothing because it simply writes to disk. Parameters: pd_df ( DataFrame ) \u2013 a pandas dataframe to be converted current_file_path ( PathLike ) \u2013 The path/to/current_file on local machine. Source code in src/data_ingest.py 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 def pd_to_feather ( pd_df : pd . DataFrame , current_file_path : PathLike ): \"\"\"Used by the any_to_pd function to writes a Pandas dataframe to feather for quick reading and retrieval later. This function returns nothing because it simply writes to disk. Args: pd_df (pd.DataFrame): a pandas dataframe to be converted current_file_path (PathLike): The path/to/current_file on local machine. \"\"\" feather_path = os . path . splitext ( current_file_path )[ 0 ] + '.feather' # TODO: this function should make use of _persistent_existsD if not os . path . isfile ( feather_path ): print ( f \"Writing Pandas dataframe to feather at { feather_path } \" ) feather . write_feather ( pd_df , feather_path ) print ( \"Feather already exists\" ) persistent_exists ( persistent_path ) cached Checks if a persistent file or directory already exists or not. Parameters: persistent_path ( PathLike ) \u2013 path to check. Returns: bool \u2013 True if a persistent file or directory already exists. Source code in src/data_ingest.py 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 @lru_cache def persistent_exists ( persistent_path ): \"\"\"Checks if a persistent file or directory already exists or not. Args: persistent_path (PathLike): path to check. Returns: bool: True if a persistent file or directory already exists. \"\"\" # Change directory into project root os . chdir ( CWD ) if os . path . exists ( persistent_path ): print ( f \" { persistent_path } already exists\" ) return True else : print ( f \" { persistent_path } does not exist\" ) return False read_file_if_exists ( file_path , read_func ) Checks if a file exists and reads it if it does using a function supplied. Parameters: file_path ( str ) \u2013 A path to a file created by os.path.join read_func ( func ) \u2013 A function to read the file. Raises: FileNotFoundError \u2013 description Returns: _type_ \u2013 description Source code in src/data_ingest.py 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 def read_file_if_exists ( file_path , read_func ): \"\"\"Checks if a file exists and reads it if it does using a function supplied. Args: file_path (str): A path to a file created by os.path.join read_func (func): A function to read the file. Raises: FileNotFoundError: _description_ Returns: _type_: _description_ \"\"\" if persistent_exists ( file_path ): return read_func ( file_path ) else : raise FileNotFoundError ( f \"File not found: { file_path } . Run preprocessing first.\" ) read_ni_age_df ( path ) Reads in the nothern ireland age information Parameters: path ( str ) \u2013 the path of the file Returns: \u2013 pd.DataFrame the age_df dataframe Source code in src/data_ingest.py 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 def read_ni_age_df ( path ): \"\"\" Reads in the nothern ireland age information Args: path (str): the path of the file Returns: pd.DataFrame the age_df dataframe \"\"\" # read in age df age_df = pd . read_excel ( path_or_url ( path ), sheet_name = \"SA\" , header = 5 , index_col = \"SA Code\" ) # replace col names from string e.g \"Age 82\" # to just the number 82 for col in age_df . columns : if bool ( re . search ( r \"\\d\" , col )): number = re . findall ( r \"[0-9]{1,3}\" , col )[ 0 ] age_df . rename ( columns = { col : number }, inplace = True ) return age_df read_scottish_age ( path ) Reads in the scottish age information Parameters: path ( str ) \u2013 the path of the file Returns: Bool \u2013 True if path has not been modified within the number of days specified Source code in src/data_ingest.py 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 def read_scottish_age ( path ): \"\"\" Reads in the scottish age information Args: path (str): the path of the file Returns: Bool: True if path has not been modified within the number of days specified \"\"\" # read in scottish file age_scotland_df = pd . read_csv ( path_or_url ( path ), skiprows = 4 ) # dropping first row as this is the whole of scotland age_scotland_df = age_scotland_df . iloc [ 1 : - 4 , :] age_scotland_df = age_scotland_df . reset_index () return age_scotland_df read_urb_rur_class_scotland ( urb_rur_path ) Reads the urb/rural classification for Scotland. This reads in the file containing all the urban/rural class Then applies a mapping based on if living in a settlement >10,000 then urban, else rural. Parameters: path ( str ) \u2013 the path where the file exists Returns: \u2013 pd.DataFrame: the classfication dataframe Source code in src/data_ingest.py 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 def read_urb_rur_class_scotland ( urb_rur_path ): \"\"\"Reads the urb/rural classification for Scotland. This reads in the file containing all the urban/rural class Then applies a mapping based on if living in a settlement >10,000 then urban, else rural. Args: path (str): the path where the file exists Returns: pd.DataFrame: the classfication dataframe \"\"\" urb_rur = pd . read_csv ( path_or_url ( urb_rur_path ), usecols = [ \"OA2011\" , \"UR6_2013_2014\" ]) urb_rur [ \"urb_rur_class\" ] = np . where ( ( urb_rur [ \"UR6_2013_2014\" ] == 1 ) | ( urb_rur [ \"UR6_2013_2014\" ] == 2 ), \"urban\" , \"rural\" ) return urb_rur read_urb_rur_ni ( urb_rur_path ) Reads small areas as urban or rural for Northern Ireland. Urban is classified as living in a settlement >10,000, and rural is classified as <10,000. Args: path (str): the path where the file exists Returns: \u2013 pd.DataFrame: the urban rural classification dataframe Source code in src/data_ingest.py 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 def read_urb_rur_ni ( urb_rur_path ): \"\"\" Reads small areas as urban or rural for Northern Ireland. Urban is classified as living in a settlement >10,000, and rural is classified as <10,000. Args: path (str): the path where the file exists Returns: pd.DataFrame: the urban rural classification dataframe \"\"\" urb_rur = pd . read_csv ( path_or_url ( urb_rur_path ), skiprows = 3 ) urb_rur = urb_rur [[ 'SA2011_Code' , 'Settlement Classification Band' ]] # split classification bands into urban and rural urb_list = [ 'A' , 'B' , 'C' , 'D' ] rural_list = [ 'E' , 'F' , 'G' , 'H' ] urb_rur . loc [ urb_rur [ 'Settlement Classification Band' ] . isin ( urb_list ), 'urb_rur_class' ] = 'urban' urb_rur . loc [ urb_rur [ 'Settlement Classification Band' ] . isin ( rural_list ), 'urb_rur_class' ] = 'rural' return urb_rur read_usual_pop_scotland ( path ) Reads the usual population Scotland. This reads in the file containing all the output areas in scotland with their corresponding population number. Parameters: path ( str ) \u2013 the path where the file exists Returns: \u2013 pd.DataFrame the usual population dataframe Source code in src/data_ingest.py 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 def read_usual_pop_scotland ( path : str ): \"\"\"Reads the usual population Scotland. This reads in the file containing all the output areas in scotland with their corresponding population number. Args: path (str): the path where the file exists Returns: pd.DataFrame the usual population dataframe \"\"\" # reads in data and crops of header and footer df = pd . read_csv ( path_or_url ( path ), header = 4 , index_col = 0 , skipfooter = 4 ) # only want OA so drop Scotland df_oa_only = df . drop ( index = [ 'Scotland' ]) # only use columns that we need essential_cols = [ \"All people\" , \"Males\" , \"Females\" ] df_essential_cols = df_oa_only [ essential_cols ] # ensure no commas in the dataset so no errors with dtypes for col in essential_cols : df_essential_cols [ col ] = df_essential_cols [ col ] . str . replace ( ',' , '' ) df_essential_cols [ col ] = df_essential_cols [ col ] . astype ( int ) return df_essential_cols save_latest_stops_as_feather ( file_name ) Saves the latest stop file as a feather file into the data folder. Parameters: file_name ( str ) \u2013 file path for latest stop file. Source code in src/data_ingest.py 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 def save_latest_stops_as_feather ( file_name ): \"\"\"Saves the latest stop file as a feather file into the data folder. Args: file_name (str): file path for latest stop file. \"\"\" # read in csv file = pd . read_csv ( file_name , usecols = config [ \"naptan_types\" ] . keys (), dtype = config [ \"naptan_types\" ]) # get output path output_path = os . path . join ( os . getcwd (), \"data\" , \"stops\" , \"Stops.feather\" ) # output to feather file . to_feather ( output_path ) return output_path","title":"Data ingest"},{"location":"data_ingest/#src.data_ingest.GCPBucket","text":"This class sets up an instance of a GCP storage client with the right credentials. The methods either download a file from the bucket or generate a signed url for a file. Source code in src/data_ingest.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 class GCPBucket : \"\"\" This class sets up an instance of a GCP storage client with the right credentials. The methods either download a file from the bucket or generate a signed url for a file. \"\"\" def __init__ ( self ): self . credentials = service_account . Credentials . from_service_account_file ( glob . glob ( 'secrets/*.json' )[ 0 ]) self . client = storage . Client ( credentials = self . credentials ) self . bucket_name = '11-2-1-all-data' self . bucket = self . client . bucket ( self . bucket_name ) def download_file ( self , file_name , destination_file_name ): \"\"\"Downloads a blob from the bucket.\"\"\" blob = self . bucket . blob ( file_name ) # Create the folder (if it doesn't exist) folder_path = os . path . dirname ( destination_file_name ) make_non_existent_folder ( folder_path ) blob . download_to_filename ( destination_file_name ) print ( f \"Blob { self . client } downloaded to { destination_file_name } .\" ) def generate_signed_url ( self , object_name ): \"\"\" This will generate a signed URL that is valid for 5 minutes. The URL should be able to be used by our data ingest functions to download any file.\"\"\" expiration = datetime . utcnow () + timedelta ( hours = 1 ) url = self . bucket . blob ( object_name ) . generate_signed_url ( expiration , method = 'GET' , credentials = self . credentials , ) return url def get_file_list ( self ): \"\"\"This will return a list of all the files in the bucket, as \"blobs\".\"\"\" files = self . bucket . list_blobs () return files def get_file_names ( self ): \"\"\"Prints and returns a list of all the file names in the bucket. Returns: List: List of file names in the bucket. \"\"\" files = self . bucket . list_blobs () file_names = [ file . name for file in files ] print ( f \"Found { len ( file_names ) } files in the bucket.\" ) #for file in file_names: #print(file) return file_names","title":"GCPBucket"},{"location":"data_ingest/#src.data_ingest.GCPBucket.download_file","text":"Downloads a blob from the bucket. Source code in src/data_ingest.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def download_file ( self , file_name , destination_file_name ): \"\"\"Downloads a blob from the bucket.\"\"\" blob = self . bucket . blob ( file_name ) # Create the folder (if it doesn't exist) folder_path = os . path . dirname ( destination_file_name ) make_non_existent_folder ( folder_path ) blob . download_to_filename ( destination_file_name ) print ( f \"Blob { self . client } downloaded to { destination_file_name } .\" )","title":"download_file"},{"location":"data_ingest/#src.data_ingest.GCPBucket.generate_signed_url","text":"This will generate a signed URL that is valid for 5 minutes. The URL should be able to be used by our data ingest functions to download any file. Source code in src/data_ingest.py 68 69 70 71 72 73 74 75 76 77 78 79 def generate_signed_url ( self , object_name ): \"\"\" This will generate a signed URL that is valid for 5 minutes. The URL should be able to be used by our data ingest functions to download any file.\"\"\" expiration = datetime . utcnow () + timedelta ( hours = 1 ) url = self . bucket . blob ( object_name ) . generate_signed_url ( expiration , method = 'GET' , credentials = self . credentials , ) return url","title":"generate_signed_url"},{"location":"data_ingest/#src.data_ingest.GCPBucket.get_file_list","text":"This will return a list of all the files in the bucket, as \"blobs\". Source code in src/data_ingest.py 81 82 83 84 85 86 def get_file_list ( self ): \"\"\"This will return a list of all the files in the bucket, as \"blobs\".\"\"\" files = self . bucket . list_blobs () return files","title":"get_file_list"},{"location":"data_ingest/#src.data_ingest.GCPBucket.get_file_names","text":"Prints and returns a list of all the file names in the bucket. Returns: List \u2013 List of file names in the bucket. Source code in src/data_ingest.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def get_file_names ( self ): \"\"\"Prints and returns a list of all the file names in the bucket. Returns: List: List of file names in the bucket. \"\"\" files = self . bucket . list_blobs () file_names = [ file . name for file in files ] print ( f \"Found { len ( file_names ) } files in the bucket.\" ) #for file in file_names: #print(file) return file_names","title":"get_file_names"},{"location":"data_ingest/#src.data_ingest.best_before","text":"Checks whether a path has been modified within a period of days. Parameters: path ( str ) \u2013 the path to check number_of_days ( int ) \u2013 number of days previous to check for modifications Returns: Bool \u2013 True if path has not been modified within the number of days specified Source code in src/data_ingest.py 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 def best_before ( path , number_of_days ): \"\"\" Checks whether a path has been modified within a period of days. Args: path (str): the path to check number_of_days (int): number of days previous to check for modifications Returns: Bool: True if path has not been modified within the number of days specified \"\"\" todays_date = datetime . today () last_modified_date = datetime . fromtimestamp ( os . path . getmtime ( path )) days_since_last_modification = ( todays_date - last_modified_date ) . days if days_since_last_modification > number_of_days : expired = True else : expired = False return expired","title":"best_before"},{"location":"data_ingest/#src.data_ingest.capture_region","text":"Extracts the region name from the ONS population estimate excel files. Parameters: file_nm ( str ) \u2013 Full name of the regional population Excel file. Returns: str \u2013 The name of the region that the file covers Source code in src/data_ingest.py 410 411 412 413 414 415 416 417 418 419 420 421 422 def capture_region ( file_nm : str ): \"\"\"Extracts the region name from the ONS population estimate excel files. Args: file_nm (str): Full name of the regional population Excel file. Returns: str: The name of the region that the file covers \"\"\" patt = re . compile ( \"^(.*estimates[-]?)(?P<region>.*)(\\.xls)\" ) region = re . search ( patt , file_nm ) . group ( \"region\" ) region = region . replace ( \"-\" , \" \" ) . capitalize () return region","title":"capture_region"},{"location":"data_ingest/#src.data_ingest.csv_to_df","text":"Creates pandas DataFrame from csv; optionally using datatypes & selected columns. Sub func of both any_to_pd and _import_extract_delete_zip. Parameters: file_nm ( str ) \u2013 The name of the source csv without the extension. e.g. \"stops\", not \"stops.csv\". csv_path ( PathLike ) \u2013 The path/to/csv_file on local machine. dtypes ( Optional [ Dict ] ) \u2013 Datatypes of columns in the csv. Helps optimise import. persistent_exists ( bool , default: True ) \u2013 Boolean supplied by the _persistent_exists function. Defaults to True. zip_url ( str , default: None ) \u2013 URL for the zip resource if it is to be downloaded. Defaults to None. Returns: DataFrame \u2013 pd.DataFrame Source code in src/data_ingest.py 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 def csv_to_df ( file_nm : str , csv_path : PathLike , dtypes : Optional [ Dict ], persistent_exists = True , zip_url = None ) -> pd . DataFrame : \"\"\"Creates pandas DataFrame from csv; optionally using datatypes & selected columns. Sub func of both any_to_pd and _import_extract_delete_zip. Args: file_nm (str): The name of the source csv without the extension. e.g. \"stops\", not \"stops.csv\". csv_path (PathLike): The path/to/csv_file on local machine. dtypes (Optional[Dict]): Datatypes of columns in the csv. Helps optimise import. persistent_exists (bool, optional): Boolean supplied by the _persistent_exists function. Defaults to True. zip_url (str, optional): URL for the zip resource if it is to be downloaded. Defaults to None. Returns: pd.DataFrame \"\"\" print ( f \"Reading { file_nm } .csv from { csv_path } .\" ) if dtypes : cols = list ( dtypes . keys ()) tic = perf_counter () pd_df = pd . read_csv ( csv_path , usecols = cols , dtype = dtypes , encoding_errors = \"ignore\" ) toc = perf_counter () print ( f \"Time taken for csv reading is { toc - tic : .2f } seconds\" ) else : pd_df = pd . read_csv ( csv_path ) # Calling the pd_to_feather function to make a persistent feather file # for faster retrieval pd_to_feather ( pd_df , csv_path ) return pd_df","title":"csv_to_df"},{"location":"data_ingest/#src.data_ingest.delete_junk","text":"Used by _import_extract_delete_zip function to delete the zip file after it was downloaded and the needed data extracted it. Parameters: file_nm ( str ) \u2013 the name of the file without extension. zip_path ( PathLike ) \u2013 path/to/local/zip/file.zip that is to deleted. Source code in src/data_ingest.py 296 297 298 299 300 301 302 303 304 305 306 307 def delete_junk ( file_nm : str , zip_path : PathLike ): \"\"\"Used by _import_extract_delete_zip function to delete the zip file after it was downloaded and the needed data extracted it. Args: file_nm (str): the name of the file without extension. zip_path (PathLike): path/to/local/zip/file.zip that is to deleted. \"\"\" # Delete the zipfile as it's uneeded now print ( f \"Deleting { file_nm } from { zip_path } \" ) os . remove ( zip_path )","title":"delete_junk"},{"location":"data_ingest/#src.data_ingest.dl_stops_make_df","text":"Downloads the latest data from api, saves as csv & feather, returns df Parameters: today ( str ) \u2013 todays date url ( str ) \u2013 API URL Returns: \u2013 pd.DataFrame: df of latest stops data Source code in src/data_ingest.py 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 def dl_stops_make_df ( today , url ): \"\"\"Downloads the latest data from api, saves as csv & feather, returns df Args: today (str): todays date url (str): API URL Returns: pd.DataFrame: df of latest stops data \"\"\" csv_path = os . path . join ( os . getcwd (), \"data\" , \"stops\" , f \"stops_ { today } .csv\" ) # Save latest data as csv _get_stops_from_api ( url , csv_path ) # Save as feather feather_path = save_latest_stops_as_feather ( csv_path ) # Load feather as pd df stops_df = pd . read_feather ( feather_path ) return stops_df","title":"dl_stops_make_df"},{"location":"data_ingest/#src.data_ingest.download_shp_data","text":"This function downloads data from the cloud so we can process shapefiles. Parameters: file_path_to_get ( str ) \u2013 the filepath/folder we want to download data from Returns: \u2013 None, it will download the data locally. Source code in src/data_ingest.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def download_shp_data ( file_path_to_get ): \"\"\"This function downloads data from the cloud so we can process shapefiles. Args: file_path_to_get (str): the filepath/folder we want to download data from Returns: None, it will download the data locally. \"\"\" if CLOUD_LOCAL == \"cloud\" : lst = bucket . get_file_names () file_paths = [ x for x in lst if str ( file_path_to_get ) in x ] for file in file_paths : bucket . download_file ( file , os . path . join ( file )) else : pass","title":"download_shp_data"},{"location":"data_ingest/#src.data_ingest.extract_zip","text":"Used by _import_extract_delete_zip function to extract the needed csv dataset from the locally stored zip file. Parameters: file_nm ( str ) \u2013 the name of the file without extension. csv_nm ( str ) \u2013 the name of the csv file that is expected inside the zip file. zip_path ( PathLike ) \u2013 path/to/local/zip/file.zip. csv_path ( PathLike ) \u2013 The path where the csv should be written to, e.g. /data/. Source code in src/data_ingest.py 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 def extract_zip ( file_nm : str , csv_nm : str , zip_path : PathLike , csv_path : PathLike ): \"\"\"Used by _import_extract_delete_zip function to extract the needed csv dataset from the locally stored zip file. Args: file_nm (str): the name of the file without extension. csv_nm (str): the name of the csv file that is expected inside the zip file. zip_path (PathLike): path/to/local/zip/file.zip. csv_path (PathLike): The path where the csv should be written to, e.g. /data/. \"\"\" # Open the zip file and extract with ZipFile ( zip_path , 'r' ) as zip : print ( f \"Extracting { csv_nm } from { zip_path } \" ) try : _ = zip . extract ( csv_nm , csv_path ) except BaseException : csv_nm","title":"extract_zip"},{"location":"data_ingest/#src.data_ingest.feath_to_df","text":"Feather reading function used by the any_to_pd function. Parameters: file_nm ( str ) \u2013 the name of the file without extension. feather_path ( PathLike ) \u2013 the path/to/the/featherfile. Returns: DataFrame \u2013 pd.DataFrame: Pandas dataframe read from the persistent feather file. Source code in src/data_ingest.py 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 def feath_to_df ( file_nm : str , feather_path : PathLike ) -> pd . DataFrame : \"\"\"Feather reading function used by the any_to_pd function. Args: file_nm (str): the name of the file without extension. feather_path (PathLike): the path/to/the/featherfile. Returns: pd.DataFrame: Pandas dataframe read from the persistent feather file. \"\"\" print ( f \"Reading { file_nm } .feather from { feather_path } .\" ) # check if supplied path is a directory or a file if os . path . isdir ( feather_path ): # if directory then append the file name feather_path = os . path . join ( feather_path , f \" { file_nm } .feather\" ) # Time the read tic = perf_counter () pd_df = pd . read_feather ( feather_path ) toc = perf_counter () print ( f \"\"\"Time taken for { file_nm } .feather reading is { toc - tic : .2f } seconds\"\"\" ) return pd_df","title":"feath_to_df"},{"location":"data_ingest/#src.data_ingest.geo_df_from_geospatialfile","text":"Function to create a Geo-dataframe from a geospatial (geojson, shp) file. The process goes via Pandas.s Parameters: path_to_file ( str ) \u2013 path to the geojson, shp and other geospatial data files. Returns: \u2013 Geopandas Dataframe Source code in src/data_ingest.py 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 def geo_df_from_geospatialfile ( path_to_file , crs = 'epsg:27700' ): \"\"\"Function to create a Geo-dataframe from a geospatial (geojson, shp) file. The process goes via Pandas.s Args: path_to_file (str): path to the geojson, shp and other geospatial data files. Returns: Geopandas Dataframe \"\"\" geo_df = gpd . read_file ( path_to_file ) if geo_df . crs is None : geo_df . crs = 'epsg:27700' elif geo_df . crs != crs : geo_df = geo_df . to_crs ( 'epsg:27700' ) return geo_df","title":"geo_df_from_geospatialfile"},{"location":"data_ingest/#src.data_ingest.get_abspath_or_list_files","text":"Takes a directory as str and returns the absolute path of output area csv file. Parameters: dir ( str ) \u2013 Path created with os.path.join. list_or_abs ( str ) \u2013 either \"list\" or \"abs\" to return either a list of paths/urls or a single path/url extension ( str ) \u2013 a file extension, e.g. \"csv\" to search for files of that type in the specified directory Returns: \u2013 Union [str, List]: Absolute path or list of the paths of file of the the given extension in the directory provided. Source code in src/data_ingest.py 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 def get_abspath_or_list_files ( dir , list_or_abs , extension ): \"\"\"Takes a directory as str and returns the absolute path of output area csv file. Args: dir (str): Path created with os.path.join. list_or_abs (str): either \"list\" or \"abs\" to return either a list of paths/urls or a single path/url extension (str): a file extension, e.g. \"csv\" to search for files of that type in the specified directory Returns: Union [str, List]: Absolute path or list of the paths of file of the the given extension in the directory provided. \"\"\" make_non_existent_folder ( dir ) files = os . listdir ( dir ) csv_files = [ file for file in files if file . endswith ( f \". { extension } \" )] if csv_files : if list_or_abs == \"list\" : list_of_paths = [ os . path . join ( dir , csv_file ) for csv_file in csv_files ] return list_of_paths elif list_or_abs == \"abs\" : csv_file = csv_files [ 0 ] absolute_path = os . path . join ( dir , csv_file ) return absolute_path else : blob_list = list ( bucket . bucket . list_blobs ( prefix = dir )) csv_files = [ file for file in blob_list if file . name . endswith ( f \". { extension } \" )] if list_or_abs == \"list\" : list_of_urls = [ path_or_url ( csv_file . name ) for csv_file in csv_files ] return list_of_urls elif list_or_abs == \"abs\" : csv_file = csv_files [ 0 ] . name absolute_path = path_or_url ( csv_file ) return absolute_path","title":"get_abspath_or_list_files"},{"location":"data_ingest/#src.data_ingest.get_and_save_geo_dataset","text":"Fetches a geodataset in json format from a web resource and saves it to the local data/ directory and returns the json as a dict into memory. Parameters: filename ( str ) \u2013 the name of file as it should be saved locally. url ( str ) \u2013 URL of the web resource where json file is hosted. localpath ( str ) \u2013 path to folder where json is to be saved locally. Returns: \u2013 dict Source code in src/data_ingest.py 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 def get_and_save_geo_dataset ( url , localpath , filename ): \"\"\"Fetches a geodataset in json format from a web resource and saves it to the local data/ directory and returns the json as a dict into memory. Args: filename (str): the name of file as it should be saved locally. url (str): URL of the web resource where json file is hosted. localpath (str): path to folder where json is to be saved locally. Returns: dict \"\"\" file = requests . get ( url ) . json () full_path = os . path . join ( localpath , filename ) with open ( full_path , 'w' ) as dset : json . dump ( file , dset ) return file","title":"get_and_save_geo_dataset"},{"location":"data_ingest/#src.data_ingest.get_shp_abs_path","text":"Passed a directory into the function and returns the absolute path of where the shp file is within that directory. Parameters: dir ( PathLike ) \u2013 /path/to/directory/of/shape_files. Returns: str \u2013 the absolute path of the .shp file within a directory. Source code in src/data_ingest.py 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 def get_shp_abs_path ( dir ): \"\"\"Passed a directory into the function and returns the absolute path of where the shp file is within that directory. Args: dir (PathLike): /path/to/directory/of/shape_files. Returns: str: the absolute path of the .shp file within a directory. \"\"\" files = os . listdir ( dir ) shp_files = [ file for file in files if file . endswith ( \".shp\" )] # Add warning if there isn't a shp file in the directory if len ( shp_files ) == 0 : raise ValueError ( f \"No .shp file in directory { dir } \" ) shp_file = shp_files [ 0 ] absolute_path = os . path . join ( dir , shp_file ) return absolute_path","title":"get_shp_abs_path"},{"location":"data_ingest/#src.data_ingest.get_stops_file","text":"Gets the latest stop dataset. If the latest stop df from the api is older then 28 days then function grabs a new version of file from API and saves this as a feather file. If the latest stop df from the api is less then 28 days old then just grabs the feather file. Parameters: url ( str ) \u2013 NAPTAN API url. dir ( str ) \u2013 directory where the stop data is stored. Returns: \u2013 pd.DataFrame Source code in src/data_ingest.py 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 def get_stops_file ( url , dir ): \"\"\"Gets the latest stop dataset. If the latest stop df from the api is older then 28 days then function grabs a new version of file from API and saves this as a feather file. If the latest stop df from the api is less then 28 days old then just grabs the feather file. Args: url (str): NAPTAN API url. dir (str): directory where the stop data is stored. Returns: pd.DataFrame \"\"\" # gets todays date and latest date of stops df today = int ( datetime . now () . strftime ( '%Y%m %d ' )) # gets feather stop path feather_path = os . path . join ( os . getcwd (), \"data\" , \"stops\" , \"Stops.feather\" ) # Check that the feather exists if not persistent_exists ( feather_path ): print ( f \"Downloading stops file from { url } \" ) stops_df = dl_stops_make_df ( today , url ) else : # does exist latest_date = _get_latest_stop_file_date ( dir ) if today - latest_date < 28 : stops_df = pd . read_feather ( feather_path ) else : stops_df = dl_stops_make_df ( today , url ) return stops_df","title":"get_stops_file"},{"location":"data_ingest/#src.data_ingest.get_whole_nation_pop_df","text":"Gets the population data for all regions in the country and puts them into one dataframe. Parameters: pop_files ( list ) \u2013 Population data to be unioned. pop_year ( str ) \u2013 The year of population estimation data to process. Returns: \u2013 pd.DataFrame: Dataframe of population data for all regions in the country Source code in src/data_ingest.py 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 def get_whole_nation_pop_df ( pop_files , pop_year ): \"\"\"Gets the population data for all regions in the country and puts them into one dataframe. Args: pop_files (list): Population data to be unioned. pop_year (str): The year of population estimation data to process. Returns: pd.DataFrame: Dataframe of population data for all regions in the country \"\"\" # Remove gitkeep file from list of pop files pop_files = [ f for f in pop_files if f != '.gitkeep' ] # Dict of region:file_name. Capture the region name from the filename region_dict = { capture_region ( file ): file for file in pop_files } # make a df of each region then concat national_pop_feather_path = os . path . join ( DATA_DIR , f \"whole_nation_ { pop_year } .feather\" ) if not os . path . exists ( national_pop_feather_path ): print ( f \"No national_pop_feather found for { pop_year } \" ) print ( f \"Rebuilding population file. This will take a while!\" ) region_dfs_dict = {} for region in region_dict : print ( f \"Reading { region } Excel file\" ) xls_path = region_dict [ region ] # Read Excel file as object xlFile = pd . ExcelFile ( xls_path ) # Access sheets in Excel file total_cols = [ \"OA11CD\" , \"All Ages\" ] . append ( config [ \"age_lst\" ]) total_pop = pd . read_excel ( xlFile , f \"Mid- { pop_year } Persons\" , header = 4 , usecols = total_cols ) males_pop = pd . read_excel ( xlFile , f \"Mid- { pop_year } Males\" , header = 4 , usecols = [ \"OA11CD\" , \"All Ages\" ]) fem_pop = pd . read_excel ( xlFile , f \"Mid- { pop_year } Females\" , header = 4 , usecols = [ \"OA11CD\" , \"All Ages\" ]) # Rename the \"All Ages\" columns appropriately before concating total_pop . rename ( columns = { \"All Ages\" : \"pop_count\" }, inplace = True ) males_pop . rename ( columns = { \"All Ages\" : \"males_pop\" }, inplace = True ) fem_pop . rename ( columns = { \"All Ages\" : \"fem_pop\" }, inplace = True ) # Merge the data from different sheets dfs_to_merge = [ total_pop , males_pop , fem_pop ] df_final = reduce ( lambda left , right : pd . merge ( left , right , on = 'OA11CD' ), dfs_to_merge ) # Store merged df in dict under region name region_dfs_dict [ region ] = df_final # Concat all regions into national pop df whole_nation_pop_df = pd . concat ( region_dfs_dict . values ()) # Create the pop_year column to show which year the data is from whole_nation_pop_df [ \"pop_year\" ] = pop_year # Change all column names to str whole_nation_pop_df . columns = whole_nation_pop_df . columns . astype ( str ) # Write df out to feather for quicker reading print ( \"Writing whole_nation_pop_df.feather\" ) whole_nation_pop_df . reset_index () . to_feather ( national_pop_feather_path ) else : # if it exists, read from a feather for quicker data retreval print ( f \"Reading whole_nation_pop_df from { national_pop_feather_path } \" ) whole_nation_pop_df = pd . read_feather ( national_pop_feather_path ) # Temporary TODO: remove this line whole_nation_pop_df . rename ( columns = { \"total_pop\" : \"pop_count\" }, inplace = True ) return whole_nation_pop_df","title":"get_whole_nation_pop_df"},{"location":"data_ingest/#src.data_ingest.grab_zip","text":"Used by _import_extract_delete_zip function to download a zip file from the URI specified in the the zip_link parameter. Parameters: file_nm ( str ) \u2013 the name of the file without extension. zip_link ( str ) \u2013 URI to the zip to be downloaded. zip_path ( PathLike ) \u2013 path/to/the/write/directory, e.g. '/data/'. Source code in src/data_ingest.py 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 def grab_zip ( file_nm : str , zip_link , zip_path : PathLike ): \"\"\"Used by _import_extract_delete_zip function to download a zip file from the URI specified in the the zip_link parameter. Args: file_nm (str): the name of the file without extension. zip_link (str): URI to the zip to be downloaded. zip_path (PathLike): path/to/the/write/directory, e.g. '/data/'. \"\"\" # Grab the zipfile from URI print ( f \"Downloading { file_nm } from { zip_link } \" ) r = requests . get ( zip_link ) with open ( zip_path , 'wb' ) as output_file : print ( f \"Saving { file_nm } to { zip_path } \" ) output_file . write ( r . content )","title":"grab_zip"},{"location":"data_ingest/#src.data_ingest.import_extract_delete_zip","text":"Downloads and opens zip file, extracts contents, deletes zip. Subfunc of any_to_pd. Parameters: file_nm ( str ) \u2013 The name of the target csv within the zip. zip_path ( PathLike ) \u2013 the path/to/zip_file on local machine persistent_exists ( bool , default: True ) \u2013 Boolean supplied by the _persistent_exists function. Defaults to True. zip_url ( str , default: None ) \u2013 URL for the zip resource if it is to be downloaded. Defaults to None. Returns: DataFrame \u2013 pd.DataFrame: dataframe of the data from the CSV Source code in src/data_ingest.py 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 def import_extract_delete_zip ( file_nm : str , zip_path : PathLike , persistent_exists = True , zip_url = None , * cols , ** dtypes ) -> pd . DataFrame : \"\"\"Downloads and opens zip file, extracts contents, deletes zip. Subfunc of any_to_pd. Args: file_nm (str): The name of the target csv within the zip. zip_path (PathLike): the path/to/zip_file on local machine persistent_exists (bool, optional): Boolean supplied by the _persistent_exists function. Defaults to True. zip_url (str, optional): URL for the zip resource if it is to be downloaded. Defaults to None. Returns: pd.DataFrame: dataframe of the data from the CSV \"\"\" if not persistent_exists : # checking if a persistent zip exists to save downloading grab_zip ( file_nm , zip_url , zip_path ) csv_nm = file_nm + \".csv\" csv_path = _make_data_path ( \"data\" , csv_nm ) extract_zip ( file_nm , csv_nm , zip_path , csv_path ) delete_junk ( file_nm , zip_path ) pd_df = csv_to_df ( file_nm , csv_path , dtypes ) return pd_df","title":"import_extract_delete_zip"},{"location":"data_ingest/#src.data_ingest.make_non_existent_folder","text":"Creates a folder if it doesn't exist. Source code in src/data_ingest.py 142 143 144 145 146 147 148 149 150 151 152 153 def make_non_existent_folder ( folder_path : pl . Path ) -> None : \"\"\"Creates a folder if it doesn't exist.\"\"\" # Check if folder_path is str if isinstance ( folder_path , str ): folder_path = pl . Path ( folder_path ) if not folder_path . exists (): MainLogger . info ( f \"Folder { folder_path } does not exist. Creating it now.\" ) folder_path . mkdir ( parents = True ) # Add a .gitkeep file to the folder so it gets pushed to GitHub ( folder_path / \".gitkeep\" ) . touch () return None","title":"make_non_existent_folder"},{"location":"data_ingest/#src.data_ingest.path_or_url","text":"Function to read data depending on config if cloud or local. Parameters: file_path ( str ) \u2013 File path you want to read Returns: \u2013 file_path or url depending if cloud or local Source code in src/data_ingest.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 def path_or_url ( file_path ): \"\"\"Function to read data depending on config if cloud or local. Args: file_path (str): File path you want to read Returns: file_path or url depending if cloud or local \"\"\" if CLOUD_LOCAL == \"cloud\" : url = bucket . generate_signed_url ( file_path ) return url elif CLOUD_LOCAL == \"local\" : return file_path else : MainLogger . error ( \"Cloud or local configuration is incorrect\" ) raise ImportError","title":"path_or_url"},{"location":"data_ingest/#src.data_ingest.pd_to_feather","text":"Used by the any_to_pd function to writes a Pandas dataframe to feather for quick reading and retrieval later. This function returns nothing because it simply writes to disk. Parameters: pd_df ( DataFrame ) \u2013 a pandas dataframe to be converted current_file_path ( PathLike ) \u2013 The path/to/current_file on local machine. Source code in src/data_ingest.py 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 def pd_to_feather ( pd_df : pd . DataFrame , current_file_path : PathLike ): \"\"\"Used by the any_to_pd function to writes a Pandas dataframe to feather for quick reading and retrieval later. This function returns nothing because it simply writes to disk. Args: pd_df (pd.DataFrame): a pandas dataframe to be converted current_file_path (PathLike): The path/to/current_file on local machine. \"\"\" feather_path = os . path . splitext ( current_file_path )[ 0 ] + '.feather' # TODO: this function should make use of _persistent_existsD if not os . path . isfile ( feather_path ): print ( f \"Writing Pandas dataframe to feather at { feather_path } \" ) feather . write_feather ( pd_df , feather_path ) print ( \"Feather already exists\" )","title":"pd_to_feather"},{"location":"data_ingest/#src.data_ingest.persistent_exists","text":"Checks if a persistent file or directory already exists or not. Parameters: persistent_path ( PathLike ) \u2013 path to check. Returns: bool \u2013 True if a persistent file or directory already exists. Source code in src/data_ingest.py 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 @lru_cache def persistent_exists ( persistent_path ): \"\"\"Checks if a persistent file or directory already exists or not. Args: persistent_path (PathLike): path to check. Returns: bool: True if a persistent file or directory already exists. \"\"\" # Change directory into project root os . chdir ( CWD ) if os . path . exists ( persistent_path ): print ( f \" { persistent_path } already exists\" ) return True else : print ( f \" { persistent_path } does not exist\" ) return False","title":"persistent_exists"},{"location":"data_ingest/#src.data_ingest.read_file_if_exists","text":"Checks if a file exists and reads it if it does using a function supplied. Parameters: file_path ( str ) \u2013 A path to a file created by os.path.join read_func ( func ) \u2013 A function to read the file. Raises: FileNotFoundError \u2013 description Returns: _type_ \u2013 description Source code in src/data_ingest.py 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 def read_file_if_exists ( file_path , read_func ): \"\"\"Checks if a file exists and reads it if it does using a function supplied. Args: file_path (str): A path to a file created by os.path.join read_func (func): A function to read the file. Raises: FileNotFoundError: _description_ Returns: _type_: _description_ \"\"\" if persistent_exists ( file_path ): return read_func ( file_path ) else : raise FileNotFoundError ( f \"File not found: { file_path } . Run preprocessing first.\" )","title":"read_file_if_exists"},{"location":"data_ingest/#src.data_ingest.read_ni_age_df","text":"Reads in the nothern ireland age information Parameters: path ( str ) \u2013 the path of the file Returns: \u2013 pd.DataFrame the age_df dataframe Source code in src/data_ingest.py 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 def read_ni_age_df ( path ): \"\"\" Reads in the nothern ireland age information Args: path (str): the path of the file Returns: pd.DataFrame the age_df dataframe \"\"\" # read in age df age_df = pd . read_excel ( path_or_url ( path ), sheet_name = \"SA\" , header = 5 , index_col = \"SA Code\" ) # replace col names from string e.g \"Age 82\" # to just the number 82 for col in age_df . columns : if bool ( re . search ( r \"\\d\" , col )): number = re . findall ( r \"[0-9]{1,3}\" , col )[ 0 ] age_df . rename ( columns = { col : number }, inplace = True ) return age_df","title":"read_ni_age_df"},{"location":"data_ingest/#src.data_ingest.read_scottish_age","text":"Reads in the scottish age information Parameters: path ( str ) \u2013 the path of the file Returns: Bool \u2013 True if path has not been modified within the number of days specified Source code in src/data_ingest.py 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 def read_scottish_age ( path ): \"\"\" Reads in the scottish age information Args: path (str): the path of the file Returns: Bool: True if path has not been modified within the number of days specified \"\"\" # read in scottish file age_scotland_df = pd . read_csv ( path_or_url ( path ), skiprows = 4 ) # dropping first row as this is the whole of scotland age_scotland_df = age_scotland_df . iloc [ 1 : - 4 , :] age_scotland_df = age_scotland_df . reset_index () return age_scotland_df","title":"read_scottish_age"},{"location":"data_ingest/#src.data_ingest.read_urb_rur_class_scotland","text":"Reads the urb/rural classification for Scotland. This reads in the file containing all the urban/rural class Then applies a mapping based on if living in a settlement >10,000 then urban, else rural. Parameters: path ( str ) \u2013 the path where the file exists Returns: \u2013 pd.DataFrame: the classfication dataframe Source code in src/data_ingest.py 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 def read_urb_rur_class_scotland ( urb_rur_path ): \"\"\"Reads the urb/rural classification for Scotland. This reads in the file containing all the urban/rural class Then applies a mapping based on if living in a settlement >10,000 then urban, else rural. Args: path (str): the path where the file exists Returns: pd.DataFrame: the classfication dataframe \"\"\" urb_rur = pd . read_csv ( path_or_url ( urb_rur_path ), usecols = [ \"OA2011\" , \"UR6_2013_2014\" ]) urb_rur [ \"urb_rur_class\" ] = np . where ( ( urb_rur [ \"UR6_2013_2014\" ] == 1 ) | ( urb_rur [ \"UR6_2013_2014\" ] == 2 ), \"urban\" , \"rural\" ) return urb_rur","title":"read_urb_rur_class_scotland"},{"location":"data_ingest/#src.data_ingest.read_urb_rur_ni","text":"Reads small areas as urban or rural for Northern Ireland. Urban is classified as living in a settlement >10,000, and rural is classified as <10,000. Args: path (str): the path where the file exists Returns: \u2013 pd.DataFrame: the urban rural classification dataframe Source code in src/data_ingest.py 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 def read_urb_rur_ni ( urb_rur_path ): \"\"\" Reads small areas as urban or rural for Northern Ireland. Urban is classified as living in a settlement >10,000, and rural is classified as <10,000. Args: path (str): the path where the file exists Returns: pd.DataFrame: the urban rural classification dataframe \"\"\" urb_rur = pd . read_csv ( path_or_url ( urb_rur_path ), skiprows = 3 ) urb_rur = urb_rur [[ 'SA2011_Code' , 'Settlement Classification Band' ]] # split classification bands into urban and rural urb_list = [ 'A' , 'B' , 'C' , 'D' ] rural_list = [ 'E' , 'F' , 'G' , 'H' ] urb_rur . loc [ urb_rur [ 'Settlement Classification Band' ] . isin ( urb_list ), 'urb_rur_class' ] = 'urban' urb_rur . loc [ urb_rur [ 'Settlement Classification Band' ] . isin ( rural_list ), 'urb_rur_class' ] = 'rural' return urb_rur","title":"read_urb_rur_ni"},{"location":"data_ingest/#src.data_ingest.read_usual_pop_scotland","text":"Reads the usual population Scotland. This reads in the file containing all the output areas in scotland with their corresponding population number. Parameters: path ( str ) \u2013 the path where the file exists Returns: \u2013 pd.DataFrame the usual population dataframe Source code in src/data_ingest.py 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 def read_usual_pop_scotland ( path : str ): \"\"\"Reads the usual population Scotland. This reads in the file containing all the output areas in scotland with their corresponding population number. Args: path (str): the path where the file exists Returns: pd.DataFrame the usual population dataframe \"\"\" # reads in data and crops of header and footer df = pd . read_csv ( path_or_url ( path ), header = 4 , index_col = 0 , skipfooter = 4 ) # only want OA so drop Scotland df_oa_only = df . drop ( index = [ 'Scotland' ]) # only use columns that we need essential_cols = [ \"All people\" , \"Males\" , \"Females\" ] df_essential_cols = df_oa_only [ essential_cols ] # ensure no commas in the dataset so no errors with dtypes for col in essential_cols : df_essential_cols [ col ] = df_essential_cols [ col ] . str . replace ( ',' , '' ) df_essential_cols [ col ] = df_essential_cols [ col ] . astype ( int ) return df_essential_cols","title":"read_usual_pop_scotland"},{"location":"data_ingest/#src.data_ingest.save_latest_stops_as_feather","text":"Saves the latest stop file as a feather file into the data folder. Parameters: file_name ( str ) \u2013 file path for latest stop file. Source code in src/data_ingest.py 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 def save_latest_stops_as_feather ( file_name ): \"\"\"Saves the latest stop file as a feather file into the data folder. Args: file_name (str): file path for latest stop file. \"\"\" # read in csv file = pd . read_csv ( file_name , usecols = config [ \"naptan_types\" ] . keys (), dtype = config [ \"naptan_types\" ]) # get output path output_path = os . path . join ( os . getcwd (), \"data\" , \"stops\" , \"Stops.feather\" ) # output to feather file . to_feather ( output_path ) return output_path","title":"save_latest_stops_as_feather"},{"location":"data_output/","text":"reorder_final_df ( df ) Reorders the processed dataframe before writing to csv. Parameters: df ( DataFrame ) \u2013 Dataframe to reorder. Returns: \u2013 pd.DataFrame: Reordered dataframe. Source code in src/data_output.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def reorder_final_df ( df ): \"\"\"Reorders the processed dataframe before writing to csv. Args: df (pd.DataFrame): Dataframe to reorder. Returns: pd.DataFrame: Reordered dataframe. \"\"\" df = df [[ \"Year\" , \"Sex\" , \"Age\" , \"Disability Status\" , \"Local Authority\" , \"Urban/Rural\" , \"Series\" , \"Observation Status\" , \"Unit Multiplier\" , \"Unit Measure\" , \"Value\" ]] return df reshape_for_output ( df , id_col , local_auth , id_rename = None ) Reshapes the output of served_proportions_disagg to data team requirements. The steps the function goes through are as follows. | 1) Transpose the df | 2) Reset the index | 3) Rename column from index to Age or other id column | 4) melt df with Age as ID vars, all the rest value vars | 5) Replace word \"Total\" with blanks, \"variable\" with \"Series\" | 6) Create \"Unit Multiplier\" map across from variable (percent or individual) | 7) Create \"Unit Measure\" and \"Observation Status\" columns | 7) Create the local auth col with this iteration's LA | 8) Rename column header of ID var | 9) re-order columns to match required output Parameters: df ( DataFrame ) \u2013 Dataframe to reshape. id_col ( str ) \u2013 Name of the column that index will get renamed to. local_auth ( str ) \u2013 The local authority of interest. id_rename ( str , default: None ) \u2013 Name if renaming ID column. Defaults to None. Returns: \u2013 pd.DataFrame: Reshaped dataframe. Source code in src/data_output.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def reshape_for_output ( df , id_col , local_auth , id_rename = None ): \"\"\" Reshapes the output of served_proportions_disagg to data team requirements. The steps the function goes through are as follows. | 1) Transpose the df | 2) Reset the index | 3) Rename column from index to Age or other id column | 4) melt df with Age as ID vars, all the rest value vars | 5) Replace word \"Total\" with blanks, \"variable\" with \"Series\" | 6) Create \"Unit Multiplier\" map across from variable (percent or individual) | 7) Create \"Unit Measure\" and \"Observation Status\" columns | 7) Create the local auth col with this iteration's LA | 8) Rename column header of ID var | 9) re-order columns to match required output Args: df (pd.DataFrame): Dataframe to reshape. id_col (str): Name of the column that index will get renamed to. local_auth (str): The local authority of interest. id_rename (str, optional): Name if renaming ID column. Defaults to None. Returns: pd.DataFrame: Reshaped dataframe. \"\"\" # Transpose the df df = df . T # Reset the index df = df . reset_index () # Rename column from index to id_column, e.g \"Age\" df_renamed = df . rename ( columns = { \"index\" : id_col }) # Melt df with Age as ID vars, all the rest value vars df = pd . melt ( df_renamed , id_vars = id_col , value_vars = [ \"Total\" , \"Served\" , \"Unserved\" , \"Percentage served\" , \"Percentage unserved\" ]) # add in total population for la when id_col = total if id_col == \"Total\" : extra_row = pd . DataFrame ({ \"Total\" : [ \"Total\" ], \"variable\" : [ \"\" ], \"value\" : [ df_renamed [ \"All_pop\" ][ 0 ]]}) df = pd . concat ([ extra_row , df ]) # Replace word \"Total\" with blanks df = df . replace ({ \"Total\" : \"\" }) # Create \"Unit Multiplier\" map across from variable (percent or individual) df [ \"Unit Measure\" ] = np . where ( df . variable . str . contains ( \"Percentage\" ), \"percent\" , \"individual\" ) # Create \"Unit Measure\" and \"Observation Status\" columns df [ \"Unit Multiplier\" ] = \"Units\" df [ \"Observation Status\" ] = \"Undefined\" # Rename the variables in the \"variable\" column df [ \"variable\" ] . replace ( to_replace = \"Percentage served\" , value = \"Served\" , inplace = True ) df [ \"variable\" ] . replace ( to_replace = \"Percentage unserved\" , value = \"Unserved\" , inplace = True ) # Rename the \"variable\" col to \"Series\" df . rename ( columns = { \"variable\" : \"Series\" }, inplace = True ) # Rename \"value\" to \"Value\" as required df . rename ( columns = { \"value\" : \"Value\" }, inplace = True ) # Add and populate the \"Local Authority\" column df [ \"Local Authority\" ] = local_auth df = df [[ id_col , \"Local Authority\" , \"Series\" , \"Observation Status\" , \"Unit Multiplier\" , \"Unit Measure\" , \"Value\" ]] # Rename column header of ID var if id_rename : df . rename ( columns = { id_col : id_rename }, inplace = True ) return df","title":"Data output"},{"location":"data_output/#src.data_output.reorder_final_df","text":"Reorders the processed dataframe before writing to csv. Parameters: df ( DataFrame ) \u2013 Dataframe to reorder. Returns: \u2013 pd.DataFrame: Reordered dataframe. Source code in src/data_output.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def reorder_final_df ( df ): \"\"\"Reorders the processed dataframe before writing to csv. Args: df (pd.DataFrame): Dataframe to reorder. Returns: pd.DataFrame: Reordered dataframe. \"\"\" df = df [[ \"Year\" , \"Sex\" , \"Age\" , \"Disability Status\" , \"Local Authority\" , \"Urban/Rural\" , \"Series\" , \"Observation Status\" , \"Unit Multiplier\" , \"Unit Measure\" , \"Value\" ]] return df","title":"reorder_final_df"},{"location":"data_output/#src.data_output.reshape_for_output","text":"Reshapes the output of served_proportions_disagg to data team requirements. The steps the function goes through are as follows. | 1) Transpose the df | 2) Reset the index | 3) Rename column from index to Age or other id column | 4) melt df with Age as ID vars, all the rest value vars | 5) Replace word \"Total\" with blanks, \"variable\" with \"Series\" | 6) Create \"Unit Multiplier\" map across from variable (percent or individual) | 7) Create \"Unit Measure\" and \"Observation Status\" columns | 7) Create the local auth col with this iteration's LA | 8) Rename column header of ID var | 9) re-order columns to match required output Parameters: df ( DataFrame ) \u2013 Dataframe to reshape. id_col ( str ) \u2013 Name of the column that index will get renamed to. local_auth ( str ) \u2013 The local authority of interest. id_rename ( str , default: None ) \u2013 Name if renaming ID column. Defaults to None. Returns: \u2013 pd.DataFrame: Reshaped dataframe. Source code in src/data_output.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def reshape_for_output ( df , id_col , local_auth , id_rename = None ): \"\"\" Reshapes the output of served_proportions_disagg to data team requirements. The steps the function goes through are as follows. | 1) Transpose the df | 2) Reset the index | 3) Rename column from index to Age or other id column | 4) melt df with Age as ID vars, all the rest value vars | 5) Replace word \"Total\" with blanks, \"variable\" with \"Series\" | 6) Create \"Unit Multiplier\" map across from variable (percent or individual) | 7) Create \"Unit Measure\" and \"Observation Status\" columns | 7) Create the local auth col with this iteration's LA | 8) Rename column header of ID var | 9) re-order columns to match required output Args: df (pd.DataFrame): Dataframe to reshape. id_col (str): Name of the column that index will get renamed to. local_auth (str): The local authority of interest. id_rename (str, optional): Name if renaming ID column. Defaults to None. Returns: pd.DataFrame: Reshaped dataframe. \"\"\" # Transpose the df df = df . T # Reset the index df = df . reset_index () # Rename column from index to id_column, e.g \"Age\" df_renamed = df . rename ( columns = { \"index\" : id_col }) # Melt df with Age as ID vars, all the rest value vars df = pd . melt ( df_renamed , id_vars = id_col , value_vars = [ \"Total\" , \"Served\" , \"Unserved\" , \"Percentage served\" , \"Percentage unserved\" ]) # add in total population for la when id_col = total if id_col == \"Total\" : extra_row = pd . DataFrame ({ \"Total\" : [ \"Total\" ], \"variable\" : [ \"\" ], \"value\" : [ df_renamed [ \"All_pop\" ][ 0 ]]}) df = pd . concat ([ extra_row , df ]) # Replace word \"Total\" with blanks df = df . replace ({ \"Total\" : \"\" }) # Create \"Unit Multiplier\" map across from variable (percent or individual) df [ \"Unit Measure\" ] = np . where ( df . variable . str . contains ( \"Percentage\" ), \"percent\" , \"individual\" ) # Create \"Unit Measure\" and \"Observation Status\" columns df [ \"Unit Multiplier\" ] = \"Units\" df [ \"Observation Status\" ] = \"Undefined\" # Rename the variables in the \"variable\" column df [ \"variable\" ] . replace ( to_replace = \"Percentage served\" , value = \"Served\" , inplace = True ) df [ \"variable\" ] . replace ( to_replace = \"Percentage unserved\" , value = \"Unserved\" , inplace = True ) # Rename the \"variable\" col to \"Series\" df . rename ( columns = { \"variable\" : \"Series\" }, inplace = True ) # Rename \"value\" to \"Value\" as required df . rename ( columns = { \"value\" : \"Value\" }, inplace = True ) # Add and populate the \"Local Authority\" column df [ \"Local Authority\" ] = local_auth df = df [[ id_col , \"Local Authority\" , \"Series\" , \"Observation Status\" , \"Unit Multiplier\" , \"Unit Measure\" , \"Value\" ]] # Rename column header of ID var if id_rename : df . rename ( columns = { id_col : id_rename }, inplace = True ) return df","title":"reshape_for_output"},{"location":"data_quality/","text":"Data Quality NAPTAN The NaPTAN data is open source and available for anyone under a Government License. In our methodology we need to distinguish between high and low capacity transport. There is no specific marker for this in the dataset. Therefore we use the stop type variable to determine whether a stop is high or low capacity transport. Trams and Metro have the same stop_type. According to the UN definition, Metros are high capacity and Trams are low capacity. However we are unable to distinguish between the two in our data. Here is the issue in more detail. The data is submitted on an adhoc basis by local authorities. Details of their last submission can be seen here . As this is done on an adhoc basis there needs to be an element of downloading and timestamping data to ensure analysis is reproducible. Furthermore, the API which we get the data from is updated on a daily basis and it is difficult to tell whether the data has changed from the last upload. There is a column called status which determines whether a stop is inactive or active. We use this variable to ensure all stops used in our calculation are active. However, there are some other values such as pending and blank which are ambiguous. A conversation with the department of transport documented here clarified how to deal with each value. Even though it is the local authorities responsibility to submit to NaPTAN. It is unclear how they submit this information. Do they liaise with the bus companies to get this information and how timely is each submission? An important issue is that it is not clear whether each stop has step free access and is accessible to all. Therefore there could be stops that are in the data which can\u2019t be used. This is particularly relevant to our analysis as we produce breakdowns by disability. Population Weighted Centroids The OA Population Weighted Centroids are an official geographical product from the Office For National Statistics. These are used to determine where an Output Area can be allocated to, when aggregating statistics from OA to any other geographical level such as local authorities. Centroids are calculated via the median centroid algorithm. The methodology is detailed here . The population centroids are on an Output area basis. There are approximately 300 people living in an OA, therefore it automatically assumes that all 300 people live within this one point. As we apply a Eucliden buffer around the centroid (a circle with radius 500/1000M) there may not be an accessible path/pavement or road to get from the centroid to the stop that lies within the buffer. The centroid might be in the middle of a lake or field and therefore not actually representative of where the population actually live.The centroids were calculated in 2011. There is a possibility that the areas in which people lived in have changed. For example if a new housing estate has been built since 2011, this may shift where the centroid lies. Urban/Rural Classification The definition of whether an OA is urban/rural can be found here . This is one definition of how to classify this. The EU uses a different definition and therefore there would be differences in the output statistics. Urban/Rural classification uses a 2011 classification applied to the current OA boundaries. Classification of urban/rural might have changed since then. For example if a big housing estate was created in the countryside on the edge of a city, this might lead to that OA being classified as urban instead of rural. Disability Data The disability data used iis from the 2011 census, therefore a proportion is applied to population estimates for subsequent years. This method was suggested by the ONS Geo Spatial Department. These demographics might have changed since then. The definition used for disability is the GSS harmonized disability data . The UN definition appears to be much wider for disability. This could lead to different output statistics when aggregating up. Annual Population Estimates These are the annual population estimates that ONS produce every year. They are an official statistic. These, unlike the census, are estimates and have a defined methodology here . The methodology includes a section on statistical uncertainty about these numbers. LAD Boundaries There are several different types of geo spatial boundaries that can be used. (BFE) Best Full Extent is the boundary type we use. To be consistent with official statistics that ONS produce this is the type of boundary we use. LA Boundaries are subject to change year on year. Big changes can occur such as merging of local authorities and name changes. Therefore comparisons between years need to be made carefully. There is a discussion on this point documented here .","title":"Data Quality"},{"location":"data_quality/#data-quality","text":"","title":"Data Quality"},{"location":"data_quality/#naptan","text":"The NaPTAN data is open source and available for anyone under a Government License. In our methodology we need to distinguish between high and low capacity transport. There is no specific marker for this in the dataset. Therefore we use the stop type variable to determine whether a stop is high or low capacity transport. Trams and Metro have the same stop_type. According to the UN definition, Metros are high capacity and Trams are low capacity. However we are unable to distinguish between the two in our data. Here is the issue in more detail. The data is submitted on an adhoc basis by local authorities. Details of their last submission can be seen here . As this is done on an adhoc basis there needs to be an element of downloading and timestamping data to ensure analysis is reproducible. Furthermore, the API which we get the data from is updated on a daily basis and it is difficult to tell whether the data has changed from the last upload. There is a column called status which determines whether a stop is inactive or active. We use this variable to ensure all stops used in our calculation are active. However, there are some other values such as pending and blank which are ambiguous. A conversation with the department of transport documented here clarified how to deal with each value. Even though it is the local authorities responsibility to submit to NaPTAN. It is unclear how they submit this information. Do they liaise with the bus companies to get this information and how timely is each submission? An important issue is that it is not clear whether each stop has step free access and is accessible to all. Therefore there could be stops that are in the data which can\u2019t be used. This is particularly relevant to our analysis as we produce breakdowns by disability.","title":"NAPTAN"},{"location":"data_quality/#population-weighted-centroids","text":"The OA Population Weighted Centroids are an official geographical product from the Office For National Statistics. These are used to determine where an Output Area can be allocated to, when aggregating statistics from OA to any other geographical level such as local authorities. Centroids are calculated via the median centroid algorithm. The methodology is detailed here . The population centroids are on an Output area basis. There are approximately 300 people living in an OA, therefore it automatically assumes that all 300 people live within this one point. As we apply a Eucliden buffer around the centroid (a circle with radius 500/1000M) there may not be an accessible path/pavement or road to get from the centroid to the stop that lies within the buffer. The centroid might be in the middle of a lake or field and therefore not actually representative of where the population actually live.The centroids were calculated in 2011. There is a possibility that the areas in which people lived in have changed. For example if a new housing estate has been built since 2011, this may shift where the centroid lies.","title":"Population Weighted Centroids"},{"location":"data_quality/#urbanrural-classification","text":"The definition of whether an OA is urban/rural can be found here . This is one definition of how to classify this. The EU uses a different definition and therefore there would be differences in the output statistics. Urban/Rural classification uses a 2011 classification applied to the current OA boundaries. Classification of urban/rural might have changed since then. For example if a big housing estate was created in the countryside on the edge of a city, this might lead to that OA being classified as urban instead of rural.","title":"Urban/Rural Classification"},{"location":"data_quality/#disability-data","text":"The disability data used iis from the 2011 census, therefore a proportion is applied to population estimates for subsequent years. This method was suggested by the ONS Geo Spatial Department. These demographics might have changed since then. The definition used for disability is the GSS harmonized disability data . The UN definition appears to be much wider for disability. This could lead to different output statistics when aggregating up.","title":"Disability Data"},{"location":"data_quality/#annual-population-estimates","text":"These are the annual population estimates that ONS produce every year. They are an official statistic. These, unlike the census, are estimates and have a defined methodology here . The methodology includes a section on statistical uncertainty about these numbers.","title":"Annual Population Estimates"},{"location":"data_quality/#lad-boundaries","text":"There are several different types of geo spatial boundaries that can be used. (BFE) Best Full Extent is the boundary type we use. To be consistent with official statistics that ONS produce this is the type of boundary we use. LA Boundaries are subject to change year on year. Big changes can occur such as merging of local authorities and name changes. Therefore comparisons between years need to be made carefully. There is a discussion on this point documented here .","title":"LAD Boundaries"},{"location":"data_transform/","text":"bin_pop_ages ( age_df , age_bins , col_nms ) Bins the ages in the age_df in 5 year spans from 0 to 90+, sums the counts in those bins and drops the original age columns. Parameters: df ( DataFrame ) \u2013 A dataframe of population data containing only the age columns. Returns: \u2013 pd.DataFrame: Returns the age_df with bins. Source code in src/data_transform.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 def bin_pop_ages ( age_df , age_bins , col_nms ): \"\"\" Bins the ages in the age_df in 5 year spans from 0 to 90+, sums the counts in those bins and drops the original age columns. Args: df (pd.DataFrame): A dataframe of population data containing only the age columns. Returns: pd.DataFrame: Returns the age_df with bins. \"\"\" # Grouping ages in 5 year brackets # cleaning scottish data and changing dtype to float original_columns = age_df . columns for col in original_columns : if age_df [ col ] . dtypes == \"O\" : age_df [ col ] = age_df [ col ] . str . replace ( '-' , '0' ) age_df [ col ] = age_df [ col ] . astype ( int ) def _age_bin ( age_df , age_bins ): \"\"\"Function sums the counts for corresponding age-bins and assigns them a column in age_df.\"\"\" for bin in age_bins : age_df [ f \" { bin [ 0 ] } - { bin [ 1 ] } \" ] = ( age_df . loc [:, bin [ 0 ]: bin [ 1 ]] . sum ( axis = 1 )) return age_df # create 90+ column for when there are more columns than 90 if len ( age_df . columns ) > 91 : # create 90+ column summing all those from 90 and above. age_df [ '90+' ] = age_df . iloc [:, 90 :] . sum ( axis = 1 ) age_df = _age_bin ( age_df , age_bins ) # drop the original age columns age_df . drop ( col_nms , axis = 1 , inplace = True ) # drop the columns that we are replacing with 90+ age_df . drop ( age_df . iloc [:, 19 :], axis = 1 , inplace = True ) # moving first column to last so 90+ at the end. temp_cols = age_df . columns . tolist () new_cols = temp_cols [ 1 :] + temp_cols [ 0 : 1 ] age_df = age_df [ new_cols ] else : age_df = _age_bin ( age_df , age_bins ) # drop the original age columns age_df . drop ( col_nms , axis = 1 , inplace = True ) # rename the 90+ column age_df . rename ( columns = { '90+-90+' : '90+' }, inplace = True ) # age df has now been binned and cleaned return round ( age_df ) convert_east_north ( df , long , lat ) Converts latitude and longitude coordinates to British National Grid Args: df (pd.DataFrame): df including the longitude and latitude coordinates long(str): The name of the longitude column in df lat (str): The name of the latitude column in df Returns: pd.DataFrame: dataframe including easting and northing coordinates. Source code in src/data_transform.py 438 439 440 441 442 443 444 445 446 447 448 449 def convert_east_north ( df , long , lat ): \"\"\" Converts latitude and longitude coordinates to British National Grid Args: df (pd.DataFrame): df including the longitude and latitude coordinates long(str): The name of the longitude column in df lat (str): The name of the latitude column in df Returns: pd.DataFrame: dataframe including easting and northing coordinates. \"\"\" df [ 'Easting' ], df [ 'Northing' ] = convert_bng ( df [ long ], df [ lat ]) return df create_tiploc_col ( naptan_df ) Creates a Tiploc column from the ATCOCode column, in the NaPTAN dataset. Parameters: naptan_df ( Dataframe ) \u2013 Naptan dataset Returns: \u2013 pd.Dataframe (naptan_df): Naptan dataset with the new tiploc column \u2013 added for train stations Source code in src/data_transform.py 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 def create_tiploc_col ( naptan_df ): \"\"\"Creates a Tiploc column from the ATCOCode column, in the NaPTAN dataset. Args: naptan_df (pd.Dataframe): Naptan dataset Returns: pd.Dataframe (naptan_df): Naptan dataset with the new tiploc column added for train stations \"\"\" # Applying only to train stations, RLY is the stop type for train stations rail_filter = naptan_df . StopType == \"RLY\" # Create a new pd.Dataframe for Tiploc by extracting upto 7 alpha # characters tiploc_col = ( naptan_df . loc [ rail_filter ] . ATCOCode . str . extract ( r '([A-Za-z]{1,7})' ) ) tiploc_col . columns = [ \"tiploc_code\" ] # Merge the new Tiploc column with the naptan_df naptan_df = naptan_df . merge ( tiploc_col , how = 'left' , left_index = True , right_index = True ) return naptan_df disab_dict ( la_pop_df , pop_in_poly_df , disability_dict , local_auth ) Creates the dataframe including those who are and are not served by public transport and places it into a disability dictionary for each local authority of interest for the final csv output. Parameters: la_pop_df ( GeoDataFrame ) \u2013 GeoPandas Dataframe that includes output area codes and population estimates. pop_in_poly_df ( GeoDataFrame ) \u2013 A geodata frame with the points inside the polygon. disability_dict ( dict ) \u2013 Dictionary to store the disability dataframe. local_auth ( str ) \u2013 The local authority of interest. Returns: disability_dict ( dict ) \u2013 Dictionary with a disability total dataframe for unserved and served populations for all given local authorities. Source code in src/data_transform.py 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 def disab_dict ( la_pop_df , pop_in_poly_df , disability_dict , local_auth ): \"\"\"Creates the dataframe including those who are and are not served by public transport and places it into a disability dictionary for each local authority of interest for the final csv output. Args: la_pop_df (gpd.GeoDataFrame): GeoPandas Dataframe that includes output area codes and population estimates. pop_in_poly_df (gpd.GeoDataFrame): A geodata frame with the points inside the polygon. disability_dict (dict): Dictionary to store the disability dataframe. local_auth (str): The local authority of interest. Returns: disability_dict (dict): Dictionary with a disability total dataframe for unserved and served populations for all given local authorities. \"\"\" # Calculating those served and not served by disability disab_cols = [ \"number_disabled\" ] disab_servd_df = served_proportions_disagg ( la_pop_df , pop_in_poly_df , disab_cols ) # Feeding the results to the reshaper disab_servd_df_out = do . reshape_for_output ( disab_servd_df , id_col = disab_cols [ 0 ], local_auth = local_auth , id_rename = \"Disability Status\" ) # The disability df is unusual. I think all rows correspond to people with # disabilities only. There is no \"not-disabled\" status here (I think) disab_servd_df_out . replace ( to_replace = \"number_disabled\" , value = \"Disabled\" , inplace = True ) # Calculating non-disabled people served and not served non_disab_cols = [ \"number_non-disabled\" ] non_disab_servd_df = served_proportions_disagg ( pop_df = la_pop_df , pop_in_poly_df = pop_in_poly_df , cols_lst = non_disab_cols ) # Feeding the results to the reshaper non_disab_servd_df_out = do . reshape_for_output ( non_disab_servd_df , id_col = disab_cols [ 0 ], local_auth = local_auth , id_rename = \"Disability Status\" ) # The disability df is unusual. I think all rows correspond to people with # disabilities only. There is no \"not-disabled\" status here (I think) non_disab_servd_df_out . replace ( to_replace = \"number_non-disabled\" , value = \"Non-disabled\" , inplace = True ) # Concatting non-disabled and disabled dataframes non_disab_disab_servd_df_out = pd . concat ( [ non_disab_servd_df_out , disab_servd_df_out ]) # Output this local auth's disab df to the dict disability_dict [ local_auth ] = non_disab_disab_servd_df_out return disability_dict disab_disagg ( disability_df , la_pop_df ) Calculates number of people in the population that are classified as disabled or not disabled and this is merged onto the local authority population dataframe. Args: disability_df (pd.DataFrame): Dataframe that includes disability estimates for each output area. la_pop_df (gpd.GeoDataFrame): GeoPandas Dataframe that includes output area codes and population estimates. Returns: gpd.GeoDataFrame: GeoPandas Dataframe that includes population estimates, geographical location, and proportion of disabled/non-disabled for each output area in the local authority chosen. Source code in src/data_transform.py 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 def disab_disagg ( disability_df , la_pop_df ): \"\"\"Calculates number of people in the population that are classified as disabled or not disabled and this is merged onto the local authority population dataframe. Args: disability_df (pd.DataFrame): Dataframe that includes disability estimates for each output area. la_pop_df (gpd.GeoDataFrame): GeoPandas Dataframe that includes output area codes and population estimates. Returns: gpd.GeoDataFrame: GeoPandas Dataframe that includes population estimates, geographical location, and proportion of disabled/non-disabled for each output area in the local authority chosen. \"\"\" # Getting the disab total disability_df [ \"disb_total\" ] = ( disability_df [ \"disab_ltd_lot\" ] + disability_df [ \"disab_ltd_little\" ]) # Calcualting the total \"non-disabled\" la_pop_only = la_pop_df [[ 'OA11CD' , 'pop_count' ]] disability_df = la_pop_only . merge ( disability_df , on = \"OA11CD\" ) # Putting the result back into the disability df disability_df [ \"non-disabled\" ] = disability_df [ \"pop_count\" ] - \\ disability_df [ 'disb_total' ] # Calculating the proportion of disabled people in each OA disability_df [ \"proportion_disabled\" ] = ( disability_df [ 'disb_total' ] / disability_df [ 'pop_count' ] ) # Calcualting the proportion of non-disabled people in each OA disability_df [ \"proportion_non-disabled\" ] = ( disability_df [ 'non-disabled' ] / disability_df [ 'pop_count' ] ) # Slice disability df that only has the proportion disabled column and the # OA11CD col disab_prop_df = disability_df [[ 'OA11CD' , 'proportion_disabled' , 'proportion_non-disabled' ]] # Merge the proportion disability df into main the pop df with a left join la_pop_df = la_pop_df . merge ( disab_prop_df , on = 'OA11CD' , how = \"left\" ) # Make the calculation of the number of people with disabilities in the # year of the population estimates la_pop_df [ \"number_disabled\" ] = ( round ( la_pop_df [ \"pop_count\" ] * la_pop_df [ \"proportion_disabled\" ]) ) # la_pop_df[\"number_disabled\"] = la_pop_df[\"number_disabled\"].astype(int) # Make the calculation of the number of non-disabled people in the year # of the population estimates la_pop_df [ \"number_non-disabled\" ] = ( round ( la_pop_df [ \"pop_count\" ] * la_pop_df [ \"proportion_non-disabled\" ]) ) la_pop_df [ \"number_non-disabled\" ] = la_pop_df [ \"number_non-disabled\" ] . astype ( int ) return la_pop_df get_col_bins ( col_nms ) Function to group/bin the ages together in 5 year steps. Starts the sequence at age 0. Will return the ages in a list of tuples. The 0th position of the tuple being the lower limit of the age bin, the 1st position of the tuple being the upper limit. Parameters: col_nms ( list of str ) \u2013 a list of the age columns as strings. Returns: \u2013 list of tuples: a list of the ages with 5 year gaps. Source code in src/data_transform.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def get_col_bins ( col_nms : List [ str ]): \"\"\"Function to group/bin the ages together in 5 year steps. Starts the sequence at age 0. Will return the ages in a list of tuples. The 0th position of the tuple being the lower limit of the age bin, the 1st position of the tuple being the upper limit. Args: col_nms (list of str): a list of the age columns as strings. Returns: list of tuples: a list of the ages with 5 year gaps. \"\"\" # Make a lists of starting and finishing indexes cols_start = col_nms [ 0 :: 5 ] cols_fin = col_nms [ 4 :: 5 ] # Generating a list of tuples which will be the age groupings col_bins = [( s , f ) for s , f in zip ( cols_start , cols_fin )] # Again adding \"90+\", doubling it so it's doubled, like the other tuples col_bins . append (( cols_start [ - 1 :] * 2 )) # TODO: make this more intelligent. Only if there is one col name left # over it should be doubled. return col_bins mid_year_age_estimates ( age_df , pop_estimates_df , pop_year ) Takes mid-year population estimates for each small area in NI and uses proportions to calculate mid-year estimates for each age. Args: age_df (pd.DataFrame): Census 2011 age estimates dataframe pop_estimates_df (pd.DataFrame): population estimates for each small area dataframe pop_year (str): population year Source code in src/data_transform.py 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 def mid_year_age_estimates ( age_df , pop_estimates_df , pop_year ): \"\"\" Takes mid-year population estimates for each small area in NI and uses proportions to calculate mid-year estimates for each age. Args: age_df (pd.DataFrame): Census 2011 age estimates dataframe pop_estimates_df (pd.DataFrame): population estimates for each small area dataframe pop_year (str): population year \"\"\" # get all age columns in a list age_cols = [ str ( y ) for y in range ( 101 )] # iterate through each age for age in range ( len ( age_cols )): # calculates the proportions for each age and each SA code age_df [ age_cols [ age ]] = age_df [ age_cols [ age ]] / age_df [ 'All usual residents' ] age_df . drop ([ 'SA' , 'All usual residents' ], axis = 1 , inplace = True ) # merges pop df and proportions together pop_estimates_df = pop_estimates_df . merge ( age_df . reset_index (), left_on = 'Area_Code' , right_on = 'SA Code' , how = 'left' ) # calculates pop estimates for each age in each small area using proportions for age in range ( len ( age_cols )): pop_estimates_df [ age_cols [ age ]] = pop_estimates_df [ age_cols [ age ]] * pop_estimates_df [ pop_year ] pop_estimates_df . set_index ( 'Area_Code' , inplace = True ) return pop_estimates_df served_proportions_disagg ( pop_df , pop_in_poly_df , cols_lst ) Calculates the number of people in each category, as specified by the column (e.g age range, or disability status) who are served and not served by public transport, and gives those as a proportion of the total. Note: the numeric values in the dataframe are return as strings for formatting reasons Parameters: pop_df ( DataFrame ) \u2013 population dataframe. pop_in_poly_df ( DataFrame ) \u2013 dataframe resulting in the points in polygons enquiry to count (sum) the population within the service area polygon. cols_lst ( List [ str ] ) \u2013 a list of the column names in the population dataframe supplied which contain population figures, and are to be summed and assessed as served/unserved by public transport. Returns: \u2013 pd.DataFrame: a dataframe summarising \u2013 i) the total number of people that column (e.g. age range, sex) \u2013 ii) the number served by public transport \u2013 iii) the proportion who are served by public transport \u2013 iv) the proportion who are not served by public transport Source code in src/data_transform.py 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 def served_proportions_disagg ( pop_df : pd . DataFrame , pop_in_poly_df : pd . DataFrame , cols_lst : List [ str ]): \"\"\"Calculates the number of people in each category, as specified by the column (e.g age range, or disability status) who are served and not served by public transport, and gives those as a proportion of the total. Note: the numeric values in the dataframe are return as strings for formatting reasons Args: pop_df (pd.DataFrame): population dataframe. pop_in_poly_df (pd.DataFrame): dataframe resulting in the points in polygons enquiry to count (sum) the population within the service area polygon. cols_lst (List[str]): a list of the column names in the population dataframe supplied which contain population figures, and are to be summed and assessed as served/unserved by public transport. Returns: pd.DataFrame: a dataframe summarising i) the total number of people that column (e.g. age range, sex) ii) the number served by public transport iii) the proportion who are served by public transport iv) the proportion who are not served by public transport \"\"\" # First list the age bin columns pop_sums = {} for col in cols_lst : # Total pop total_pop = int ( pop_df [ col ] . sum ()) # Served pop servd_pop = int ( pop_in_poly_df [ col ] . sum ()) # Unserved pop unsrvd_pop = int ( total_pop - servd_pop ) if total_pop == 0 : # If the total population for that column is 0 # this standard of zeros and Nones is returned pop_sums [ col ] = { \"Total\" : str ( total_pop ), \"Served\" : str ( servd_pop ), \"Unserved\" : str ( unsrvd_pop ), \"Percentage served\" : \"None\" , \"Percentage unserved\" : \"None\" } elif total_pop > 0 : pop_sums [ col ] = _calc_proprtn_srvd_unsrvd ( total_pop , servd_pop , unsrvd_pop ) # Make a df from the total and served pop tot_servd_df = pd . DataFrame ( pop_sums ) return tot_servd_df slice_age_df ( df , col_nms ) Slices a dataframe according to the list of column names provided. Parameters: df ( DataFrame ) \u2013 DataFrame to be sliced. col_nms ( List [ str ] ) \u2013 column names as string in a list. Returns: \u2013 pd.DataFrame: A dataframe sliced down to only the columns required. Source code in src/data_transform.py 26 27 28 29 30 31 32 33 34 35 36 37 def slice_age_df ( df : pd . DataFrame , col_nms : List [ str ]): \"\"\"Slices a dataframe according to the list of column names provided. Args: df (pd.DataFrame): DataFrame to be sliced. col_nms (List[str]): column names as string in a list. Returns: pd.DataFrame: A dataframe sliced down to only the columns required. \"\"\" age_df = df . loc [:, col_nms ] return age_df urban_rural_results ( la_pop_df , pop_in_poly_df , urb_rur_dict , local_auth ) Creates two dataframes, urban and rural and classifies proportion of people served and not served by public transport. This is placed into a urban rural dictionary for each local authority of interest for the final csv output. Parameters: la_pop_df ( GeoDataFrame ) \u2013 GeoPandas Dataframe that includes output area codes and population estimates. pop_in_poly_df ( GeoDataFrame ) \u2013 A geodata frame with the points inside the polygon. urb_rur_dict ( dict ) \u2013 Dictionary to store the urban rural dataframe. local_auth ( str ) \u2013 The local authority of interest. Returns: urb_rur_dict ( dict ) \u2013 Dictionary with a disability total dataframe for unserved and served populations for all given local authorities. Source code in src/data_transform.py 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 def urban_rural_results ( la_pop_df , pop_in_poly_df , urb_rur_dict , local_auth ): \"\"\" Creates two dataframes, urban and rural and classifies proportion of people served and not served by public transport. This is placed into a urban rural dictionary for each local authority of interest for the final csv output. Args: la_pop_df (gpd.GeoDataFrame): GeoPandas Dataframe that includes output area codes and population estimates. pop_in_poly_df (gpd.GeoDataFrame): A geodata frame with the points inside the polygon. urb_rur_dict (dict): Dictionary to store the urban rural dataframe. local_auth (str): The local authority of interest. Returns: urb_rur_dict (dict): Dictionary with a disability total dataframe for unserved and served populations for all given local authorities. \"\"\" # Urban/Rural disaggregation # split into two different dataframes urb_df = la_pop_df [ la_pop_df . urb_rur_class == \"urban\" ] rur_df = la_pop_df [ la_pop_df . urb_rur_class == \"rural\" ] urb_df_poly = pop_in_poly_df [ pop_in_poly_df . urb_rur_class == \"urban\" ] rur_df_poly = pop_in_poly_df [ pop_in_poly_df . urb_rur_class == \"rural\" ] urb_servd_df = served_proportions_disagg ( pop_df = urb_df , pop_in_poly_df = urb_df_poly , cols_lst = [ 'pop_count' ]) rur_servd_df = served_proportions_disagg ( pop_df = rur_df , pop_in_poly_df = rur_df_poly , cols_lst = [ 'pop_count' ]) # Renaming pop_count to either urban or rural urb_servd_df . rename ( columns = { \"pop_count\" : \"Urban\" }, inplace = True ) rur_servd_df . rename ( columns = { \"pop_count\" : \"Rural\" }, inplace = True ) # Sending each to reshaper urb_servd_df_out = do . reshape_for_output ( urb_servd_df , id_col = \"Urban\" , local_auth = local_auth ) rur_servd_df_out = do . reshape_for_output ( rur_servd_df , id_col = \"Rural\" , local_auth = local_auth ) # Renaming their columns to Urban/Rural urb_servd_df_out . rename ( columns = { \"Urban\" : \"Urban/Rural\" }, inplace = True ) rur_servd_df_out . rename ( columns = { \"Rural\" : \"Urban/Rural\" }, inplace = True ) # Combining urban and rural dfs urb_rur_servd_df_out = pd . concat ([ urb_servd_df_out , rur_servd_df_out ]) # Output this iteration's urb and rur df to the dict urb_rur_dict [ local_auth ] = urb_rur_servd_df_out return urb_rur_dict","title":"Data transform"},{"location":"data_transform/#src.data_transform.bin_pop_ages","text":"Bins the ages in the age_df in 5 year spans from 0 to 90+, sums the counts in those bins and drops the original age columns. Parameters: df ( DataFrame ) \u2013 A dataframe of population data containing only the age columns. Returns: \u2013 pd.DataFrame: Returns the age_df with bins. Source code in src/data_transform.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 def bin_pop_ages ( age_df , age_bins , col_nms ): \"\"\" Bins the ages in the age_df in 5 year spans from 0 to 90+, sums the counts in those bins and drops the original age columns. Args: df (pd.DataFrame): A dataframe of population data containing only the age columns. Returns: pd.DataFrame: Returns the age_df with bins. \"\"\" # Grouping ages in 5 year brackets # cleaning scottish data and changing dtype to float original_columns = age_df . columns for col in original_columns : if age_df [ col ] . dtypes == \"O\" : age_df [ col ] = age_df [ col ] . str . replace ( '-' , '0' ) age_df [ col ] = age_df [ col ] . astype ( int ) def _age_bin ( age_df , age_bins ): \"\"\"Function sums the counts for corresponding age-bins and assigns them a column in age_df.\"\"\" for bin in age_bins : age_df [ f \" { bin [ 0 ] } - { bin [ 1 ] } \" ] = ( age_df . loc [:, bin [ 0 ]: bin [ 1 ]] . sum ( axis = 1 )) return age_df # create 90+ column for when there are more columns than 90 if len ( age_df . columns ) > 91 : # create 90+ column summing all those from 90 and above. age_df [ '90+' ] = age_df . iloc [:, 90 :] . sum ( axis = 1 ) age_df = _age_bin ( age_df , age_bins ) # drop the original age columns age_df . drop ( col_nms , axis = 1 , inplace = True ) # drop the columns that we are replacing with 90+ age_df . drop ( age_df . iloc [:, 19 :], axis = 1 , inplace = True ) # moving first column to last so 90+ at the end. temp_cols = age_df . columns . tolist () new_cols = temp_cols [ 1 :] + temp_cols [ 0 : 1 ] age_df = age_df [ new_cols ] else : age_df = _age_bin ( age_df , age_bins ) # drop the original age columns age_df . drop ( col_nms , axis = 1 , inplace = True ) # rename the 90+ column age_df . rename ( columns = { '90+-90+' : '90+' }, inplace = True ) # age df has now been binned and cleaned return round ( age_df )","title":"bin_pop_ages"},{"location":"data_transform/#src.data_transform.convert_east_north","text":"Converts latitude and longitude coordinates to British National Grid Args: df (pd.DataFrame): df including the longitude and latitude coordinates long(str): The name of the longitude column in df lat (str): The name of the latitude column in df Returns: pd.DataFrame: dataframe including easting and northing coordinates. Source code in src/data_transform.py 438 439 440 441 442 443 444 445 446 447 448 449 def convert_east_north ( df , long , lat ): \"\"\" Converts latitude and longitude coordinates to British National Grid Args: df (pd.DataFrame): df including the longitude and latitude coordinates long(str): The name of the longitude column in df lat (str): The name of the latitude column in df Returns: pd.DataFrame: dataframe including easting and northing coordinates. \"\"\" df [ 'Easting' ], df [ 'Northing' ] = convert_bng ( df [ long ], df [ lat ]) return df","title":"convert_east_north"},{"location":"data_transform/#src.data_transform.create_tiploc_col","text":"Creates a Tiploc column from the ATCOCode column, in the NaPTAN dataset. Parameters: naptan_df ( Dataframe ) \u2013 Naptan dataset Returns: \u2013 pd.Dataframe (naptan_df): Naptan dataset with the new tiploc column \u2013 added for train stations Source code in src/data_transform.py 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 def create_tiploc_col ( naptan_df ): \"\"\"Creates a Tiploc column from the ATCOCode column, in the NaPTAN dataset. Args: naptan_df (pd.Dataframe): Naptan dataset Returns: pd.Dataframe (naptan_df): Naptan dataset with the new tiploc column added for train stations \"\"\" # Applying only to train stations, RLY is the stop type for train stations rail_filter = naptan_df . StopType == \"RLY\" # Create a new pd.Dataframe for Tiploc by extracting upto 7 alpha # characters tiploc_col = ( naptan_df . loc [ rail_filter ] . ATCOCode . str . extract ( r '([A-Za-z]{1,7})' ) ) tiploc_col . columns = [ \"tiploc_code\" ] # Merge the new Tiploc column with the naptan_df naptan_df = naptan_df . merge ( tiploc_col , how = 'left' , left_index = True , right_index = True ) return naptan_df","title":"create_tiploc_col"},{"location":"data_transform/#src.data_transform.disab_dict","text":"Creates the dataframe including those who are and are not served by public transport and places it into a disability dictionary for each local authority of interest for the final csv output. Parameters: la_pop_df ( GeoDataFrame ) \u2013 GeoPandas Dataframe that includes output area codes and population estimates. pop_in_poly_df ( GeoDataFrame ) \u2013 A geodata frame with the points inside the polygon. disability_dict ( dict ) \u2013 Dictionary to store the disability dataframe. local_auth ( str ) \u2013 The local authority of interest. Returns: disability_dict ( dict ) \u2013 Dictionary with a disability total dataframe for unserved and served populations for all given local authorities. Source code in src/data_transform.py 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 def disab_dict ( la_pop_df , pop_in_poly_df , disability_dict , local_auth ): \"\"\"Creates the dataframe including those who are and are not served by public transport and places it into a disability dictionary for each local authority of interest for the final csv output. Args: la_pop_df (gpd.GeoDataFrame): GeoPandas Dataframe that includes output area codes and population estimates. pop_in_poly_df (gpd.GeoDataFrame): A geodata frame with the points inside the polygon. disability_dict (dict): Dictionary to store the disability dataframe. local_auth (str): The local authority of interest. Returns: disability_dict (dict): Dictionary with a disability total dataframe for unserved and served populations for all given local authorities. \"\"\" # Calculating those served and not served by disability disab_cols = [ \"number_disabled\" ] disab_servd_df = served_proportions_disagg ( la_pop_df , pop_in_poly_df , disab_cols ) # Feeding the results to the reshaper disab_servd_df_out = do . reshape_for_output ( disab_servd_df , id_col = disab_cols [ 0 ], local_auth = local_auth , id_rename = \"Disability Status\" ) # The disability df is unusual. I think all rows correspond to people with # disabilities only. There is no \"not-disabled\" status here (I think) disab_servd_df_out . replace ( to_replace = \"number_disabled\" , value = \"Disabled\" , inplace = True ) # Calculating non-disabled people served and not served non_disab_cols = [ \"number_non-disabled\" ] non_disab_servd_df = served_proportions_disagg ( pop_df = la_pop_df , pop_in_poly_df = pop_in_poly_df , cols_lst = non_disab_cols ) # Feeding the results to the reshaper non_disab_servd_df_out = do . reshape_for_output ( non_disab_servd_df , id_col = disab_cols [ 0 ], local_auth = local_auth , id_rename = \"Disability Status\" ) # The disability df is unusual. I think all rows correspond to people with # disabilities only. There is no \"not-disabled\" status here (I think) non_disab_servd_df_out . replace ( to_replace = \"number_non-disabled\" , value = \"Non-disabled\" , inplace = True ) # Concatting non-disabled and disabled dataframes non_disab_disab_servd_df_out = pd . concat ( [ non_disab_servd_df_out , disab_servd_df_out ]) # Output this local auth's disab df to the dict disability_dict [ local_auth ] = non_disab_disab_servd_df_out return disability_dict","title":"disab_dict"},{"location":"data_transform/#src.data_transform.disab_disagg","text":"Calculates number of people in the population that are classified as disabled or not disabled and this is merged onto the local authority population dataframe. Args: disability_df (pd.DataFrame): Dataframe that includes disability estimates for each output area. la_pop_df (gpd.GeoDataFrame): GeoPandas Dataframe that includes output area codes and population estimates. Returns: gpd.GeoDataFrame: GeoPandas Dataframe that includes population estimates, geographical location, and proportion of disabled/non-disabled for each output area in the local authority chosen. Source code in src/data_transform.py 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 def disab_disagg ( disability_df , la_pop_df ): \"\"\"Calculates number of people in the population that are classified as disabled or not disabled and this is merged onto the local authority population dataframe. Args: disability_df (pd.DataFrame): Dataframe that includes disability estimates for each output area. la_pop_df (gpd.GeoDataFrame): GeoPandas Dataframe that includes output area codes and population estimates. Returns: gpd.GeoDataFrame: GeoPandas Dataframe that includes population estimates, geographical location, and proportion of disabled/non-disabled for each output area in the local authority chosen. \"\"\" # Getting the disab total disability_df [ \"disb_total\" ] = ( disability_df [ \"disab_ltd_lot\" ] + disability_df [ \"disab_ltd_little\" ]) # Calcualting the total \"non-disabled\" la_pop_only = la_pop_df [[ 'OA11CD' , 'pop_count' ]] disability_df = la_pop_only . merge ( disability_df , on = \"OA11CD\" ) # Putting the result back into the disability df disability_df [ \"non-disabled\" ] = disability_df [ \"pop_count\" ] - \\ disability_df [ 'disb_total' ] # Calculating the proportion of disabled people in each OA disability_df [ \"proportion_disabled\" ] = ( disability_df [ 'disb_total' ] / disability_df [ 'pop_count' ] ) # Calcualting the proportion of non-disabled people in each OA disability_df [ \"proportion_non-disabled\" ] = ( disability_df [ 'non-disabled' ] / disability_df [ 'pop_count' ] ) # Slice disability df that only has the proportion disabled column and the # OA11CD col disab_prop_df = disability_df [[ 'OA11CD' , 'proportion_disabled' , 'proportion_non-disabled' ]] # Merge the proportion disability df into main the pop df with a left join la_pop_df = la_pop_df . merge ( disab_prop_df , on = 'OA11CD' , how = \"left\" ) # Make the calculation of the number of people with disabilities in the # year of the population estimates la_pop_df [ \"number_disabled\" ] = ( round ( la_pop_df [ \"pop_count\" ] * la_pop_df [ \"proportion_disabled\" ]) ) # la_pop_df[\"number_disabled\"] = la_pop_df[\"number_disabled\"].astype(int) # Make the calculation of the number of non-disabled people in the year # of the population estimates la_pop_df [ \"number_non-disabled\" ] = ( round ( la_pop_df [ \"pop_count\" ] * la_pop_df [ \"proportion_non-disabled\" ]) ) la_pop_df [ \"number_non-disabled\" ] = la_pop_df [ \"number_non-disabled\" ] . astype ( int ) return la_pop_df","title":"disab_disagg"},{"location":"data_transform/#src.data_transform.get_col_bins","text":"Function to group/bin the ages together in 5 year steps. Starts the sequence at age 0. Will return the ages in a list of tuples. The 0th position of the tuple being the lower limit of the age bin, the 1st position of the tuple being the upper limit. Parameters: col_nms ( list of str ) \u2013 a list of the age columns as strings. Returns: \u2013 list of tuples: a list of the ages with 5 year gaps. Source code in src/data_transform.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def get_col_bins ( col_nms : List [ str ]): \"\"\"Function to group/bin the ages together in 5 year steps. Starts the sequence at age 0. Will return the ages in a list of tuples. The 0th position of the tuple being the lower limit of the age bin, the 1st position of the tuple being the upper limit. Args: col_nms (list of str): a list of the age columns as strings. Returns: list of tuples: a list of the ages with 5 year gaps. \"\"\" # Make a lists of starting and finishing indexes cols_start = col_nms [ 0 :: 5 ] cols_fin = col_nms [ 4 :: 5 ] # Generating a list of tuples which will be the age groupings col_bins = [( s , f ) for s , f in zip ( cols_start , cols_fin )] # Again adding \"90+\", doubling it so it's doubled, like the other tuples col_bins . append (( cols_start [ - 1 :] * 2 )) # TODO: make this more intelligent. Only if there is one col name left # over it should be doubled. return col_bins","title":"get_col_bins"},{"location":"data_transform/#src.data_transform.mid_year_age_estimates","text":"Takes mid-year population estimates for each small area in NI and uses proportions to calculate mid-year estimates for each age. Args: age_df (pd.DataFrame): Census 2011 age estimates dataframe pop_estimates_df (pd.DataFrame): population estimates for each small area dataframe pop_year (str): population year Source code in src/data_transform.py 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 def mid_year_age_estimates ( age_df , pop_estimates_df , pop_year ): \"\"\" Takes mid-year population estimates for each small area in NI and uses proportions to calculate mid-year estimates for each age. Args: age_df (pd.DataFrame): Census 2011 age estimates dataframe pop_estimates_df (pd.DataFrame): population estimates for each small area dataframe pop_year (str): population year \"\"\" # get all age columns in a list age_cols = [ str ( y ) for y in range ( 101 )] # iterate through each age for age in range ( len ( age_cols )): # calculates the proportions for each age and each SA code age_df [ age_cols [ age ]] = age_df [ age_cols [ age ]] / age_df [ 'All usual residents' ] age_df . drop ([ 'SA' , 'All usual residents' ], axis = 1 , inplace = True ) # merges pop df and proportions together pop_estimates_df = pop_estimates_df . merge ( age_df . reset_index (), left_on = 'Area_Code' , right_on = 'SA Code' , how = 'left' ) # calculates pop estimates for each age in each small area using proportions for age in range ( len ( age_cols )): pop_estimates_df [ age_cols [ age ]] = pop_estimates_df [ age_cols [ age ]] * pop_estimates_df [ pop_year ] pop_estimates_df . set_index ( 'Area_Code' , inplace = True ) return pop_estimates_df","title":"mid_year_age_estimates"},{"location":"data_transform/#src.data_transform.served_proportions_disagg","text":"Calculates the number of people in each category, as specified by the column (e.g age range, or disability status) who are served and not served by public transport, and gives those as a proportion of the total. Note: the numeric values in the dataframe are return as strings for formatting reasons Parameters: pop_df ( DataFrame ) \u2013 population dataframe. pop_in_poly_df ( DataFrame ) \u2013 dataframe resulting in the points in polygons enquiry to count (sum) the population within the service area polygon. cols_lst ( List [ str ] ) \u2013 a list of the column names in the population dataframe supplied which contain population figures, and are to be summed and assessed as served/unserved by public transport. Returns: \u2013 pd.DataFrame: a dataframe summarising \u2013 i) the total number of people that column (e.g. age range, sex) \u2013 ii) the number served by public transport \u2013 iii) the proportion who are served by public transport \u2013 iv) the proportion who are not served by public transport Source code in src/data_transform.py 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 def served_proportions_disagg ( pop_df : pd . DataFrame , pop_in_poly_df : pd . DataFrame , cols_lst : List [ str ]): \"\"\"Calculates the number of people in each category, as specified by the column (e.g age range, or disability status) who are served and not served by public transport, and gives those as a proportion of the total. Note: the numeric values in the dataframe are return as strings for formatting reasons Args: pop_df (pd.DataFrame): population dataframe. pop_in_poly_df (pd.DataFrame): dataframe resulting in the points in polygons enquiry to count (sum) the population within the service area polygon. cols_lst (List[str]): a list of the column names in the population dataframe supplied which contain population figures, and are to be summed and assessed as served/unserved by public transport. Returns: pd.DataFrame: a dataframe summarising i) the total number of people that column (e.g. age range, sex) ii) the number served by public transport iii) the proportion who are served by public transport iv) the proportion who are not served by public transport \"\"\" # First list the age bin columns pop_sums = {} for col in cols_lst : # Total pop total_pop = int ( pop_df [ col ] . sum ()) # Served pop servd_pop = int ( pop_in_poly_df [ col ] . sum ()) # Unserved pop unsrvd_pop = int ( total_pop - servd_pop ) if total_pop == 0 : # If the total population for that column is 0 # this standard of zeros and Nones is returned pop_sums [ col ] = { \"Total\" : str ( total_pop ), \"Served\" : str ( servd_pop ), \"Unserved\" : str ( unsrvd_pop ), \"Percentage served\" : \"None\" , \"Percentage unserved\" : \"None\" } elif total_pop > 0 : pop_sums [ col ] = _calc_proprtn_srvd_unsrvd ( total_pop , servd_pop , unsrvd_pop ) # Make a df from the total and served pop tot_servd_df = pd . DataFrame ( pop_sums ) return tot_servd_df","title":"served_proportions_disagg"},{"location":"data_transform/#src.data_transform.slice_age_df","text":"Slices a dataframe according to the list of column names provided. Parameters: df ( DataFrame ) \u2013 DataFrame to be sliced. col_nms ( List [ str ] ) \u2013 column names as string in a list. Returns: \u2013 pd.DataFrame: A dataframe sliced down to only the columns required. Source code in src/data_transform.py 26 27 28 29 30 31 32 33 34 35 36 37 def slice_age_df ( df : pd . DataFrame , col_nms : List [ str ]): \"\"\"Slices a dataframe according to the list of column names provided. Args: df (pd.DataFrame): DataFrame to be sliced. col_nms (List[str]): column names as string in a list. Returns: pd.DataFrame: A dataframe sliced down to only the columns required. \"\"\" age_df = df . loc [:, col_nms ] return age_df","title":"slice_age_df"},{"location":"data_transform/#src.data_transform.urban_rural_results","text":"Creates two dataframes, urban and rural and classifies proportion of people served and not served by public transport. This is placed into a urban rural dictionary for each local authority of interest for the final csv output. Parameters: la_pop_df ( GeoDataFrame ) \u2013 GeoPandas Dataframe that includes output area codes and population estimates. pop_in_poly_df ( GeoDataFrame ) \u2013 A geodata frame with the points inside the polygon. urb_rur_dict ( dict ) \u2013 Dictionary to store the urban rural dataframe. local_auth ( str ) \u2013 The local authority of interest. Returns: urb_rur_dict ( dict ) \u2013 Dictionary with a disability total dataframe for unserved and served populations for all given local authorities. Source code in src/data_transform.py 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 def urban_rural_results ( la_pop_df , pop_in_poly_df , urb_rur_dict , local_auth ): \"\"\" Creates two dataframes, urban and rural and classifies proportion of people served and not served by public transport. This is placed into a urban rural dictionary for each local authority of interest for the final csv output. Args: la_pop_df (gpd.GeoDataFrame): GeoPandas Dataframe that includes output area codes and population estimates. pop_in_poly_df (gpd.GeoDataFrame): A geodata frame with the points inside the polygon. urb_rur_dict (dict): Dictionary to store the urban rural dataframe. local_auth (str): The local authority of interest. Returns: urb_rur_dict (dict): Dictionary with a disability total dataframe for unserved and served populations for all given local authorities. \"\"\" # Urban/Rural disaggregation # split into two different dataframes urb_df = la_pop_df [ la_pop_df . urb_rur_class == \"urban\" ] rur_df = la_pop_df [ la_pop_df . urb_rur_class == \"rural\" ] urb_df_poly = pop_in_poly_df [ pop_in_poly_df . urb_rur_class == \"urban\" ] rur_df_poly = pop_in_poly_df [ pop_in_poly_df . urb_rur_class == \"rural\" ] urb_servd_df = served_proportions_disagg ( pop_df = urb_df , pop_in_poly_df = urb_df_poly , cols_lst = [ 'pop_count' ]) rur_servd_df = served_proportions_disagg ( pop_df = rur_df , pop_in_poly_df = rur_df_poly , cols_lst = [ 'pop_count' ]) # Renaming pop_count to either urban or rural urb_servd_df . rename ( columns = { \"pop_count\" : \"Urban\" }, inplace = True ) rur_servd_df . rename ( columns = { \"pop_count\" : \"Rural\" }, inplace = True ) # Sending each to reshaper urb_servd_df_out = do . reshape_for_output ( urb_servd_df , id_col = \"Urban\" , local_auth = local_auth ) rur_servd_df_out = do . reshape_for_output ( rur_servd_df , id_col = \"Rural\" , local_auth = local_auth ) # Renaming their columns to Urban/Rural urb_servd_df_out . rename ( columns = { \"Urban\" : \"Urban/Rural\" }, inplace = True ) rur_servd_df_out . rename ( columns = { \"Rural\" : \"Urban/Rural\" }, inplace = True ) # Combining urban and rural dfs urb_rur_servd_df_out = pd . concat ([ urb_servd_df_out , rur_servd_df_out ]) # Output this iteration's urb and rur df to the dict urb_rur_dict [ local_auth ] = urb_rur_servd_df_out return urb_rur_dict","title":"urban_rural_results"},{"location":"developers_guide/","text":"For Developers Requirements A number of problems with dependencies have been experienced while developing this, so it is strongly recommended that you use a virtual environment (either conda or venv) and use the provided requirements.txt to install the needed versions of packages. Note for ONS staff: It is unlikely that you will be able to install all the needed dependencies to run this script, therefore it is recommended that your devlopment work is carried out on an off-network computer. Before starting this process, please ensure that Anaconda3 and Git are installed. We recommend running the script using VSCode, as this is what we use in these instructions, and is downloadable here . If you are using Windows, you will have to add the conda and python path into your Windows Path environment. For guidance, follow the tutorial here . Cloning the repository The first step is setting up your SSH key for GitHub. The process will slightly vary depending on what OS you are running from. Here are useful tutorials for Windows 10 or Mac and Linux . Further information on adding your SSH key to GitHub is linked here . You should now have your SSH key set up. To clone the repository, we need to first go to the project directory (where you would like it saved on your local drive). $ cd project-directory Then activate use the SSH address to clone the repository $ git clone SSH_address You can then open the folder SDG_11.2.1 within VSCode using \"Open Folder\" in Source Control. Create an environment Virtual environments are extremely useful when working on different projects as they can be set up in a way to only have packages installed in that environment and not globally - it does not affect base Python installation in any way. Create an environment called \"SDG_11.2.1\" with the version of Python that this was developed in conda create --name SDG_11.2.1 python=3.8 Troubleshooting: When creating the environment on Windows, it might fail with a HTTP error when trying to fetch package metadata. Following these steps should resolve the issue. Activate the environment This tutorial under the 'Manually specify an interpreter' section allows you to add in a virtual environment to use. To select an interpreter, use CTRL+SHIFT+P or command+SHIFT+P then write 'Python: Select Interpreter' to which you can add a file path similar to '.venv/Scripts/python.exe'. You can also use the Find button to locate the specific file in your file system. After this has been set up, go to the project directory/wherever you have saved to on your local drive. On Windows this may look like: $ cd C:\\Users\\name\\project-directory Then activate the environment $ conda activate SDG_11.2.1 The above command will not work for Windows unless you have the Conda path linked to your Windows path environment (see this tutorial ). If you do not have the Conda path linked, you can use the following for Windows: $ activate SDG_11.2.1 Then you should see the environment name in brackets before the prompt, similar to: (SDG_11.2.1) $ Troubleshooting: Activating the environment in the Powershell terminal does not work for Windows. Set the default terminal to Command Prompt and activate from here. Press CTRL+Shift+P and search for 'Terminal: Configure Terminal Settings'. Under the 'Terminal > External: Windows Exec' section enter the path to your cmd. E.g. C:\\WINDOWS\\System32\\cmd.exe Make sure you are using the correct Python interpreter by checking your Python path: In Linux: $ which python And in Windows use the following command in cmd or Anaconda prompt: $ where python Which should return something like: C:\\Python36\\envs\\SDG_11.2.1\\python.exe Showing your are using the Python from the virtual environment, not the base installation of Python. Installing dependencies First, ensure you are in the project directory $ cd C:\\Users\\name\\project-directory To install the requirements write conda install --file requirements.txt This may throw an error if you do not have all the packages required PackageNotFoundError: The following packages are not available from current channels If this does, write the following with the package names that the error has shown you are missing: pip install packagename The script should now be set up to use. Setting Git configuration To be able to contribute to the project via Git, you will need to add the email and user name associated to your account to the config file git config --global user.email \"email\" git config --global user.name \"username\" Pre-commit hooks In this project, we use pre-commit hooks to stop us from accidentally uploading files to GitHub. We have the .pre-commit-config.yaml which has the set up for the pre-commit hooks you will need. However, to ensure that they are active, it is essential to set up pre-commit hooks. The instructions below follows this guidance . You should have pre-commit installed as it is part of requirements.txt . You can check this by writing pip freeze on the command line which shows all the python packages installed in your virtual environment. If this has not installed, write: pip install pre-commit To set up the git hook scripts from the .pre-commit-config.yaml , run: pre-commit install Now this should have the hooks running when you try to commit. You can test this out by creating dummy data and trying to commit this to GitHub.","title":"For Developers"},{"location":"developers_guide/#for-developers","text":"","title":"For Developers"},{"location":"developers_guide/#requirements","text":"A number of problems with dependencies have been experienced while developing this, so it is strongly recommended that you use a virtual environment (either conda or venv) and use the provided requirements.txt to install the needed versions of packages. Note for ONS staff: It is unlikely that you will be able to install all the needed dependencies to run this script, therefore it is recommended that your devlopment work is carried out on an off-network computer. Before starting this process, please ensure that Anaconda3 and Git are installed. We recommend running the script using VSCode, as this is what we use in these instructions, and is downloadable here . If you are using Windows, you will have to add the conda and python path into your Windows Path environment. For guidance, follow the tutorial here .","title":"Requirements"},{"location":"developers_guide/#cloning-the-repository","text":"The first step is setting up your SSH key for GitHub. The process will slightly vary depending on what OS you are running from. Here are useful tutorials for Windows 10 or Mac and Linux . Further information on adding your SSH key to GitHub is linked here . You should now have your SSH key set up. To clone the repository, we need to first go to the project directory (where you would like it saved on your local drive). $ cd project-directory Then activate use the SSH address to clone the repository $ git clone SSH_address You can then open the folder SDG_11.2.1 within VSCode using \"Open Folder\" in Source Control.","title":"Cloning the repository"},{"location":"developers_guide/#create-an-environment","text":"Virtual environments are extremely useful when working on different projects as they can be set up in a way to only have packages installed in that environment and not globally - it does not affect base Python installation in any way. Create an environment called \"SDG_11.2.1\" with the version of Python that this was developed in conda create --name SDG_11.2.1 python=3.8 Troubleshooting: When creating the environment on Windows, it might fail with a HTTP error when trying to fetch package metadata. Following these steps should resolve the issue.","title":"Create an environment"},{"location":"developers_guide/#activate-the-environment","text":"This tutorial under the 'Manually specify an interpreter' section allows you to add in a virtual environment to use. To select an interpreter, use CTRL+SHIFT+P or command+SHIFT+P then write 'Python: Select Interpreter' to which you can add a file path similar to '.venv/Scripts/python.exe'. You can also use the Find button to locate the specific file in your file system. After this has been set up, go to the project directory/wherever you have saved to on your local drive. On Windows this may look like: $ cd C:\\Users\\name\\project-directory Then activate the environment $ conda activate SDG_11.2.1 The above command will not work for Windows unless you have the Conda path linked to your Windows path environment (see this tutorial ). If you do not have the Conda path linked, you can use the following for Windows: $ activate SDG_11.2.1 Then you should see the environment name in brackets before the prompt, similar to: (SDG_11.2.1) $ Troubleshooting: Activating the environment in the Powershell terminal does not work for Windows. Set the default terminal to Command Prompt and activate from here. Press CTRL+Shift+P and search for 'Terminal: Configure Terminal Settings'. Under the 'Terminal > External: Windows Exec' section enter the path to your cmd. E.g. C:\\WINDOWS\\System32\\cmd.exe Make sure you are using the correct Python interpreter by checking your Python path: In Linux: $ which python And in Windows use the following command in cmd or Anaconda prompt: $ where python Which should return something like: C:\\Python36\\envs\\SDG_11.2.1\\python.exe Showing your are using the Python from the virtual environment, not the base installation of Python.","title":"Activate the environment"},{"location":"developers_guide/#installing-dependencies","text":"First, ensure you are in the project directory $ cd C:\\Users\\name\\project-directory To install the requirements write conda install --file requirements.txt This may throw an error if you do not have all the packages required PackageNotFoundError: The following packages are not available from current channels If this does, write the following with the package names that the error has shown you are missing: pip install packagename The script should now be set up to use.","title":"Installing dependencies"},{"location":"developers_guide/#setting-git-configuration","text":"To be able to contribute to the project via Git, you will need to add the email and user name associated to your account to the config file git config --global user.email \"email\" git config --global user.name \"username\"","title":"Setting Git configuration"},{"location":"developers_guide/#pre-commit-hooks","text":"In this project, we use pre-commit hooks to stop us from accidentally uploading files to GitHub. We have the .pre-commit-config.yaml which has the set up for the pre-commit hooks you will need. However, to ensure that they are active, it is essential to set up pre-commit hooks. The instructions below follows this guidance . You should have pre-commit installed as it is part of requirements.txt . You can check this by writing pip freeze on the command line which shows all the python packages installed in your virtual environment. If this has not installed, write: pip install pre-commit To set up the git hook scripts from the .pre-commit-config.yaml , run: pre-commit install Now this should have the hooks running when you try to commit. You can test this out by creating dummy data and trying to commit this to GitHub.","title":"Pre-commit hooks"},{"location":"for_developers/","text":"For Developers Requirements A number of problems with dependencies have been experienced while developing this, so it is strongly recommended that you use a virtual environment (either conda or venv) and use the provided requirements.txt to install the needed versions of packages. Note for ONS staff: It is unlikely that you will be able to install all the needed dependencies to run this script, therefore it is recommended that your devlopment work is carried out on an off-network computer. Before starting this process, please ensure that Anaconda3 and Git are installed. We recommend running the script using VSCode, as this is what we use in these instructions, and is downloadable here . If you are using Windows, you will have to add the conda and python path into your Windows Path environment. For guidance, follow the tutorial here . Cloning the repository The first step is setting up your SSH key for GitHub. The process will slightly vary depending on what OS you are running from. Here are useful tutorials for Windows 10 or Mac and Linux . You should now have your SSH key set up. To clone the repository, we need to first go to the project directory (where you would like it saved on your local drive). $ cd project-directory Then activate use the SSH address to clone the repository $ git clone SSH_address You can then open the folder SDG_11.2.1 within VSCode using \"Open Folder\" in Source Control. Create an environment Virtual environments are extremely useful when working on different projects as they can be set up in a way to only have packages installed in that environment and not globally - it does not affect base Python installation in any way. Create an environment called \"SDG_11.2.1\" with the version of Python that this was developed in conda create --name SDG_11.2.1 python=3.10 Troubleshooting: When creating the environment on Windows, it might fail with a HTTP error when trying to fetch package metadata. Following these steps should resolve the issue. Activate the environment First go to the project directory/wherever you have saved to on your local drive. On Windows this may look like: $ cd C:\\Users\\name\\project-directory Then activate the environment $ conda activate SDG_11.2.1 The above command will not work for Windows unless you have the Conda path linked to your Windows path environment (see this tutorial ). If you do not have the Conda path linked, you can use the following for Windows: $ activate SDG_11.2.1 Then you should see the environment name in brackets before the prompt, similar to: (SDG_11.2.1) $ Troubleshooting: Activating the environment in the Powershell terminal does not work for Windows. Set the default terminal to Command Prompt and activate from here. Press CTRL+Shift+P and search for 'Terminal: Configure Terminal Settings'. Under the 'Terminal > External: Windows Exec' section enter the path to your cmd. E.g. C:\\WINDOWS\\System32\\cmd.exe Make sure you are using the correct Python interpreter by checking your Python path: In Linux: $ which python And in Windows use the following command in cmd or Anaconda prompt: $ where python Which should return something like: C:\\Python36\\envs\\SDG_11.2.1\\python.exe Showing your are using the Python from the virtual environment, not the base installation of Python. Installing dependencies First, ensure you are in the project directory $ cd C:\\Users\\name\\project-directory To install the requirements write conda install --file requirements.txt This may throw an error if you do not have all the packages required PackageNotFoundError: The following packages are not available from current channels If this does, write the following with the package names that the error has shown you are missing: pip install packagename The script should now be set up to use. Setting Git configuration To be able to contribute to the project via Git, you will need to add the email and user name associated to your account to the config file git config --global user.email \"email\" git config --global user.name \"username\" Order to run the script In order to generate the output data, run the scripts in the order given below from the main directory of the repository. time_table directory SDG_bus_timetable.py SDG_train_timetable.py pre_processing directory eng_wales_pre_process.py After running the scripts above, we can finally run the main scripts given below (can be found in main directory of the repository) SDG_eng_wales.py SDG_scotland.py SDG_ni.py","title":"For Developers"},{"location":"for_developers/#for-developers","text":"","title":"For Developers"},{"location":"for_developers/#requirements","text":"A number of problems with dependencies have been experienced while developing this, so it is strongly recommended that you use a virtual environment (either conda or venv) and use the provided requirements.txt to install the needed versions of packages. Note for ONS staff: It is unlikely that you will be able to install all the needed dependencies to run this script, therefore it is recommended that your devlopment work is carried out on an off-network computer. Before starting this process, please ensure that Anaconda3 and Git are installed. We recommend running the script using VSCode, as this is what we use in these instructions, and is downloadable here . If you are using Windows, you will have to add the conda and python path into your Windows Path environment. For guidance, follow the tutorial here .","title":"Requirements"},{"location":"for_developers/#cloning-the-repository","text":"The first step is setting up your SSH key for GitHub. The process will slightly vary depending on what OS you are running from. Here are useful tutorials for Windows 10 or Mac and Linux . You should now have your SSH key set up. To clone the repository, we need to first go to the project directory (where you would like it saved on your local drive). $ cd project-directory Then activate use the SSH address to clone the repository $ git clone SSH_address You can then open the folder SDG_11.2.1 within VSCode using \"Open Folder\" in Source Control.","title":"Cloning the repository"},{"location":"for_developers/#create-an-environment","text":"Virtual environments are extremely useful when working on different projects as they can be set up in a way to only have packages installed in that environment and not globally - it does not affect base Python installation in any way. Create an environment called \"SDG_11.2.1\" with the version of Python that this was developed in conda create --name SDG_11.2.1 python=3.10 Troubleshooting: When creating the environment on Windows, it might fail with a HTTP error when trying to fetch package metadata. Following these steps should resolve the issue.","title":"Create an environment"},{"location":"for_developers/#activate-the-environment","text":"First go to the project directory/wherever you have saved to on your local drive. On Windows this may look like: $ cd C:\\Users\\name\\project-directory Then activate the environment $ conda activate SDG_11.2.1 The above command will not work for Windows unless you have the Conda path linked to your Windows path environment (see this tutorial ). If you do not have the Conda path linked, you can use the following for Windows: $ activate SDG_11.2.1 Then you should see the environment name in brackets before the prompt, similar to: (SDG_11.2.1) $ Troubleshooting: Activating the environment in the Powershell terminal does not work for Windows. Set the default terminal to Command Prompt and activate from here. Press CTRL+Shift+P and search for 'Terminal: Configure Terminal Settings'. Under the 'Terminal > External: Windows Exec' section enter the path to your cmd. E.g. C:\\WINDOWS\\System32\\cmd.exe Make sure you are using the correct Python interpreter by checking your Python path: In Linux: $ which python And in Windows use the following command in cmd or Anaconda prompt: $ where python Which should return something like: C:\\Python36\\envs\\SDG_11.2.1\\python.exe Showing your are using the Python from the virtual environment, not the base installation of Python.","title":"Activate the environment"},{"location":"for_developers/#installing-dependencies","text":"First, ensure you are in the project directory $ cd C:\\Users\\name\\project-directory To install the requirements write conda install --file requirements.txt This may throw an error if you do not have all the packages required PackageNotFoundError: The following packages are not available from current channels If this does, write the following with the package names that the error has shown you are missing: pip install packagename The script should now be set up to use.","title":"Installing dependencies"},{"location":"for_developers/#setting-git-configuration","text":"To be able to contribute to the project via Git, you will need to add the email and user name associated to your account to the config file git config --global user.email \"email\" git config --global user.name \"username\"","title":"Setting Git configuration"},{"location":"for_developers/#order-to-run-the-script","text":"In order to generate the output data, run the scripts in the order given below from the main directory of the repository. time_table directory SDG_bus_timetable.py SDG_train_timetable.py pre_processing directory eng_wales_pre_process.py After running the scripts above, we can finally run the main scripts given below (can be found in main directory of the repository) SDG_eng_wales.py SDG_scotland.py SDG_ni.py","title":"Order to run the script"},{"location":"geospatial_mods/","text":"buffer_points ( geo_df ) Creates a 500m or 1000m buffer around points. Draws 500m if the capacity_type is low Draws 1000m if the capacity_type is high Puts the results into a new column called \"geometry\" As 'epsg:27700' projections units of km, 500m is 0.5km. Args: geo_df (gpd.DataFrame): Data frame of points to be buffered including a column with the capacity_type for each point. Returns: gpd.DataFrame: A dataframe of polygons create from the buffer. Source code in src/geospatial_mods.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def buffer_points ( geo_df : gpd . GeoDataFrame ) -> gpd . GeoDataFrame : \"\"\"Creates a 500m or 1000m buffer around points. Draws 500m if the capacity_type is low Draws 1000m if the capacity_type is high Puts the results into a new column called \"geometry\" As 'epsg:27700' projections units of km, 500m is 0.5km. Args: geo_df (gpd.DataFrame): Data frame of points to be buffered including a column with the capacity_type for each point. Returns: gpd.DataFrame: A dataframe of polygons create from the buffer. \"\"\" # raise an error if high or low not correct capacity type for value in geo_df [ \"capacity_type\" ]: if value not in [ \"high\" , \"low\" ]: raise ValueError ( f \"\"\" { value } is not a valid capacity type, should be either high or low\"\"\" ) # conditions conditions = [ geo_df [ 'capacity_type' ] == \"low\" , geo_df [ 'capacity_type' ] == \"high\" ] # values values = [ geo_df . geometry . buffer ( LOWERBUFFER ), geo_df . geometry . buffer ( UPPERBUFFER )] # apply conditions geo_df [ 'geometry' ] = np . select ( condlist = conditions , choicelist = values ) return geo_df find_points_in_poly ( geo_df , polygon_obj ) Find points in polygon using geopandas' spatial join which joins the supplied geo_df (as left_df) and the polygon (as right_df). Then drops all rows where the point is not in the polygon (based on column index_right not being NaN). Finally it drop all column names from that were created in the join, leaving only the columns of the original geo_df. Parameters: geo_df ( DatFrame ) \u2013 a geo pandas dataframe. polygon_obj ( str ) \u2013 a geopandas dataframe with a polygon column. Returns: \u2013 gpd.GeoDataFrame: A geodata frame with the points inside the supplied \u2013 polygon. Source code in src/geospatial_mods.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def find_points_in_poly ( geo_df : gpd . GeoDataFrame , polygon_obj ): \"\"\"Find points in polygon using geopandas' spatial join which joins the supplied geo_df (as left_df) and the polygon (as right_df). Then drops all rows where the point is not in the polygon (based on column index_right not being NaN). Finally it drop all column names from that were created in the join, leaving only the columns of the original geo_df. Args: geo_df (gpg.DatFrame): a geo pandas dataframe. polygon_obj (str): a geopandas dataframe with a polygon column. Returns: gpd.GeoDataFrame: A geodata frame with the points inside the supplied polygon. \"\"\" wanted_cols = geo_df . columns . to_list () joined_df = ( gpd . sjoin ( geo_df , polygon_obj , how = 'left' , predicate = 'intersects' )) # op = 'within' filtered_df = ( joined_df [ joined_df [ 'index_right' ] . notna ()]) filtered_df = filtered_df [ wanted_cols ] return filtered_df geo_df_from_pd_df ( pd_df , geom_x , geom_y , crs ) Function to create a Geo-dataframe from a Pandas DataFrame. Parameters: pd_df ( DataFrame ) \u2013 a pandas dataframe object to be converted. geom_x ( str ) \u2013 name of the column that contains the longitude data. geom_y ( str ) \u2013 name of the column that contains the latitude data. crs ( str ) \u2013 the coordinate reference system required. Returns: \u2013 Geopandas Dataframe Source code in src/geospatial_mods.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 def geo_df_from_pd_df ( pd_df , geom_x , geom_y , crs ): \"\"\"Function to create a Geo-dataframe from a Pandas DataFrame. Arguments: pd_df (pd.DataFrame): a pandas dataframe object to be converted. geom_x (str):name of the column that contains the longitude data. geom_y (str):name of the column that contains the latitude data. crs (str): the coordinate reference system required. Returns: Geopandas Dataframe \"\"\" geometry = [ Point ( xy ) for xy in zip ( pd_df [ geom_x ], pd_df [ geom_y ])] geo_df = gpd . GeoDataFrame ( pd_df , geometry = geometry ) geo_df . crs = crs geo_df . to_crs ( 'EPSG:27700' , inplace = True ) return geo_df get_polygons_of_loccode ( geo_df , dissolveby = 'OA11CD' , search = None ) Gets the polygon for a place based on it name, LSOA code or OA code. Parameters: geo_df ( GeoDataFrame ) \u2013 Lookup geospatial data frame. loc_code ( str ) \u2013 Can be one of LSOA11CD, OA11CD or LSOA11NM. OA11CD by default. search ( str , default: None ) \u2013 Search terms to find in the LSOA11NM column. Only needed if intending to dissolve on a name in the LSOA11NM column. Defualt is None. Returns: GeoDataFrame \u2013 gpd.DataFrame: GeoDataFrame with multipolygons agregated on LSOA, OA code, or a search in the LSOA11NM column. Source code in src/geospatial_mods.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def get_polygons_of_loccode ( geo_df : gpd . GeoDataFrame , dissolveby = 'OA11CD' , search = None ) -> gpd . GeoDataFrame : \"\"\"Gets the polygon for a place based on it name, LSOA code or OA code. Args: geo_df (gpd.GeoDataFrame): Lookup geospatial data frame. loc_code (str): Can be one of LSOA11CD, OA11CD or LSOA11NM. OA11CD by default. search (str): Search terms to find in the LSOA11NM column. Only needed if intending to dissolve on a name in the LSOA11NM column. Defualt is None. Returns: gpd.DataFrame: GeoDataFrame with multipolygons agregated on LSOA, OA code, or a search in the LSOA11NM column. \"\"\" if dissolveby in [ 'LSOA11CD' , 'OA11CD' ]: polygon_df = geo_df . dissolve ( by = dissolveby ) else : filtered_df = geo_df [ geo_df [ f ' { dissolveby } ' ] . str . contains ( search )] filtered_df . insert ( 0 , 'place_name' , search ) polygon_df = filtered_df . dissolve ( by = 'place_name' ) polygon_df = gpd . GeoDataFrame ( polygon_df . pop ( 'geometry' )) return polygon_df","title":"Geospatial mods"},{"location":"geospatial_mods/#src.geospatial_mods.buffer_points","text":"Creates a 500m or 1000m buffer around points. Draws 500m if the capacity_type is low Draws 1000m if the capacity_type is high Puts the results into a new column called \"geometry\" As 'epsg:27700' projections units of km, 500m is 0.5km. Args: geo_df (gpd.DataFrame): Data frame of points to be buffered including a column with the capacity_type for each point. Returns: gpd.DataFrame: A dataframe of polygons create from the buffer. Source code in src/geospatial_mods.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def buffer_points ( geo_df : gpd . GeoDataFrame ) -> gpd . GeoDataFrame : \"\"\"Creates a 500m or 1000m buffer around points. Draws 500m if the capacity_type is low Draws 1000m if the capacity_type is high Puts the results into a new column called \"geometry\" As 'epsg:27700' projections units of km, 500m is 0.5km. Args: geo_df (gpd.DataFrame): Data frame of points to be buffered including a column with the capacity_type for each point. Returns: gpd.DataFrame: A dataframe of polygons create from the buffer. \"\"\" # raise an error if high or low not correct capacity type for value in geo_df [ \"capacity_type\" ]: if value not in [ \"high\" , \"low\" ]: raise ValueError ( f \"\"\" { value } is not a valid capacity type, should be either high or low\"\"\" ) # conditions conditions = [ geo_df [ 'capacity_type' ] == \"low\" , geo_df [ 'capacity_type' ] == \"high\" ] # values values = [ geo_df . geometry . buffer ( LOWERBUFFER ), geo_df . geometry . buffer ( UPPERBUFFER )] # apply conditions geo_df [ 'geometry' ] = np . select ( condlist = conditions , choicelist = values ) return geo_df","title":"buffer_points"},{"location":"geospatial_mods/#src.geospatial_mods.find_points_in_poly","text":"Find points in polygon using geopandas' spatial join which joins the supplied geo_df (as left_df) and the polygon (as right_df). Then drops all rows where the point is not in the polygon (based on column index_right not being NaN). Finally it drop all column names from that were created in the join, leaving only the columns of the original geo_df. Parameters: geo_df ( DatFrame ) \u2013 a geo pandas dataframe. polygon_obj ( str ) \u2013 a geopandas dataframe with a polygon column. Returns: \u2013 gpd.GeoDataFrame: A geodata frame with the points inside the supplied \u2013 polygon. Source code in src/geospatial_mods.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def find_points_in_poly ( geo_df : gpd . GeoDataFrame , polygon_obj ): \"\"\"Find points in polygon using geopandas' spatial join which joins the supplied geo_df (as left_df) and the polygon (as right_df). Then drops all rows where the point is not in the polygon (based on column index_right not being NaN). Finally it drop all column names from that were created in the join, leaving only the columns of the original geo_df. Args: geo_df (gpg.DatFrame): a geo pandas dataframe. polygon_obj (str): a geopandas dataframe with a polygon column. Returns: gpd.GeoDataFrame: A geodata frame with the points inside the supplied polygon. \"\"\" wanted_cols = geo_df . columns . to_list () joined_df = ( gpd . sjoin ( geo_df , polygon_obj , how = 'left' , predicate = 'intersects' )) # op = 'within' filtered_df = ( joined_df [ joined_df [ 'index_right' ] . notna ()]) filtered_df = filtered_df [ wanted_cols ] return filtered_df","title":"find_points_in_poly"},{"location":"geospatial_mods/#src.geospatial_mods.geo_df_from_pd_df","text":"Function to create a Geo-dataframe from a Pandas DataFrame. Parameters: pd_df ( DataFrame ) \u2013 a pandas dataframe object to be converted. geom_x ( str ) \u2013 name of the column that contains the longitude data. geom_y ( str ) \u2013 name of the column that contains the latitude data. crs ( str ) \u2013 the coordinate reference system required. Returns: \u2013 Geopandas Dataframe Source code in src/geospatial_mods.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 def geo_df_from_pd_df ( pd_df , geom_x , geom_y , crs ): \"\"\"Function to create a Geo-dataframe from a Pandas DataFrame. Arguments: pd_df (pd.DataFrame): a pandas dataframe object to be converted. geom_x (str):name of the column that contains the longitude data. geom_y (str):name of the column that contains the latitude data. crs (str): the coordinate reference system required. Returns: Geopandas Dataframe \"\"\" geometry = [ Point ( xy ) for xy in zip ( pd_df [ geom_x ], pd_df [ geom_y ])] geo_df = gpd . GeoDataFrame ( pd_df , geometry = geometry ) geo_df . crs = crs geo_df . to_crs ( 'EPSG:27700' , inplace = True ) return geo_df","title":"geo_df_from_pd_df"},{"location":"geospatial_mods/#src.geospatial_mods.get_polygons_of_loccode","text":"Gets the polygon for a place based on it name, LSOA code or OA code. Parameters: geo_df ( GeoDataFrame ) \u2013 Lookup geospatial data frame. loc_code ( str ) \u2013 Can be one of LSOA11CD, OA11CD or LSOA11NM. OA11CD by default. search ( str , default: None ) \u2013 Search terms to find in the LSOA11NM column. Only needed if intending to dissolve on a name in the LSOA11NM column. Defualt is None. Returns: GeoDataFrame \u2013 gpd.DataFrame: GeoDataFrame with multipolygons agregated on LSOA, OA code, or a search in the LSOA11NM column. Source code in src/geospatial_mods.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def get_polygons_of_loccode ( geo_df : gpd . GeoDataFrame , dissolveby = 'OA11CD' , search = None ) -> gpd . GeoDataFrame : \"\"\"Gets the polygon for a place based on it name, LSOA code or OA code. Args: geo_df (gpd.GeoDataFrame): Lookup geospatial data frame. loc_code (str): Can be one of LSOA11CD, OA11CD or LSOA11NM. OA11CD by default. search (str): Search terms to find in the LSOA11NM column. Only needed if intending to dissolve on a name in the LSOA11NM column. Defualt is None. Returns: gpd.DataFrame: GeoDataFrame with multipolygons agregated on LSOA, OA code, or a search in the LSOA11NM column. \"\"\" if dissolveby in [ 'LSOA11CD' , 'OA11CD' ]: polygon_df = geo_df . dissolve ( by = dissolveby ) else : filtered_df = geo_df [ geo_df [ f ' { dissolveby } ' ] . str . contains ( search )] filtered_df . insert ( 0 , 'place_name' , search ) polygon_df = filtered_df . dissolve ( by = 'place_name' ) polygon_df = gpd . GeoDataFrame ( polygon_df . pop ( 'geometry' )) return polygon_df","title":"get_polygons_of_loccode"},{"location":"main/","text":"The main pipeline sits here.","title":"Main"},{"location":"main_method/","text":"Calculation process Process diagram Main method of calculation The key step of calculation is a spatial join between geolocated urban population data and public transport service areas. A discussion of how each component of that join was calculated from their component parts follows. Geocoding the population data The questions we are trying to answer in this analysis is \u201cWhat proportion of people in the UK have access to public transport access points from their home?\u201d. And then the same question for people in each age, sex and disability category. To make the calculation as accurate as possible, we sourced the most granularly geolocated population data possible. Ideally, this would have been right down to the individual place of living (house/flat etc.) but such granularity is not available publicly, as it would be disclosive of individuals\u2019 living situations. The most granular level published that we are currently aware of is the \u201coutput area\u201d (OA) data, which derives from the census. Output areas (more fully defined and discussed in the data section of this methodology write up) are a conglomerate of the postcodes. Those postcodes are joined together on the basis that they are: Adjacent Of the same urban/rural classification In order to calculate the distance from the place any person lives, to a public transport access node (station or stop) we need a point-geometry for their residence. As we cannot geolocate individual residencies, we used an approximation in the form of population weighted centroids (PWCs). PWCs are a population-weighted location (x,y point) at which is a geospatial mean of the locations of all residences in that output area. The PWC, while extremely helpful in giving us a geospatial point to define where the population of any output area lives, is an approximation and is only used due to limitations in our data. Joining the data Population weighted centroid (PWC) data is geolocated with x,y coordinates and has a common data column with the output area data (output area code, \u201cOA11CD\u201d) so a table join can be performed on those datasets. Then the entire population of any output area is approximated to live at the centroid. Limitations and improvements Our team recognises that PWCs do not accurately represent the location of any individual\u2019s place of residence and we intend to research methods of making calculations at even more granular levels, such as postcodes or smaller. Currently however we agreed with the our data end user and the geospatial department that the method to be used for version 1.0 will use the described approximation to geolocate the population. Disaggregated data We disaggregate our data into age, sex, and disability. For all years aside from Census years (2011, 2021), we don't have disability data. Thus we calculated the proportions of people classified as disabled and not disabled in all output areas and applied this proportion to population estimates for non-Census years (2011, 2021). Similarly, Scotland and Northern Ireland do not have sex and Age data available for non-Census years. We used a similar method calculating proportions of people for a given age group or sex and applied these proportions to the mid-year population estimates for that output area. Delimitation of urban areas As Target 11 is concerned with sustainable urban environments, we ultimately had to select only urban areas and exclude rural areas from our analysis. Urban areas are defined as Ordnance Survey mapping that have resident populations above 10,000 people (2011 Census) and the methodology is available here . The urban/rural lookup data was sourced from ONS\u2019s Geography Portal and it provides a classification of each output area as either urban or rural categories which are further subdivided as follows. Classification Description Code Urban Major Conurbation A1 Urban Minor Conurbation B1 Urban City and Town C1 Urban City and Town in a Sparse Setting C2 Rural Town and Fringe D1 Rural Town and Fringe in a Sparse Setting D2 Rural Village E1 Rural Village in a Sparse Setting E2 Rural Hamlets and Isolated Dwellings F1 Rural Hamlets and Isolated Dwellings in a Sparse Setting F2 In our calculation we group A1, B1, C1 and C2 as urban and any other code as rural. Joining the data The population table for the whole nation was joined onto the table with the urban/rural classification for each output area on the output area code \u201cOA11CD\u201d, hence bringing in the classification into the population data. Every OA and associated PWC is therefore categorised as either urban or rural. Later we use this classification to filter the data, and disaggregate our analysis. For the SDG analysis we remove the OAs classified as rural from our analysis, but we can choose to include them to expose transport availability in rural areas too. Computation of service areas As described in the methodology of the UN Metadata for this indicator, public transport service areas had to be calculated. Two methods to calculate service areas are described in the methodology: 1) using a Euclidean buffer to create polygon, and 2) to create a network/path calculation. Our team opted for the Euclidean buffer method for a number of reasons: We followed advice from the ONS Geospatial department that the simpler buffering method would be adequate for our needs Computationally Euclidean is would be much less resource intensive and does not require distributed compute power, whereas a network calculation would require scaled compute power. Other countries (such as Estonia , Norway and others) used the Euclidean buffer method and published their results. Our results will be more comparable with theirs Research shows that the network enquiry requires a complete path network, (as pointed out by Sweden in their write up, see section \u201c Result from the network distance calculations\u201d ) Euclidean buffering methodology We use Geopandas and for all geospatial operations in our analysis and the buffering operation is actually carried out by the Shapely object.buffer() function. This operation takes a geospatial point, and uses a radius of a given length to creates a polygon around the point which approximates a circle. The standard buffering of a point yields a polygon with 99.8% of the area of the circular disk it approximates. Figure 1: Illustration of the process of buffering a geospatial Point The resulting geospatial polygons are then joined and can be used for further calculations. Figure 2: Process of combining polygons to create the service area Network query method A network query would be calculated by taking paths of a specified length (500m or 1km) in every direction from a specified point; for this project that point would be a transport stop or station. Following these paths for the specified distance would create many end points. Finally end points are joined to create a perimeter, within which lies the service area. Figure 3: An example visualisation of a network distance calculation, taken from the Swedish methodology write up at https://www.efgs.info/11-2-1-sweden/ . The image shows the stops in blue, surrounded by the Euclidean buffer, shaded in green and a 500m limit shown. Calculation of population within service areas With the service areas calculated, the population that resides within a service area is calculated by a two stage process: A points in polygons enquiry which then filters the PWCs, so that the data set contains only those which are within the service areas, The population figure (number of individuals) associated with each PWC is summed, meaning that only the population within the service areas is counted as the population outside of the service areas was filtered out at stage 1. The proportion of the population inside a service area is calculated as a proportion of the total population. Currently for version 1.0, this is carried out at Local Authority (LA) level. Timetable method More information on the timetable method can be found under the 'Timetable method' section. Northern Ireland Due to complexity of the work that will need to be undertaken, Northern Ireland timetable data will be included in Version 1.3 - enhanced functions and calculation. Currently, we will be using the stops sourced from NAPTAN. Disaggregations As required for the SDG indicator we are producing this data for, the output data from this project has been disaggregated by sex, age and disability status.","title":"Main method"},{"location":"main_method/#calculation-process","text":"","title":"Calculation process"},{"location":"main_method/#process-diagram","text":"","title":"Process diagram"},{"location":"main_method/#main-method-of-calculation","text":"The key step of calculation is a spatial join between geolocated urban population data and public transport service areas. A discussion of how each component of that join was calculated from their component parts follows.","title":"Main method of calculation"},{"location":"main_method/#geocoding-the-population-data","text":"The questions we are trying to answer in this analysis is \u201cWhat proportion of people in the UK have access to public transport access points from their home?\u201d. And then the same question for people in each age, sex and disability category. To make the calculation as accurate as possible, we sourced the most granularly geolocated population data possible. Ideally, this would have been right down to the individual place of living (house/flat etc.) but such granularity is not available publicly, as it would be disclosive of individuals\u2019 living situations. The most granular level published that we are currently aware of is the \u201coutput area\u201d (OA) data, which derives from the census. Output areas (more fully defined and discussed in the data section of this methodology write up) are a conglomerate of the postcodes. Those postcodes are joined together on the basis that they are: Adjacent Of the same urban/rural classification In order to calculate the distance from the place any person lives, to a public transport access node (station or stop) we need a point-geometry for their residence. As we cannot geolocate individual residencies, we used an approximation in the form of population weighted centroids (PWCs). PWCs are a population-weighted location (x,y point) at which is a geospatial mean of the locations of all residences in that output area. The PWC, while extremely helpful in giving us a geospatial point to define where the population of any output area lives, is an approximation and is only used due to limitations in our data.","title":"Geocoding the population data"},{"location":"main_method/#joining-the-data","text":"Population weighted centroid (PWC) data is geolocated with x,y coordinates and has a common data column with the output area data (output area code, \u201cOA11CD\u201d) so a table join can be performed on those datasets. Then the entire population of any output area is approximated to live at the centroid.","title":"Joining the data"},{"location":"main_method/#limitations-and-improvements","text":"Our team recognises that PWCs do not accurately represent the location of any individual\u2019s place of residence and we intend to research methods of making calculations at even more granular levels, such as postcodes or smaller. Currently however we agreed with the our data end user and the geospatial department that the method to be used for version 1.0 will use the described approximation to geolocate the population.","title":"Limitations and improvements"},{"location":"main_method/#disaggregated-data","text":"We disaggregate our data into age, sex, and disability. For all years aside from Census years (2011, 2021), we don't have disability data. Thus we calculated the proportions of people classified as disabled and not disabled in all output areas and applied this proportion to population estimates for non-Census years (2011, 2021). Similarly, Scotland and Northern Ireland do not have sex and Age data available for non-Census years. We used a similar method calculating proportions of people for a given age group or sex and applied these proportions to the mid-year population estimates for that output area.","title":"Disaggregated data"},{"location":"main_method/#delimitation-of-urban-areas","text":"As Target 11 is concerned with sustainable urban environments, we ultimately had to select only urban areas and exclude rural areas from our analysis. Urban areas are defined as Ordnance Survey mapping that have resident populations above 10,000 people (2011 Census) and the methodology is available here . The urban/rural lookup data was sourced from ONS\u2019s Geography Portal and it provides a classification of each output area as either urban or rural categories which are further subdivided as follows. Classification Description Code Urban Major Conurbation A1 Urban Minor Conurbation B1 Urban City and Town C1 Urban City and Town in a Sparse Setting C2 Rural Town and Fringe D1 Rural Town and Fringe in a Sparse Setting D2 Rural Village E1 Rural Village in a Sparse Setting E2 Rural Hamlets and Isolated Dwellings F1 Rural Hamlets and Isolated Dwellings in a Sparse Setting F2 In our calculation we group A1, B1, C1 and C2 as urban and any other code as rural.","title":"Delimitation of urban areas"},{"location":"main_method/#joining-the-data_1","text":"The population table for the whole nation was joined onto the table with the urban/rural classification for each output area on the output area code \u201cOA11CD\u201d, hence bringing in the classification into the population data. Every OA and associated PWC is therefore categorised as either urban or rural. Later we use this classification to filter the data, and disaggregate our analysis. For the SDG analysis we remove the OAs classified as rural from our analysis, but we can choose to include them to expose transport availability in rural areas too.","title":"Joining the data"},{"location":"main_method/#computation-of-service-areas","text":"As described in the methodology of the UN Metadata for this indicator, public transport service areas had to be calculated. Two methods to calculate service areas are described in the methodology: 1) using a Euclidean buffer to create polygon, and 2) to create a network/path calculation. Our team opted for the Euclidean buffer method for a number of reasons: We followed advice from the ONS Geospatial department that the simpler buffering method would be adequate for our needs Computationally Euclidean is would be much less resource intensive and does not require distributed compute power, whereas a network calculation would require scaled compute power. Other countries (such as Estonia , Norway and others) used the Euclidean buffer method and published their results. Our results will be more comparable with theirs Research shows that the network enquiry requires a complete path network, (as pointed out by Sweden in their write up, see section \u201c Result from the network distance calculations\u201d )","title":"Computation of service areas"},{"location":"main_method/#euclidean-buffering-methodology","text":"We use Geopandas and for all geospatial operations in our analysis and the buffering operation is actually carried out by the Shapely object.buffer() function. This operation takes a geospatial point, and uses a radius of a given length to creates a polygon around the point which approximates a circle. The standard buffering of a point yields a polygon with 99.8% of the area of the circular disk it approximates. Figure 1: Illustration of the process of buffering a geospatial Point The resulting geospatial polygons are then joined and can be used for further calculations. Figure 2: Process of combining polygons to create the service area","title":"Euclidean buffering methodology"},{"location":"main_method/#network-query-method","text":"A network query would be calculated by taking paths of a specified length (500m or 1km) in every direction from a specified point; for this project that point would be a transport stop or station. Following these paths for the specified distance would create many end points. Finally end points are joined to create a perimeter, within which lies the service area. Figure 3: An example visualisation of a network distance calculation, taken from the Swedish methodology write up at https://www.efgs.info/11-2-1-sweden/ . The image shows the stops in blue, surrounded by the Euclidean buffer, shaded in green and a 500m limit shown.","title":"Network query method"},{"location":"main_method/#calculation-of-population-within-service-areas","text":"With the service areas calculated, the population that resides within a service area is calculated by a two stage process: A points in polygons enquiry which then filters the PWCs, so that the data set contains only those which are within the service areas, The population figure (number of individuals) associated with each PWC is summed, meaning that only the population within the service areas is counted as the population outside of the service areas was filtered out at stage 1. The proportion of the population inside a service area is calculated as a proportion of the total population. Currently for version 1.0, this is carried out at Local Authority (LA) level.","title":"Calculation of population within service areas"},{"location":"main_method/#timetable-method","text":"More information on the timetable method can be found under the 'Timetable method' section.","title":"Timetable method"},{"location":"main_method/#northern-ireland","text":"Due to complexity of the work that will need to be undertaken, Northern Ireland timetable data will be included in Version 1.3 - enhanced functions and calculation. Currently, we will be using the stops sourced from NAPTAN.","title":"Northern Ireland"},{"location":"main_method/#disaggregations","text":"As required for the SDG indicator we are producing this data for, the output data from this project has been disaggregated by sex, age and disability status.","title":"Disaggregations"},{"location":"method_intro/","text":"Calculation Methodology of Indicator 11.2.1 Version 1.0 James Westwood, Antonio Felton, Paige Hunter and Nathan Shaw. Introduction Our team has calculated data for transport accessibility across the UK following, as closely as we could, the method in the UN Metadata for the SDG indicator 11.2.1 . The following is a write up of the methodology we employed to make these calculations. In this methodology writeup we aim to accurately reflect the method used in version 1.0 of our project; read more about versions below in the section \u201cProject Versions\u201d. Acknowledgement The main support for this project on both methodology and finding appropriate data sources has come from the geospatial department, in particular Musa Chirikeni, to whom we are very grateful. Other help has come from Michael Hodge who laid out the initial method we might approach in Python, and from the SDG data team who have been supportive data end users. We also thank Mark Simons who contributed to data management code in early 2022. Background The statistics calculated for this project are to be used as an indicator of the UK\u2019s progress on target 11.2 of the Sustainable Development Goals (SDGs). The SDGs are a set of 17 Goals which 193 Member states signed up to in order to coordinate efforts on more sustainable and inclusive national development. The goals cover areas such as poverty, hunger, equality and the environment. Goal 11, which the indicator we are calculating for falls under, is concerned with Sustainable Cities. The data our team produces will be available on the UK data platform for the SDG indicators , specifically on this page . Project history The project was initiated in late 2019 by James Westwood with some initial help from the UK SDG data team and Michael Hodge from the Data Science Campus, who provided guidance on the tech that would need to be used and a rough step-by-step calculation process. Project Versions The project is managed using version control on the Github platform. One feature of the version controlling using Github is that we can make releases. At the time of writing we have not made any releases of the code, but when we feel that all features to calculate the statistics required by the end user have been written into our code, we will release version 1.0. Each version has a project board, wherein we group the issues describing the work that needs to be done to develop each feature required. Project boards are hosted on Github. Version Focus Example features 1.0 Fully working reproducible calculation of transport availability across the UK Calculate the proportion of the population within 500m of a public transport access point for the whole of the UK. Disaggregate the above number by sex, age, disability status Be auditable and reproducable 1.1 Quality Assurance Phase Updates to code from feedback (e.g. from data team and topic expert) Data cleaning on import Validation on import Unit tests for functions 1.2 Enhanced functions and calculation Focus on enhancements to the calculation - Improve existing functions, making them more generic and robust - Look into improving geographical accuracy with more granular calculations - Improve the analysis with more disaggregations. 1.3 Optimise Computation Focus on enhancements to the functioning of the code: Better data management (e.g. SQL tables) Refactoring code Code optimization (speed, memory) vectorised calculation, Shortcomings of the disaggregation method Our team recognises that in our current method, ethnicity is not a disaggregation that we use. At this stage for version 1.0 we are attempting to output data called for by the methodology in the UN Metadata . We regret that this important disaggregation is not included however, so our team intend to include this additional disaggregation in version 1.1 as an enhancement above what the original methodology requires. Disaggregating on other protected characteristics, as well as deprivation levels may be considered too. Disability status Classification and calculation of people with disabilities We classify disability using data from the ONS UK census, which is consistent with GSS harmonized disability data. To understand the data, we looked at the questions and their possible responses in the Measuring disability for the Equality Act 2010 harmonisation guidance . The questions are as follows: Question Response options Do you have any physical or mental health conditions or illnesses lasting or expected to last 12 months or more? Yes / No Does your condition or illness\\do any of your conditions or illnesses reduce your ability to carry-out day-to-day activities? Yes, a lot / Yes, a little / No The guidance states that the persons meeting the following criteria include: \"_A person who says yes, they have physical or mental health condition(s) or illness(es) lasting or expected to last for 12 months or more, but it doesn\u2019t restrict their activity are non-disabled._\" Therefore in our calculation, people will be considered \"Non-disabled\" if they: answer no to the first question answer yes to the first question, but no to the second question. And the calculations are as follows: Dtot = Dlot + Dlit I.e. \"Total people with disabilities\" = \"Day-to-day activities limited a lot\" + \"Day-to-day activities limited a little\" Or, in our Python code disab_total = disab_ltd_little + disab_ltd_lot Where disab_ltd_little and disab_ltd_lot are each column or Pandas Series. Then the total of non disabled people given by \"Non-disabled\" = \"Total Population\" - \"Total people with disabilities\" Or in our Python code non-disabled = pop_count - disab_total Considerations on disability status Our team has discussed options for counting individuals as either disabled or not. This is a complex and important area, and we recognise the importance of getting this as accurate as possible, as it may highlight areas in which those with disabilities are more affected by transport accessibility issues. Distance Standard distances of 500m and 1km are applied as a radius around the transport access nodes in order to create the public transport service areas. We speculate that these distances likely do not represent an accessible distance for many people however- this might include wheelchair users, elderly people and families with young children. Definitions We have opted to use a GSS Harmonised definition of disability for our analysis and the data comes from the census as described above. On the other hand the UN Metadata defines additional criteria to categorise public transport as conveniently accessible or not: \u201cPublic transport accessible to all special-needs customers, including those who are physically, visually, and/or hearing-impaired, as well as those with temporary disabilities, the elderly, children and other people in vulnerable situations\u201d_ In our analysis we are including the entire population, however, in our disaggregations we do not create a \u201cspecial-needs\u201d group. If we were to create such a group we should include people with temporary disabilities (if the data on this can be sourced), and the elderly or children. This has been proposed for version 1.2 of this project. Selection of age bins Population data was broken down by age on a year-by-year basis, from ages 0 through to 99. Rather than reporting the data or even calculating transport availability for every year in an age range, we opted to run the calculation for ages binned in 5-year brackets. We found no standard way to group ages. In other indicators across the SDG platform we found data grouped by age in various age increments and ranges, as seen in the disaggregation report for age . We selected the 5-year brackets (0-4, 5-9, 10-14 etc) to be similar to other UK indicators such as 3.3.3 and 3.4.1 among others. However we realise there are many other indicators which are not grouped in this way. If another age binning method is required, we plan to make our age_binning function more configurable (in version 1.2 of the project) so it will take population data by age and aggregate it into bins group by ages provided a bin_size parameter. This means that if the age binning needs to be changed so that, for instance, it can be compared to another dataset, the list of age groups can be changed easily and the analysis rerun. Aggregation and reporting To establish whether public stops and stations were within reach of people's places of residence the data analysis needed to be carried out at the most granular level possible, the output area level. However there are 175,434 output areas in England and Wales, so this would be too many data points to report on the data platform. Instead we aggregate up to large areas by request of our data end-user, the SDG data team. We aggregate our analysis up to the local authority (LA) level, and output areas fit perfectly within their parent local authority. The results aggregated in this way will display well on the UK SDG data platform , as it is well for developed to take this kind of geographical data!","title":"Calculation Methodology of Indicator 11.2.1"},{"location":"method_intro/#calculation-methodology-of-indicator-1121","text":"Version 1.0 James Westwood, Antonio Felton, Paige Hunter and Nathan Shaw.","title":"Calculation Methodology of Indicator 11.2.1"},{"location":"method_intro/#introduction","text":"Our team has calculated data for transport accessibility across the UK following, as closely as we could, the method in the UN Metadata for the SDG indicator 11.2.1 . The following is a write up of the methodology we employed to make these calculations. In this methodology writeup we aim to accurately reflect the method used in version 1.0 of our project; read more about versions below in the section \u201cProject Versions\u201d.","title":"Introduction"},{"location":"method_intro/#acknowledgement","text":"The main support for this project on both methodology and finding appropriate data sources has come from the geospatial department, in particular Musa Chirikeni, to whom we are very grateful. Other help has come from Michael Hodge who laid out the initial method we might approach in Python, and from the SDG data team who have been supportive data end users. We also thank Mark Simons who contributed to data management code in early 2022.","title":"Acknowledgement"},{"location":"method_intro/#background","text":"The statistics calculated for this project are to be used as an indicator of the UK\u2019s progress on target 11.2 of the Sustainable Development Goals (SDGs). The SDGs are a set of 17 Goals which 193 Member states signed up to in order to coordinate efforts on more sustainable and inclusive national development. The goals cover areas such as poverty, hunger, equality and the environment. Goal 11, which the indicator we are calculating for falls under, is concerned with Sustainable Cities. The data our team produces will be available on the UK data platform for the SDG indicators , specifically on this page .","title":"Background"},{"location":"method_intro/#project-history","text":"The project was initiated in late 2019 by James Westwood with some initial help from the UK SDG data team and Michael Hodge from the Data Science Campus, who provided guidance on the tech that would need to be used and a rough step-by-step calculation process.","title":"Project history"},{"location":"method_intro/#project-versions","text":"The project is managed using version control on the Github platform. One feature of the version controlling using Github is that we can make releases. At the time of writing we have not made any releases of the code, but when we feel that all features to calculate the statistics required by the end user have been written into our code, we will release version 1.0. Each version has a project board, wherein we group the issues describing the work that needs to be done to develop each feature required. Project boards are hosted on Github. Version Focus Example features 1.0 Fully working reproducible calculation of transport availability across the UK Calculate the proportion of the population within 500m of a public transport access point for the whole of the UK. Disaggregate the above number by sex, age, disability status Be auditable and reproducable 1.1 Quality Assurance Phase Updates to code from feedback (e.g. from data team and topic expert) Data cleaning on import Validation on import Unit tests for functions 1.2 Enhanced functions and calculation Focus on enhancements to the calculation - Improve existing functions, making them more generic and robust - Look into improving geographical accuracy with more granular calculations - Improve the analysis with more disaggregations. 1.3 Optimise Computation Focus on enhancements to the functioning of the code: Better data management (e.g. SQL tables) Refactoring code Code optimization (speed, memory) vectorised calculation,","title":"Project Versions"},{"location":"method_intro/#shortcomings-of-the-disaggregation-method","text":"Our team recognises that in our current method, ethnicity is not a disaggregation that we use. At this stage for version 1.0 we are attempting to output data called for by the methodology in the UN Metadata . We regret that this important disaggregation is not included however, so our team intend to include this additional disaggregation in version 1.1 as an enhancement above what the original methodology requires. Disaggregating on other protected characteristics, as well as deprivation levels may be considered too.","title":"Shortcomings of the disaggregation method"},{"location":"method_intro/#disability-status","text":"","title":"Disability status"},{"location":"method_intro/#classification-and-calculation-of-people-with-disabilities","text":"We classify disability using data from the ONS UK census, which is consistent with GSS harmonized disability data. To understand the data, we looked at the questions and their possible responses in the Measuring disability for the Equality Act 2010 harmonisation guidance . The questions are as follows: Question Response options Do you have any physical or mental health conditions or illnesses lasting or expected to last 12 months or more? Yes / No Does your condition or illness\\do any of your conditions or illnesses reduce your ability to carry-out day-to-day activities? Yes, a lot / Yes, a little / No The guidance states that the persons meeting the following criteria include: \"_A person who says yes, they have physical or mental health condition(s) or illness(es) lasting or expected to last for 12 months or more, but it doesn\u2019t restrict their activity are non-disabled._\" Therefore in our calculation, people will be considered \"Non-disabled\" if they: answer no to the first question answer yes to the first question, but no to the second question. And the calculations are as follows: Dtot = Dlot + Dlit I.e. \"Total people with disabilities\" = \"Day-to-day activities limited a lot\" + \"Day-to-day activities limited a little\" Or, in our Python code disab_total = disab_ltd_little + disab_ltd_lot Where disab_ltd_little and disab_ltd_lot are each column or Pandas Series. Then the total of non disabled people given by \"Non-disabled\" = \"Total Population\" - \"Total people with disabilities\" Or in our Python code non-disabled = pop_count - disab_total","title":"Classification and calculation of people with disabilities"},{"location":"method_intro/#considerations-on-disability-status","text":"Our team has discussed options for counting individuals as either disabled or not. This is a complex and important area, and we recognise the importance of getting this as accurate as possible, as it may highlight areas in which those with disabilities are more affected by transport accessibility issues.","title":"Considerations on disability status"},{"location":"method_intro/#distance","text":"Standard distances of 500m and 1km are applied as a radius around the transport access nodes in order to create the public transport service areas. We speculate that these distances likely do not represent an accessible distance for many people however- this might include wheelchair users, elderly people and families with young children.","title":"Distance"},{"location":"method_intro/#definitions","text":"We have opted to use a GSS Harmonised definition of disability for our analysis and the data comes from the census as described above. On the other hand the UN Metadata defines additional criteria to categorise public transport as conveniently accessible or not: \u201cPublic transport accessible to all special-needs customers, including those who are physically, visually, and/or hearing-impaired, as well as those with temporary disabilities, the elderly, children and other people in vulnerable situations\u201d_ In our analysis we are including the entire population, however, in our disaggregations we do not create a \u201cspecial-needs\u201d group. If we were to create such a group we should include people with temporary disabilities (if the data on this can be sourced), and the elderly or children. This has been proposed for version 1.2 of this project.","title":"Definitions"},{"location":"method_intro/#selection-of-age-bins","text":"Population data was broken down by age on a year-by-year basis, from ages 0 through to 99. Rather than reporting the data or even calculating transport availability for every year in an age range, we opted to run the calculation for ages binned in 5-year brackets. We found no standard way to group ages. In other indicators across the SDG platform we found data grouped by age in various age increments and ranges, as seen in the disaggregation report for age . We selected the 5-year brackets (0-4, 5-9, 10-14 etc) to be similar to other UK indicators such as 3.3.3 and 3.4.1 among others. However we realise there are many other indicators which are not grouped in this way. If another age binning method is required, we plan to make our age_binning function more configurable (in version 1.2 of the project) so it will take population data by age and aggregate it into bins group by ages provided a bin_size parameter. This means that if the age binning needs to be changed so that, for instance, it can be compared to another dataset, the list of age groups can be changed easily and the analysis rerun.","title":"Selection of age bins"},{"location":"method_intro/#aggregation-and-reporting","text":"To establish whether public stops and stations were within reach of people's places of residence the data analysis needed to be carried out at the most granular level possible, the output area level. However there are 175,434 output areas in England and Wales, so this would be too many data points to report on the data platform. Instead we aggregate up to large areas by request of our data end-user, the SDG data team. We aggregate our analysis up to the local authority (LA) level, and output areas fit perfectly within their parent local authority. The results aggregated in this way will display well on the UK SDG data platform , as it is well for developed to take this kind of geographical data!","title":"Aggregation and reporting"},{"location":"method_timetable/","text":"Timetable-based filtering of transport nodes The frequency of service at transport nodes, such as stops and stations, plays a critical role in promoting sustainable and efficient transportation systems. By filtering these nodes based on their timetable frequency, we can identify areas with infrequent service, where accessibility and reliability may be compromised. Our method filters out any nodes with a frequency of less than 1 service per hour, between the hours of 6am to 8pm on weekdays. This is equivalent to a service every 60 minutes, which is a common threshold for low-frequency service in many cities around the world. We aim to produce accurate statistics which contribute to the achievement of SDG 11.2.1 by creating a sustainable and inclusive transport system that provides frequent and reliable services to all individuals, thereby improving accessibility and promoting sustainable urban and rural mobility. Parsing Train Timetable Data Train timetable data plays a crucial role in the filtering methodology for transport nodes based on timetable frequency. In our project, we utilize train timetable data obtained from the Association of Train Operating Companies (ATOC). This data is provided in the CIF (Common Interface Format) format and is updated on a weekly basis, reflecting the most recent train schedules. To facilitate data analysis and processing, a data dictionary containing file and field descriptions is made available by ATOC which the code in our parser was based on. To access the train timetable data, users are required to download the latest CIF files from ATOC. However, due to the fact that file names are changing with each update, it is not feasible to automatically retrieve the data each time the pipeline is run. To address this, our project incorporates the SDG_train_timetable module, which pre-processes the user-downloaded CIF files and saves the output in a format that is ready to be utilised within the main data pipeline. The train timetable data that is released by ATOC has multiple datasets, but for the purpose of our analysis we are only interested in two specific files: the .msn (Master Station Names) and .mca files. These files provide essential information, such as station names and unique identifiers, which we later use in the filtering process and to locate the stations accurately. The location data within the CIF files is represented in the form of grid references, without the corresponding 100km grid square information which makes it impossibly to plot the station locations accurately. To address this issue, we ultimately used the TIPLOC codes, which meant that we could join our train data back onto Naptan data - see the section \"Locating Stations with Tiploc Codes\" for more details. The two files (.msn and .mca) have specific structures that contain valuable information for our analysis. To process the data more efficiently, we parsed the data from each file into their respective dataframes. Within the .msn file, we focused on extracting the departure times (scheduled) for each stop. If the departure time was empty, we disregarded any attempts to extract public or scheduled pass times, as they were deemed irrelevant for our filtering methodology. Additionally, any entries marked as BSD (Backward Set Data) were ignored, as these entries indicate that the data has been deleted or replaced. In the case of time values with an 'H' appended were rounded up or down half a minute. To streamline the analysis, any records with empty times were removed from the dataset. This ensured that only relevant and complete records were included in subsequent filtering and analysis processes. Furthermore, we filtered the records to include only activities with the code 'T' (i.e., timing), as other activities were considered extraneous to the timetable frequency analysis. By organizing and filtering the data based on these criteria, we were able to create a refined dataset of rail stations based on the frequency of the service at each station which aligns with SDG 11.2.1 methodology. Locating Stations with Tiploc Codes During our data analysis, a mismatch was discovered between the number of stations listed in the external location data file and the number of station codes (CRS codes) obtained from the National Rail database. This mismatch was due to the fact that the external location data file contained stations that were no longer in operation. Our method used TIPLOC codes to locate stations geographically; Tiploc means \u201ctiming point locations\u201d and are a unique identifier for train stations (and other points). We used them because we found that they were embedded in Atoccode, which was in the Naptan data that we used for all stop and station locations. By extracting [4:] in the Atcocode column, you get the Tiploc code - which was useful for joining back onto train data, so we got (lat, lon) locations. Parsing the Bus Timetable Data Bus Timetable Data To complement the train timetable data, we also incorporate bus timetable data into our filtering methodology. The bus timetable data is sourced from the Bus Open Data Service managed by the Department for Transport (DFT). This service provides access to up-to-date information on bus schedules across the country. The dataset is updated daily at approximately 06:00 GMT, and we specifically download the GTFS (General Transit Feed Specification) schedule dataset, which is provided in the form of zipped text files. It is important to note that the process of downloading the bus timetable data is separate from the main pipeline of our methodology. This means that it is a one-time task and not automatically performed as part of the regular data processing. To download new data, the user needs to: a) update the filename in the configuration file b) set the download flag to true. With these changes if the current data is more than seven days old, the latest dataset is automatically downloaded to ensure that the analysis incorporates the most recent bus schedule information. For comprehensive documentation of the available files and field names, users can refer to the data dictionaries provided by the Bus Open Data Service. These resources offer detailed insights into the structure and meanings of the various files within the GTFS schedule dataset. The bus timetable data consists of multiple files, each serving a specific purpose within the dataset. An overview of the available files is as follows: Agency.txt : This file provides information about the transit agency, including its name, URL, and contact details. Stops.txt : Contains a list of all the bus stops, along with their unique identifiers, names, and geographical coordinates. Routes.txt : Describes the routes taken by buses, including their unique identifiers, names, and associated agencies. Trips.txt : Provides details about individual trips made by buses, including their unique identifiers, routes, and associated service IDs. Stop Times.txt : Contains the timetable information for each stop, including the arrival and departure times for buses. Calendar.txt : Describes the service availability for each day of the week, specifying the start and end dates for each service. Calendar Dates.txt : Provides additional service availability information, including exceptions or changes to the regular schedule. Fare Attributes.txt : Contains fare-related information, such as fare identifiers and prices. Fare Rules.txt : Specifies the fare rules and their applicability to different routes or trips. Bus Timetable Data To complement the train timetable data, we also incorporate bus timetable data into our filtering methodology. The bus timetable data is sourced from the Bus Open Data Service managed by the Department for Transport (DFT). This service provides access to up-to-date information on bus schedules across the country. The dataset is updated daily at approximately 06:00 GMT, and we specifically download the GTFS (General Transit Feed Specification) schedule dataset, which is provided in the form of zipped text files. It is important to note that the process of downloading the bus timetable data is separate from the main pipeline of our methodology. This means that it is a one-time task and not automatically performed as part of the regular data processing. To download new data, the user needs to update the filename in the configuration file and set the download flag to true. Additionally, if the current data is more than seven days old, the latest dataset is automatically downloaded to ensure that the analysis incorporates the most recent bus schedule information. For comprehensive documentation of the available files and field names, users can refer to the data dictionaries provided by the Bus Open Data Service. These resources offer detailed insights into the structure and meanings of the various files within the GTFS schedule dataset. The bus timetable data consists of multiple files, each serving a specific purpose within the dataset. An overview of the available files is as follows: Agency.txt : This file provides information about the transit agency, including its name, URL, and contact details. Stops.txt : Contains a list of all the bus stops, along with their unique identifiers, names, and geographical coordinates. Routes.txt : Describes the routes taken by buses, including their unique identifiers, names, and associated agencies. Trips.txt : Provides details about individual trips made by buses, including their unique identifiers, routes, and associated service IDs. Stop Times.txt : Contains the timetable information for each stop, including the arrival and departure times for buses. Calendar.txt : Describes the service availability for each day of the week, specifying the start and end dates for each service. Calendar Dates.txt : Provides additional service availability information, including exceptions or changes to the regular schedule. Fare Attributes.txt : Contains fare-related information, such as fare identifiers and prices. Fare Rules.txt : Specifies the fare rules and their applicability to different routes or trips. Data dictionaries of the available files and field names can be found here . Our analysis and filtering methodology, we focus specifically on three key datasets: calendar.txt, stop_times.txt, and trips.txt. These datasets contain all the necessary information we require to analyze timetable frequency and make informed decisions. The stop_times.txt dataset is crucial for our analysis as it contains the specific timetable information for each bus stop. It includes details such as the arrival and departure times of buses at each stop. This dataset enables us to extract the timetable frequency and assess the regularity of bus services at each stop. The trips.txt dataset provides essential information about individual bus trips, including the unique identifiers of the trips, associated routes, and service IDs. This dataset allows us to link the timetable information from stop_times.txt with the corresponding trips and routes. By connecting these datasets, we can accurately map the bus trips to their respective schedules and understand the relationship between stops, trips, and routes. To extract highly serviced stops (a bus stop that has 1 or more buses use it between 06:00 and 22:00 during the week) we do the following: The three datasets, calendar.txt, stop_times.txt, and trips.txt, are processed and organized into separate dataframes to facilitate further analysis in our methodology. One of the initial filtering steps involves selecting departure times within specific hours that we have defined as indicative of highly serviced periods (as specified in the configuration file). This filtering process helps remove any spurious data, such as times recorded after 24:00. To enhance data consistency and compatibility, the departure times are converted into dates. This conversion allows for easier comparison and manipulation of the data based on specific dates rather than individual time points. The next step involves joining the stop_times, trips, and calendar datasets together, leveraging their interconnectedness. Unnecessary columns that do not contribute to our analysis are dropped from the combined dataset, ensuring a more streamlined and focused dataset for subsequent processing. To analyse the bus services on a specific day, we apply a filtering mechanism based on a designated day of the week. By default, this filtering is set to Wednesday but can be adjusted by the user according to their requirements. Additionally, a function is implemented to filter services based on a specific date, providing flexibility for future use cases. This function takes as input a desired day of the week and identifies the corresponding date within the middle week between the earliest start date and the latest end date. For instance, suppose the earliest start date is January 1, 2022, the latest end date is January 31, 2022, and the desired day of the week is Wednesday. In that case, the function would select Wednesday, January 12th as the representative date for the specified day of the week.","title":"Timetable-based filtering of transport nodes"},{"location":"method_timetable/#timetable-based-filtering-of-transport-nodes","text":"The frequency of service at transport nodes, such as stops and stations, plays a critical role in promoting sustainable and efficient transportation systems. By filtering these nodes based on their timetable frequency, we can identify areas with infrequent service, where accessibility and reliability may be compromised. Our method filters out any nodes with a frequency of less than 1 service per hour, between the hours of 6am to 8pm on weekdays. This is equivalent to a service every 60 minutes, which is a common threshold for low-frequency service in many cities around the world. We aim to produce accurate statistics which contribute to the achievement of SDG 11.2.1 by creating a sustainable and inclusive transport system that provides frequent and reliable services to all individuals, thereby improving accessibility and promoting sustainable urban and rural mobility.","title":"Timetable-based filtering of transport nodes"},{"location":"method_timetable/#parsing-train-timetable-data","text":"Train timetable data plays a crucial role in the filtering methodology for transport nodes based on timetable frequency. In our project, we utilize train timetable data obtained from the Association of Train Operating Companies (ATOC). This data is provided in the CIF (Common Interface Format) format and is updated on a weekly basis, reflecting the most recent train schedules. To facilitate data analysis and processing, a data dictionary containing file and field descriptions is made available by ATOC which the code in our parser was based on. To access the train timetable data, users are required to download the latest CIF files from ATOC. However, due to the fact that file names are changing with each update, it is not feasible to automatically retrieve the data each time the pipeline is run. To address this, our project incorporates the SDG_train_timetable module, which pre-processes the user-downloaded CIF files and saves the output in a format that is ready to be utilised within the main data pipeline. The train timetable data that is released by ATOC has multiple datasets, but for the purpose of our analysis we are only interested in two specific files: the .msn (Master Station Names) and .mca files. These files provide essential information, such as station names and unique identifiers, which we later use in the filtering process and to locate the stations accurately. The location data within the CIF files is represented in the form of grid references, without the corresponding 100km grid square information which makes it impossibly to plot the station locations accurately. To address this issue, we ultimately used the TIPLOC codes, which meant that we could join our train data back onto Naptan data - see the section \"Locating Stations with Tiploc Codes\" for more details. The two files (.msn and .mca) have specific structures that contain valuable information for our analysis. To process the data more efficiently, we parsed the data from each file into their respective dataframes. Within the .msn file, we focused on extracting the departure times (scheduled) for each stop. If the departure time was empty, we disregarded any attempts to extract public or scheduled pass times, as they were deemed irrelevant for our filtering methodology. Additionally, any entries marked as BSD (Backward Set Data) were ignored, as these entries indicate that the data has been deleted or replaced. In the case of time values with an 'H' appended were rounded up or down half a minute. To streamline the analysis, any records with empty times were removed from the dataset. This ensured that only relevant and complete records were included in subsequent filtering and analysis processes. Furthermore, we filtered the records to include only activities with the code 'T' (i.e., timing), as other activities were considered extraneous to the timetable frequency analysis. By organizing and filtering the data based on these criteria, we were able to create a refined dataset of rail stations based on the frequency of the service at each station which aligns with SDG 11.2.1 methodology.","title":"Parsing Train Timetable Data"},{"location":"method_timetable/#locating-stations-with-tiploc-codes","text":"During our data analysis, a mismatch was discovered between the number of stations listed in the external location data file and the number of station codes (CRS codes) obtained from the National Rail database. This mismatch was due to the fact that the external location data file contained stations that were no longer in operation. Our method used TIPLOC codes to locate stations geographically; Tiploc means \u201ctiming point locations\u201d and are a unique identifier for train stations (and other points). We used them because we found that they were embedded in Atoccode, which was in the Naptan data that we used for all stop and station locations. By extracting [4:] in the Atcocode column, you get the Tiploc code - which was useful for joining back onto train data, so we got (lat, lon) locations.","title":"Locating Stations with Tiploc Codes"},{"location":"method_timetable/#parsing-the-bus-timetable-data","text":"","title":"Parsing the Bus Timetable Data"},{"location":"method_timetable/#bus-timetable-data","text":"To complement the train timetable data, we also incorporate bus timetable data into our filtering methodology. The bus timetable data is sourced from the Bus Open Data Service managed by the Department for Transport (DFT). This service provides access to up-to-date information on bus schedules across the country. The dataset is updated daily at approximately 06:00 GMT, and we specifically download the GTFS (General Transit Feed Specification) schedule dataset, which is provided in the form of zipped text files. It is important to note that the process of downloading the bus timetable data is separate from the main pipeline of our methodology. This means that it is a one-time task and not automatically performed as part of the regular data processing. To download new data, the user needs to: a) update the filename in the configuration file b) set the download flag to true. With these changes if the current data is more than seven days old, the latest dataset is automatically downloaded to ensure that the analysis incorporates the most recent bus schedule information. For comprehensive documentation of the available files and field names, users can refer to the data dictionaries provided by the Bus Open Data Service. These resources offer detailed insights into the structure and meanings of the various files within the GTFS schedule dataset. The bus timetable data consists of multiple files, each serving a specific purpose within the dataset. An overview of the available files is as follows: Agency.txt : This file provides information about the transit agency, including its name, URL, and contact details. Stops.txt : Contains a list of all the bus stops, along with their unique identifiers, names, and geographical coordinates. Routes.txt : Describes the routes taken by buses, including their unique identifiers, names, and associated agencies. Trips.txt : Provides details about individual trips made by buses, including their unique identifiers, routes, and associated service IDs. Stop Times.txt : Contains the timetable information for each stop, including the arrival and departure times for buses. Calendar.txt : Describes the service availability for each day of the week, specifying the start and end dates for each service. Calendar Dates.txt : Provides additional service availability information, including exceptions or changes to the regular schedule. Fare Attributes.txt : Contains fare-related information, such as fare identifiers and prices. Fare Rules.txt : Specifies the fare rules and their applicability to different routes or trips.","title":"Bus Timetable Data"},{"location":"method_timetable/#bus-timetable-data_1","text":"To complement the train timetable data, we also incorporate bus timetable data into our filtering methodology. The bus timetable data is sourced from the Bus Open Data Service managed by the Department for Transport (DFT). This service provides access to up-to-date information on bus schedules across the country. The dataset is updated daily at approximately 06:00 GMT, and we specifically download the GTFS (General Transit Feed Specification) schedule dataset, which is provided in the form of zipped text files. It is important to note that the process of downloading the bus timetable data is separate from the main pipeline of our methodology. This means that it is a one-time task and not automatically performed as part of the regular data processing. To download new data, the user needs to update the filename in the configuration file and set the download flag to true. Additionally, if the current data is more than seven days old, the latest dataset is automatically downloaded to ensure that the analysis incorporates the most recent bus schedule information. For comprehensive documentation of the available files and field names, users can refer to the data dictionaries provided by the Bus Open Data Service. These resources offer detailed insights into the structure and meanings of the various files within the GTFS schedule dataset. The bus timetable data consists of multiple files, each serving a specific purpose within the dataset. An overview of the available files is as follows: Agency.txt : This file provides information about the transit agency, including its name, URL, and contact details. Stops.txt : Contains a list of all the bus stops, along with their unique identifiers, names, and geographical coordinates. Routes.txt : Describes the routes taken by buses, including their unique identifiers, names, and associated agencies. Trips.txt : Provides details about individual trips made by buses, including their unique identifiers, routes, and associated service IDs. Stop Times.txt : Contains the timetable information for each stop, including the arrival and departure times for buses. Calendar.txt : Describes the service availability for each day of the week, specifying the start and end dates for each service. Calendar Dates.txt : Provides additional service availability information, including exceptions or changes to the regular schedule. Fare Attributes.txt : Contains fare-related information, such as fare identifiers and prices. Fare Rules.txt : Specifies the fare rules and their applicability to different routes or trips. Data dictionaries of the available files and field names can be found here . Our analysis and filtering methodology, we focus specifically on three key datasets: calendar.txt, stop_times.txt, and trips.txt. These datasets contain all the necessary information we require to analyze timetable frequency and make informed decisions. The stop_times.txt dataset is crucial for our analysis as it contains the specific timetable information for each bus stop. It includes details such as the arrival and departure times of buses at each stop. This dataset enables us to extract the timetable frequency and assess the regularity of bus services at each stop. The trips.txt dataset provides essential information about individual bus trips, including the unique identifiers of the trips, associated routes, and service IDs. This dataset allows us to link the timetable information from stop_times.txt with the corresponding trips and routes. By connecting these datasets, we can accurately map the bus trips to their respective schedules and understand the relationship between stops, trips, and routes. To extract highly serviced stops (a bus stop that has 1 or more buses use it between 06:00 and 22:00 during the week) we do the following: The three datasets, calendar.txt, stop_times.txt, and trips.txt, are processed and organized into separate dataframes to facilitate further analysis in our methodology. One of the initial filtering steps involves selecting departure times within specific hours that we have defined as indicative of highly serviced periods (as specified in the configuration file). This filtering process helps remove any spurious data, such as times recorded after 24:00. To enhance data consistency and compatibility, the departure times are converted into dates. This conversion allows for easier comparison and manipulation of the data based on specific dates rather than individual time points. The next step involves joining the stop_times, trips, and calendar datasets together, leveraging their interconnectedness. Unnecessary columns that do not contribute to our analysis are dropped from the combined dataset, ensuring a more streamlined and focused dataset for subsequent processing. To analyse the bus services on a specific day, we apply a filtering mechanism based on a designated day of the week. By default, this filtering is set to Wednesday but can be adjusted by the user according to their requirements. Additionally, a function is implemented to filter services based on a specific date, providing flexibility for future use cases. This function takes as input a desired day of the week and identifies the corresponding date within the middle week between the earliest start date and the latest end date. For instance, suppose the earliest start date is January 1, 2022, the latest end date is January 31, 2022, and the desired day of the week is Wednesday. In that case, the function would select Wednesday, January 12th as the representative date for the specified day of the week.","title":"Bus Timetable Data"},{"location":"time_table_utils/","text":"Technical documentation for the time_table_utils module. Any docstrings in this file are automatically copied to this page. All functions realted to the bus and train timetable data. add_stop_capacity_type ( stops_df ) Adds capacity_type column. Column is defined with the following dictionary using the StopType Bus stops are low capacity, train stations are high capacity. Parameters: stops_df ( DataFrame ) \u2013 The dataframe to add the column to. Returns: DataFrame \u2013 pd.DataFrame: dataframe with new capacity_type column. Source code in src/time_table/time_table_utils.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def add_stop_capacity_type ( stops_df : pd . DataFrame ) -> pd . DataFrame : \"\"\"Adds capacity_type column. Column is defined with the following dictionary using the StopType Bus stops are low capacity, train stations are high capacity. Args: stops_df (pd.DataFrame): The dataframe to add the column to. Returns: pd.DataFrame: dataframe with new capacity_type column. \"\"\" # Create a dictionary to map the StopType to capacity level capacity_map = { \"RSE\" : \"high\" , \"RLY\" : \"high\" , \"RPL\" : \"high\" , \"TMU\" : \"high\" , \"MET\" : \"high\" , \"PLT\" : \"high\" , \"BCE\" : \"low\" , \"BST\" : \"low\" , \"BCQ\" : \"low\" , \"BCS\" : \"low\" , \"BCT\" : \"low\" } # Add the capacity_type column to the stops dataframe stops_df [ \"capacity_type\" ] = stops_df [ \"StopType\" ] . map ( capacity_map ) return stops_df extract_mca ( mca_file ) Extract data from the mca file. The logic for this extraction is as follows Each new journey starts with \"BS\". Within this journey we have * multiple stops * LO is origin * LI are inbetween stops * LT is terminating stop * Then a new journey starts with BS again Within each journey are a few more lines that we can ignore e.g. * BX = extra details of the journey * CR = changes en route. Doesnt contain any arrival / departure times. Process: * Starts by finding all the schedules within the file. * Extract relevant information into the journey dataframe, and then copy unique_id * onto all trips within that journey. Parameters: mca_file ( _type_ ) \u2013 description Returns: schedules ( list ) \u2013 list of lists containing schedule information ready for dataframe stops ( list ) \u2013 list of lists containing stop information ready for dataframe Source code in src/time_table/time_table_utils.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 def extract_mca ( mca_file : str ) -> Tuple [ List [ List ], List [ List ]]: \"\"\"Extract data from the mca file. The logic for this extraction is as follows: Each new journey starts with \"BS\". Within this journey we have * multiple stops * LO is origin * LI are inbetween stops * LT is terminating stop * Then a new journey starts with BS again Within each journey are a few more lines that we can ignore e.g. * BX = extra details of the journey * CR = changes en route. Doesnt contain any arrival / departure times. Process: * Starts by finding all the schedules within the file. * Extract relevant information into the journey dataframe, and then copy unique_id * onto all trips within that journey. Args: mca_file (_type_): _description_ Returns: schedules (list): list of lists containing schedule information ready for dataframe stops (list): list of lists containing stop information ready for dataframe \"\"\" # Create a flag that specifies when we have found a new journey journey = False # Store schedule information schedules = [] # Store stop information stops = [] with open ( mca_file , 'r' ) as mca_data : # Skip the header next ( mca_data ) for line in mca_data : # A schedule is started by a record beginning with BS. # Other entries exist but are not needed for our purpose. # Schedules are then further broken down by transaction type # (N - new, R - revised, D - delete) # Ignore any that are transaction type delete. if line [ 0 : 3 ] == 'BSN' or line [ 0 : 3 ] == 'BSR' : # Switch flag on as we have found a journey journey = True # Get unique ID for schedule # ID in dataset is not actually unique as same train has several # schedules with different dates and calendars. Create ID from # these variables. schedule_id = ( line [ 3 : 9 ] + line [ 9 : 15 ] + line [ 15 : 21 ] + line [ 21 : 28 ]) # Extract start and end date of service (yymmdd) start_date = line [ 9 : 15 ] . strip () end_date = line [ 15 : 21 ] . strip () # Extract the calender information for this journey # i.e. what days of the week it runs # Only interested in weekdays for the timebeing. monday = int ( line [ 21 ] . strip ()) tuesday = int ( line [ 22 ] . strip ()) wednesday = int ( line [ 23 ] . strip ()) thursday = int ( line [ 24 ] . strip ()) friday = int ( line [ 25 ] . strip ()) # Store data to be added to the dataframe schedules . append ([ schedule_id , start_date , end_date , monday , tuesday , wednesday , thursday , friday ]) # Skip as this is all we need to do for an entry starting # with BS continue # If we have found a new journey, go through the lines that are # part of this jounrey and give them the schedule id. # Journeys split into origin (LO), stops (LI) and terminus (LT) # CR (changes en route) and BX (extra schedule details) can be # ignored as not relevant to our purpose. # If station has a departure time extract time, tiploc_code and # activity type. # NB times can end on a H sometimes which indicates a half minute # Rather than rounding up and down, just ignoring this for the moment # and taking only first four characters (hh:mm) if journey : if line [ 0 : 2 ] == 'LO' : departure_time = line [ 10 : 14 ] . strip () tiploc_code = line [ 2 : 10 ] . strip () activity_type = line [ 29 : 41 ] . strip () elif line [ 0 : 2 ] == 'LI' : departure_time = line [ 15 : 19 ] . strip () tiploc_code = line [ 2 : 10 ] . strip () activity_type = line [ 42 : 54 ] . strip () elif line [ 0 : 2 ] == 'LT' : departure_time = line [ 15 : 19 ] . strip () tiploc_code = line [ 2 : 10 ] . strip () activity_type = line [ 25 : 37 ] . strip () # As we know that LT signifies the last stop in a journey # and we have extracted everything we need we now switch the # flag off journey = False else : # Skipping BR and CX continue # Store data to be added to dataframe stops . append ([ schedule_id , departure_time , tiploc_code , activity_type ]) return schedules , stops extract_msn_data ( msn_file ) Extract data from the msn file. Parameters: msn_file ( msn ) \u2013 A text file containing the msn data. Returns: list ( List [ List ] ) \u2013 A list of lists containing the msn data. Source code in src/time_table/time_table_utils.py 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 def extract_msn_data ( msn_file : str ) -> List [ List ]: \"\"\"Extract data from the msn file. Args: msn_file (msn): A text file containing the msn data. Returns: list: A list of lists containing the msn data. \"\"\" # Store msn data msn_data_lst = [] with open ( msn_file , 'r' ) as msn_data : # Skip header next ( msn_data ) for line in msn_data : # Only interested in rows starting with A. # Rows starting with L display aliases of station names # Stripping the values because some are padded out with blank # spaces as part of the file format. # Coordinate data provided is actually the grid reference # but without the 100km square (two letters at the start) so # very difficult to extract coordinates. Hence, will add in # coordinate data from an external source. # NB tiploc_code is unique, but crs_code isnt. if line . startswith ( 'A' ): station_name = line [ 5 : 31 ] . strip () tiploc_code = line [ 36 : 43 ] . strip () crs_code = line [ 49 : 52 ] . strip () msn_data_lst . append ([ station_name , tiploc_code , crs_code ]) return msn_data_lst filter_stops ( stops_df ) Filters the stops dataframe based on two things: | 1) Status column. We want to keep stops which are active, pending or new. | 2) StopType want only to include bus and rail stops. Parameters: stops_df ( DataFrame ) \u2013 the dataframe to filter. Returns: DataFrame \u2013 pd.DataFrame: Filtered_stops which meet the criteria of keeping based on status/stoptype columns. Source code in src/time_table/time_table_utils.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def filter_stops ( stops_df : pd . DataFrame ) -> pd . DataFrame : \"\"\"Filters the stops dataframe based on two things: | 1) Status column. We want to keep stops which are active, pending or new. | 2) StopType want only to include bus and rail stops. Args: stops_df (pd.DataFrame): the dataframe to filter. Returns: pd.DataFrame: Filtered_stops which meet the criteria of keeping based on status/stoptype columns. \"\"\" # stop_types we would like to keep within the dataframe stop_types = [ \"RSE\" , \"RLY\" , \"RPL\" , \"TMU\" , \"MET\" , \"PLT\" , \"BCE\" , \"BST\" , \"BCQ\" , \"BCS\" , \"BCT\" ] # Filter the stops based on the status column (active, pending, new and # None) filtered_stops = stops_df [( stops_df [ \"Status\" ] == \"active\" ) | ( stops_df [ \"Status\" ] == \"pending\" ) | ( stops_df [ \"Status\" ] is None ) | ( stops_df [ \"Status\" ] == \"new\" )] # Filter the stops based on the stop types (bus and rail) boolean_stops_type = filtered_stops [ \"StopType\" ] . isin ( stop_types ) filter_stops = filtered_stops [ boolean_stops_type ] return filter_stops filter_timetable_by_day ( timetable_df , day ) Extract serviced stops based on specific day of the week. The day is selected from the available days in the date range present in timetable data. 1) identifies which days dates in the entire date range 2) counts days of each type to get the maximum position order 3) validates user's choice for day - provides useful errors 4) creates ord value that is half of maximum position order to ensure as many services get included as possible. 4) selects a date based on the day and ord parameters 5) filters the dataframe to that date Parameters: timetable_df ( pandas dataframe ) \u2013 df to filter day ( str) ) \u2013 day of the week in title case, e.g. \"Wednesday\" Returns: DataFrame \u2013 pd.DataFrame: filtered pandas dataframe Source code in src/time_table/time_table_utils.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 def filter_timetable_by_day ( timetable_df : pd . DataFrame , day : str ) -> pd . DataFrame : \"\"\"Extract serviced stops based on specific day of the week. The day is selected from the available days in the date range present in timetable data. 1) identifies which days dates in the entire date range 2) counts days of each type to get the maximum position order 3) validates user's choice for `day` - provides useful errors 4) creates ord value that is half of maximum position order to ensure as many services get included as possible. 4) selects a date based on the day and ord parameters 5) filters the dataframe to that date Args: timetable_df (pandas dataframe): df to filter day (str) : day of the week in title case, e.g. \"Wednesday\" Returns: pd.DataFrame: filtered pandas dataframe \"\"\" # Measure the dataframe original_rows = timetable_df . shape [ 0 ] # Count the services orig_service_count = timetable_df . service_id . unique () . shape [ 0 ] # Get the minimum date range earliest_start_date = timetable_df . start_date . min () latest_end_date = timetable_df . end_date . max () # Identify days in the range and count them date_range = pd . date_range ( earliest_start_date , latest_end_date ) date_day_couplings_df = pd . DataFrame ({ \"date\" : date_range , \"day_name\" : date_range . day_name ()}) days_counted = date_day_couplings_df . day_name . value_counts () days_counted_dict = days_counted . to_dict () # Validate user choices if day not in days_counted_dict . keys (): raise KeyError ( \"\"\"The day chosen in not available. Should be a weekday in title case.\"\"\" ) # Get the maximum position order (ordinal) max_ord = days_counted_dict [ day ] ord = round ( max_ord / 2 ) # Filter all the dates down the to the day needed day_filtered_dates = ( date_day_couplings_df [ date_day_couplings_df . day_name == day ]) # Get date of the nth (ord) day nth = ord - 1 date_of_day_entered = day_filtered_dates . iloc [ nth ] . date # Filter the timetable_df by date range timetable_df = timetable_df [( timetable_df [ 'start_date' ] <= date_of_day_entered ) & ( timetable_df [ 'end_date' ] >= date_of_day_entered )] # Then filter to day of interest timetable_df = timetable_df [ timetable_df [ day . lower ()] == 1 ] # Filter the timetable_df by date range timetable_df = timetable_df [( timetable_df [ 'start_date' ] <= date_of_day_entered ) & ( timetable_df [ 'end_date' ] >= date_of_day_entered )] # Then filter to day of interest timetable_df = timetable_df [ timetable_df [ day . lower ()] == 1 ] # Print date being used (consider logging instead) day_date = date_of_day_entered . date () logger ( f \"The date of { day } number { ord } is { day_date } \" ) # Print how many rows have been dropped (consider logging instead) logger ( f \"Selecting only services covering { day_date } reduced records\" f \"by { original_rows - timetable_df . shape [ 0 ] } rows\" ) # Print how many services are in the analysis and how many were dropped service_count = timetable_df . service_id . unique () . shape [ 0 ] dropped_services = orig_service_count - service_count logger ( f \"There are { service_count } services in the analysis\" ) logger ( f \"Filtering by day has reduced services by { dropped_services } \" ) return timetable_df","title":"Time table utils"},{"location":"time_table_utils/#src.time_table.time_table_utils.add_stop_capacity_type","text":"Adds capacity_type column. Column is defined with the following dictionary using the StopType Bus stops are low capacity, train stations are high capacity. Parameters: stops_df ( DataFrame ) \u2013 The dataframe to add the column to. Returns: DataFrame \u2013 pd.DataFrame: dataframe with new capacity_type column. Source code in src/time_table/time_table_utils.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def add_stop_capacity_type ( stops_df : pd . DataFrame ) -> pd . DataFrame : \"\"\"Adds capacity_type column. Column is defined with the following dictionary using the StopType Bus stops are low capacity, train stations are high capacity. Args: stops_df (pd.DataFrame): The dataframe to add the column to. Returns: pd.DataFrame: dataframe with new capacity_type column. \"\"\" # Create a dictionary to map the StopType to capacity level capacity_map = { \"RSE\" : \"high\" , \"RLY\" : \"high\" , \"RPL\" : \"high\" , \"TMU\" : \"high\" , \"MET\" : \"high\" , \"PLT\" : \"high\" , \"BCE\" : \"low\" , \"BST\" : \"low\" , \"BCQ\" : \"low\" , \"BCS\" : \"low\" , \"BCT\" : \"low\" } # Add the capacity_type column to the stops dataframe stops_df [ \"capacity_type\" ] = stops_df [ \"StopType\" ] . map ( capacity_map ) return stops_df","title":"add_stop_capacity_type"},{"location":"time_table_utils/#src.time_table.time_table_utils.extract_mca","text":"Extract data from the mca file. The logic for this extraction is as follows Each new journey starts with \"BS\". Within this journey we have * multiple stops * LO is origin * LI are inbetween stops * LT is terminating stop * Then a new journey starts with BS again Within each journey are a few more lines that we can ignore e.g. * BX = extra details of the journey * CR = changes en route. Doesnt contain any arrival / departure times. Process: * Starts by finding all the schedules within the file. * Extract relevant information into the journey dataframe, and then copy unique_id * onto all trips within that journey. Parameters: mca_file ( _type_ ) \u2013 description Returns: schedules ( list ) \u2013 list of lists containing schedule information ready for dataframe stops ( list ) \u2013 list of lists containing stop information ready for dataframe Source code in src/time_table/time_table_utils.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 def extract_mca ( mca_file : str ) -> Tuple [ List [ List ], List [ List ]]: \"\"\"Extract data from the mca file. The logic for this extraction is as follows: Each new journey starts with \"BS\". Within this journey we have * multiple stops * LO is origin * LI are inbetween stops * LT is terminating stop * Then a new journey starts with BS again Within each journey are a few more lines that we can ignore e.g. * BX = extra details of the journey * CR = changes en route. Doesnt contain any arrival / departure times. Process: * Starts by finding all the schedules within the file. * Extract relevant information into the journey dataframe, and then copy unique_id * onto all trips within that journey. Args: mca_file (_type_): _description_ Returns: schedules (list): list of lists containing schedule information ready for dataframe stops (list): list of lists containing stop information ready for dataframe \"\"\" # Create a flag that specifies when we have found a new journey journey = False # Store schedule information schedules = [] # Store stop information stops = [] with open ( mca_file , 'r' ) as mca_data : # Skip the header next ( mca_data ) for line in mca_data : # A schedule is started by a record beginning with BS. # Other entries exist but are not needed for our purpose. # Schedules are then further broken down by transaction type # (N - new, R - revised, D - delete) # Ignore any that are transaction type delete. if line [ 0 : 3 ] == 'BSN' or line [ 0 : 3 ] == 'BSR' : # Switch flag on as we have found a journey journey = True # Get unique ID for schedule # ID in dataset is not actually unique as same train has several # schedules with different dates and calendars. Create ID from # these variables. schedule_id = ( line [ 3 : 9 ] + line [ 9 : 15 ] + line [ 15 : 21 ] + line [ 21 : 28 ]) # Extract start and end date of service (yymmdd) start_date = line [ 9 : 15 ] . strip () end_date = line [ 15 : 21 ] . strip () # Extract the calender information for this journey # i.e. what days of the week it runs # Only interested in weekdays for the timebeing. monday = int ( line [ 21 ] . strip ()) tuesday = int ( line [ 22 ] . strip ()) wednesday = int ( line [ 23 ] . strip ()) thursday = int ( line [ 24 ] . strip ()) friday = int ( line [ 25 ] . strip ()) # Store data to be added to the dataframe schedules . append ([ schedule_id , start_date , end_date , monday , tuesday , wednesday , thursday , friday ]) # Skip as this is all we need to do for an entry starting # with BS continue # If we have found a new journey, go through the lines that are # part of this jounrey and give them the schedule id. # Journeys split into origin (LO), stops (LI) and terminus (LT) # CR (changes en route) and BX (extra schedule details) can be # ignored as not relevant to our purpose. # If station has a departure time extract time, tiploc_code and # activity type. # NB times can end on a H sometimes which indicates a half minute # Rather than rounding up and down, just ignoring this for the moment # and taking only first four characters (hh:mm) if journey : if line [ 0 : 2 ] == 'LO' : departure_time = line [ 10 : 14 ] . strip () tiploc_code = line [ 2 : 10 ] . strip () activity_type = line [ 29 : 41 ] . strip () elif line [ 0 : 2 ] == 'LI' : departure_time = line [ 15 : 19 ] . strip () tiploc_code = line [ 2 : 10 ] . strip () activity_type = line [ 42 : 54 ] . strip () elif line [ 0 : 2 ] == 'LT' : departure_time = line [ 15 : 19 ] . strip () tiploc_code = line [ 2 : 10 ] . strip () activity_type = line [ 25 : 37 ] . strip () # As we know that LT signifies the last stop in a journey # and we have extracted everything we need we now switch the # flag off journey = False else : # Skipping BR and CX continue # Store data to be added to dataframe stops . append ([ schedule_id , departure_time , tiploc_code , activity_type ]) return schedules , stops","title":"extract_mca"},{"location":"time_table_utils/#src.time_table.time_table_utils.extract_msn_data","text":"Extract data from the msn file. Parameters: msn_file ( msn ) \u2013 A text file containing the msn data. Returns: list ( List [ List ] ) \u2013 A list of lists containing the msn data. Source code in src/time_table/time_table_utils.py 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 def extract_msn_data ( msn_file : str ) -> List [ List ]: \"\"\"Extract data from the msn file. Args: msn_file (msn): A text file containing the msn data. Returns: list: A list of lists containing the msn data. \"\"\" # Store msn data msn_data_lst = [] with open ( msn_file , 'r' ) as msn_data : # Skip header next ( msn_data ) for line in msn_data : # Only interested in rows starting with A. # Rows starting with L display aliases of station names # Stripping the values because some are padded out with blank # spaces as part of the file format. # Coordinate data provided is actually the grid reference # but without the 100km square (two letters at the start) so # very difficult to extract coordinates. Hence, will add in # coordinate data from an external source. # NB tiploc_code is unique, but crs_code isnt. if line . startswith ( 'A' ): station_name = line [ 5 : 31 ] . strip () tiploc_code = line [ 36 : 43 ] . strip () crs_code = line [ 49 : 52 ] . strip () msn_data_lst . append ([ station_name , tiploc_code , crs_code ]) return msn_data_lst","title":"extract_msn_data"},{"location":"time_table_utils/#src.time_table.time_table_utils.filter_stops","text":"Filters the stops dataframe based on two things: | 1) Status column. We want to keep stops which are active, pending or new. | 2) StopType want only to include bus and rail stops. Parameters: stops_df ( DataFrame ) \u2013 the dataframe to filter. Returns: DataFrame \u2013 pd.DataFrame: Filtered_stops which meet the criteria of keeping based on status/stoptype columns. Source code in src/time_table/time_table_utils.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def filter_stops ( stops_df : pd . DataFrame ) -> pd . DataFrame : \"\"\"Filters the stops dataframe based on two things: | 1) Status column. We want to keep stops which are active, pending or new. | 2) StopType want only to include bus and rail stops. Args: stops_df (pd.DataFrame): the dataframe to filter. Returns: pd.DataFrame: Filtered_stops which meet the criteria of keeping based on status/stoptype columns. \"\"\" # stop_types we would like to keep within the dataframe stop_types = [ \"RSE\" , \"RLY\" , \"RPL\" , \"TMU\" , \"MET\" , \"PLT\" , \"BCE\" , \"BST\" , \"BCQ\" , \"BCS\" , \"BCT\" ] # Filter the stops based on the status column (active, pending, new and # None) filtered_stops = stops_df [( stops_df [ \"Status\" ] == \"active\" ) | ( stops_df [ \"Status\" ] == \"pending\" ) | ( stops_df [ \"Status\" ] is None ) | ( stops_df [ \"Status\" ] == \"new\" )] # Filter the stops based on the stop types (bus and rail) boolean_stops_type = filtered_stops [ \"StopType\" ] . isin ( stop_types ) filter_stops = filtered_stops [ boolean_stops_type ] return filter_stops","title":"filter_stops"},{"location":"time_table_utils/#src.time_table.time_table_utils.filter_timetable_by_day","text":"Extract serviced stops based on specific day of the week. The day is selected from the available days in the date range present in timetable data. 1) identifies which days dates in the entire date range 2) counts days of each type to get the maximum position order 3) validates user's choice for day - provides useful errors 4) creates ord value that is half of maximum position order to ensure as many services get included as possible. 4) selects a date based on the day and ord parameters 5) filters the dataframe to that date Parameters: timetable_df ( pandas dataframe ) \u2013 df to filter day ( str) ) \u2013 day of the week in title case, e.g. \"Wednesday\" Returns: DataFrame \u2013 pd.DataFrame: filtered pandas dataframe Source code in src/time_table/time_table_utils.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 def filter_timetable_by_day ( timetable_df : pd . DataFrame , day : str ) -> pd . DataFrame : \"\"\"Extract serviced stops based on specific day of the week. The day is selected from the available days in the date range present in timetable data. 1) identifies which days dates in the entire date range 2) counts days of each type to get the maximum position order 3) validates user's choice for `day` - provides useful errors 4) creates ord value that is half of maximum position order to ensure as many services get included as possible. 4) selects a date based on the day and ord parameters 5) filters the dataframe to that date Args: timetable_df (pandas dataframe): df to filter day (str) : day of the week in title case, e.g. \"Wednesday\" Returns: pd.DataFrame: filtered pandas dataframe \"\"\" # Measure the dataframe original_rows = timetable_df . shape [ 0 ] # Count the services orig_service_count = timetable_df . service_id . unique () . shape [ 0 ] # Get the minimum date range earliest_start_date = timetable_df . start_date . min () latest_end_date = timetable_df . end_date . max () # Identify days in the range and count them date_range = pd . date_range ( earliest_start_date , latest_end_date ) date_day_couplings_df = pd . DataFrame ({ \"date\" : date_range , \"day_name\" : date_range . day_name ()}) days_counted = date_day_couplings_df . day_name . value_counts () days_counted_dict = days_counted . to_dict () # Validate user choices if day not in days_counted_dict . keys (): raise KeyError ( \"\"\"The day chosen in not available. Should be a weekday in title case.\"\"\" ) # Get the maximum position order (ordinal) max_ord = days_counted_dict [ day ] ord = round ( max_ord / 2 ) # Filter all the dates down the to the day needed day_filtered_dates = ( date_day_couplings_df [ date_day_couplings_df . day_name == day ]) # Get date of the nth (ord) day nth = ord - 1 date_of_day_entered = day_filtered_dates . iloc [ nth ] . date # Filter the timetable_df by date range timetable_df = timetable_df [( timetable_df [ 'start_date' ] <= date_of_day_entered ) & ( timetable_df [ 'end_date' ] >= date_of_day_entered )] # Then filter to day of interest timetable_df = timetable_df [ timetable_df [ day . lower ()] == 1 ] # Filter the timetable_df by date range timetable_df = timetable_df [( timetable_df [ 'start_date' ] <= date_of_day_entered ) & ( timetable_df [ 'end_date' ] >= date_of_day_entered )] # Then filter to day of interest timetable_df = timetable_df [ timetable_df [ day . lower ()] == 1 ] # Print date being used (consider logging instead) day_date = date_of_day_entered . date () logger ( f \"The date of { day } number { ord } is { day_date } \" ) # Print how many rows have been dropped (consider logging instead) logger ( f \"Selecting only services covering { day_date } reduced records\" f \"by { original_rows - timetable_df . shape [ 0 ] } rows\" ) # Print how many services are in the analysis and how many were dropped service_count = timetable_df . service_id . unique () . shape [ 0 ] dropped_services = orig_service_count - service_count logger ( f \"There are { service_count } services in the analysis\" ) logger ( f \"Filtering by day has reduced services by { dropped_services } \" ) return timetable_df","title":"filter_timetable_by_day"}]}